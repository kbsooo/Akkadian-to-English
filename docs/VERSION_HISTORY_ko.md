# 버전 흐름 기록 (V1 → V2)

이 문서는 계획이 아니라 **실제로 시도한 것과 결과**를 정리한 기록이다.

---

## V1

### 목표
- 기본 베이스라인을 빠르게 만들고 성능 확인
- 전처리 파이프라인 구축 후 sentence‑level 데이터 생성

### 주요 시도
- **전처리 파이프라인**
  - 규칙 기반 + 어노테이션 기반 병합
  - 품질 점수/패턴 필터 도입
  - Tier 구성:
    - Tier1: valid 전체
    - Tier2: min_quality ≥ 0.7
    - Tier3: min_quality ≥ 0.7 + 패턴 필터
  - 결과: **Tier3 약 2,350쌍** 생성

- **학습**
  - 로컬(MPS) 베이스라인 시도
  - Kaggle용 학습 템플릿 구성

- **추론**
  - Kaggle inference notebook 구성
  - 모델 로딩/토크나이저 불일치 의심

### 결과
- **Public score: 0.0**
- 제출 파일에 `!!!!` 반복 출력 발생

### 실패 원인(사후 분석)
1) **모델/체크포인트 불일치 가능성**
   - 학습 시 사용 모델과 업로드 모델 구조가 다름
2) **Train/Test 전처리 불일치**
   - 문서 레벨 학습 vs 테스트 문장 레벨
3) **토크나이저 메타 문제**
   - `tokenizer_config.json`의 `extra_ids` 불일치 가능성

### 교훈
- 문장 레벨 학습으로 반드시 전환 필요
- 체크포인트 업로드/경로 검증 프로세스 필요

---

## V2

### 목표
- V1 실패 요인 제거
- Train/Test 스타일 차이를 정규화로 흡수
- 문장 레벨 데이터로 학습

### 주요 시도
- **정규화 모듈 통합**
  - diacritics/특수문자를 ASCII로 통일
  - OCR 아티팩트(„ 등) 정리

- **데이터 재구성**
  - `data/v2/` 기반 파이프라인 구축
  - 문장 레벨 데이터 생성:
    - `v2_sentence_train.csv` (2,112)
    - `v2_sentence_val.csv` (238)
  - 증강 데이터 정제:
    - `v2_train_augmented_clean.csv` 생성
    - 길이 비율/`<gap>`/`<unk>` 과다 케이스 제거

- **학습 안정화**
  - FP16 비활성화
  - max_len 256으로 축소
  - learning rate 1e-4로 안정화
  - gradient clipping 추가

### 결과
- **Public score: 11.8**
- V1 대비 의미 있는 개선 확인

### 남은 한계
- 문장 레벨 데이터 규모가 여전히 작음
- publications 기반 확장 데이터가 아직 본격적으로 반영되지 않음

### 교훈
- **문장 레벨 + 정규화 통일**이 가장 큰 개선 포인트
- 수치 안정성(FP16, 길이, LR) 영향이 매우 큼

---

## 다음 단계(비계획, 기록용 메모)
- publications 기반 데이터 확장 시도 예정
- 문장 레벨 중심 유지, doc‑level은 보조로만 활용

---

## V3

### 목표
- ByT5‑Large + LoRA로 성능 상향 시도
- V2 문장 데이터 유지, 모델 규모 확장으로 일반화 기대

### 주요 시도
- **학습**
  - 모델: `google/byt5-large`
  - 방식: LoRA (q/k/v/o 대상)
  - 데이터: `v2_sentence_train/val` 그대로 사용
  - Colab(A100)에서 학습 및 체크포인트 저장

- **추론**
  - LoRA를 merge한 **merged model 방식**으로 변경
  - 인터넷 OFF 대응 (PEFT 미사용)
  - 추론은 정상적으로 동작 확인

### 결과
- **Public score: 4.7 → 5.7 (infer 수정 후 소폭 개선)**
- V2 대비 성능 하락 (여전히 낮음)

### 추정 원인(정리)
1) **Train/Infer 전처리/토크나이저 불일치(부분 해결)**
   - 초기 V3 infer 정규화가 V2 학습 규칙과 달라 성능 급락
   - V2와 동일 정규화 + ByT5Tokenizer 고정 후 5.7까지 개선
2) **모델 패키징/경로 혼선 가능성**
   - merged model이 **베스트 체크포인트에서 merge된 것인지** 불확실
3) **모델 규모 + LoRA 제약**
   - 데이터가 작아 LoRA가 충분히 수렴하지 못했을 가능성
4) **Generation 제약(반복 페널티/노리핏)**
   - V2와 다른 디코딩 제약이 품질을 낮췄을 가능성

### 교훈
- **정규화 함수는 반드시 학습과 동일하게 유지**
- merged model을 쓸 경우 **베스트 체크포인트에서 merge했는지 확인**
- 작은 데이터에서는 **LoRA로 큰 모델을 안정적으로 올리는 게 쉽지 않음**
