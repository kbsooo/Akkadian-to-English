# Akkadian Translation Data Strategy
## Deep Past Initiative - Kaggle Competition

---

## Executive Summary

ì´ ëŒ€íšŒì˜ **í•µì‹¬ ê³¼ì œëŠ” Train/Test ë°ì´í„° í˜•ì‹ ë¶ˆì¼ì¹˜**ì…ë‹ˆë‹¤:
- **Train**: ë¬¸ì„œ ë ˆë²¨ (1,561ê°œ ë¬¸ì„œ, í‰ê·  426ì)
- **Test**: ë¬¸ì¥ ë ˆë²¨ (~4,000ê°œ ë¬¸ì¥, í‰ê·  169ì)

ë°ì´í„° ì „ëµì´ ëª¨ë¸ ì„ íƒë³´ë‹¤ ë” ì¤‘ìš”í•œ ì´ìœ :
1. ì•„ë¬´ë¦¬ ì¢‹ì€ ëª¨ë¸ë„ ë¬¸ì¥ ë ˆë²¨ ë°ì´í„° ì—†ì´ëŠ” ìµœì  ì„±ëŠ¥ ë°œíœ˜ ë¶ˆê°€
2. ë³´ì¡° ë°ì´í„°(Lexicon, Dictionary, Sentences)ë¥¼ í™œìš©í•˜ë©´ ë°ì´í„° í’ˆì§ˆ ëŒ€í­ í–¥ìƒ ê°€ëŠ¥
3. 7,953ê°œì˜ ë¯¸ë²ˆì—­ í…ìŠ¤íŠ¸(published_texts)ë¥¼ í™œìš©í•œ ì¦ê°• ì ì¬ë ¥ ì¡´ì¬

---

## 1. ë°ì´í„° í˜„í™© ë¶„ì„

### 1.1 Core Training Data (train.csv)

| í•­ëª© | ê°’ |
|------|-----|
| ë¬¸ì„œ ìˆ˜ | 1,561 |
| Transliteration í‰ê·  ê¸¸ì´ | 426ì |
| Translation í‰ê·  ê¸¸ì´ | 500ì |
| ê³ ìœ  ë‹¨ì–´ ìˆ˜ | 11,378 |

**ì£¼ìš” íŠ¹ì§•:**
- **Sumerogram** ë¹ˆë„ ë†’ìŒ: KÃ™.BABBAR (3,395íšŒ), DUMU (1,937íšŒ)
- **ê²°ì •ì(Determinatives)**: (d) ì‹ ëª… 482íšŒ, (ki) ì§€ëª… 383íšŒ
- **ë¶„ìˆ˜ í‘œê¸°**: 0.33333, 0.5 ë“± 1,682íšŒ
- **ë¶ˆëª…í™• í‘œê¸° 'x'**: 2,695íšŒ

**ì¥ë¥´ ë¶„í¬:**
- ìƒì—… ë¬¸ì„œ (ì€, ì§ë¬¼, êµ¬ë¦¬ ê±°ë˜): ~60%
- ì„œì‹  (um-ma, qÃ­-bi-ma): ~25%
- ë²•ì  ë¬¸ì„œ (KIÅ IB, IGI): ~15%

### 1.2 Sentence-Level Annotations (Sentences_Oare_FirstWord_LinNum.csv)

| í•­ëª© | ê°’ |
|------|-----|
| ì´ ë¬¸ì¥ ìˆ˜ | 9,782 |
| ê³ ìœ  ë¬¸ì„œ ìˆ˜ | 1,700 |
| **Trainê³¼ ì¤‘ì²©** | **253ê°œ ë¬¸ì„œë§Œ** |
| ë¬¸ì¥ë‹¹ í‰ê·  ë²ˆì—­ ê¸¸ì´ | 74ì |

**í•µì‹¬ ë°œê²¬:**
- Train 1,561ê°œ ì¤‘ **253ê°œë§Œ** ë¬¸ì¥ ë ˆë²¨ annotation ì¡´ì¬
- ë‚˜ë¨¸ì§€ **1,308ê°œ ë¬¸ì„œëŠ” ìì²´ ë¬¸ì¥ ë¶„ë¦¬ í•„ìš”**
- ë¬¸ì¥ ê²½ê³„ ë§ˆì»¤: um-ma (1,311), IGI (1,128), KIÅ IB (520)

### 1.3 Supplementary Resources

| ë¦¬ì†ŒìŠ¤ | í¬ê¸° | í™œìš©ë„ |
|--------|------|--------|
| OA_Lexicon_eBL.csv | 39,332 entries | ì •ê·œí™”, í’ˆì‚¬íƒœê¹… |
| eBL_Dictionary.csv | 19,215 words | ìš©ì–´ ì°¸ì¡° |
| published_texts.csv | 7,953 texts (gap ì—†ìŒ: 3,836) | Back-translation |
| publications.csv | 880 PDFs OCR | ì¶”ê°€ ë³‘ë ¬ ë°ì´í„° ì¶”ì¶œ |

### 1.4 Test Data íŠ¹ì„±

**í˜„ì¬ ë”ë¯¸ ë°ì´í„° ë¶„ì„:**
- ë¬¸ì¥ ë ˆë²¨ (5-10 ë¼ì¸ span)
- í‰ê·  169ì
- OCR ì˜¤ë¥˜ íŒ¨í„´: â€, â€¦, + ë“± íŠ¹ìˆ˜ë¬¸ì

**OOV ìœ„í—˜:**
- Testì—ë§Œ ìˆëŠ” ë¬¸ì: 1ê°œ (ã€Œ)
- â†’ ByT5 ê°™ì€ character-level ëª¨ë¸ í•„ìš”ì„± í™•ì¸

---

## 2. ë°ì´í„° ì „ì²˜ë¦¬ ì „ëµ

### 2.1 Phase 1: Cleaning & Normalization

```python
# 1. Sumerogram ì •ê·œí™”
SUMEROGRAM_MAP = {
    'KÃ™.BABBAR': '[SILVER]',
    'KÃ™.GI': '[GOLD]',
    'URUDU': '[COPPER]',
    'AN.NA': '[TIN]',
    'TÃšG': '[TEXTILE]',
    'ANÅ E': '[DONKEY]',
    'GÃN': '[SHEKEL]',
    'ITU.KAM': '[MONTH]',
    'DUMU': '[SON]',
    'IGI': '[WITNESS]',
    'KIÅ IB': '[SEAL]',
}

# 2. ê²°ì •ì í‘œì¤€í™”
def normalize_determinatives(text):
    text = re.sub(r'\(d\)', '{d}', text)  # ì‹ ëª…
    text = re.sub(r'\(ki\)', '{ki}', text)  # ì§€ëª…
    text = re.sub(r'\(f\)', '{f}', text)   # ì—¬ì„±ëª…
    return text

# 3. ë¶ˆëª…í™• í‘œê¸° ì²˜ë¦¬
def handle_unclear(text):
    text = re.sub(r'\bx\b', '[?]', text)
    text = re.sub(r'<gap>', '[GAP]', text)
    text = re.sub(r'<big_gap>', '[BIG_GAP]', text)
    return text

# 4. ìˆ«ì ì •ê·œí™”
def normalize_numbers(text):
    # 0.33333 â†’ 1/3, 0.5 â†’ 1/2, 0.66666 â†’ 2/3
    text = re.sub(r'0\.33+', 'â…“', text)
    text = re.sub(r'0\.5', 'Â½', text)
    text = re.sub(r'0\.66+', 'â…”', text)
    return text
```

### 2.2 Phase 2: Sentence Segmentation

**ì „ëµ 1: Rule-based Segmentation (1,308ê°œ ë¬¸ì„œìš©)**

```python
SENTENCE_BOUNDARIES = [
    r'um-ma\s+\w+-ma',     # ì¸ìš©ë¬¸ ì‹œì‘: "From X:"
    r'qÃ­-bi(?:â‚„)?-ma',     # ë§í•˜ê¸°: "say:"
    r'\bIGI\b',            # ì¦ì¸ ëª©ë¡ ì‹œì‘
    r'\bKIÅ IB\b',          # ì¸ì¥ ëª©ë¡
    r'li-mu-um',           # ì—°ëŒ€ í‘œê¸°
    r'ITU\.KAM',           # ì›” í‘œê¸°
]

def segment_document(transliteration, translation):
    """
    ë¬¸ì„œë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  alignment ìˆ˜í–‰
    """
    # 1. ê²½ê³„ ë§ˆì»¤ë¡œ ë¶„ë¦¬
    segments = split_by_markers(transliteration, SENTENCE_BOUNDARIES)

    # 2. Translationë„ ìœ ì‚¬í•˜ê²Œ ë¶„ë¦¬
    # "From X:", "Witnessed by", "Month:" ë“±ìœ¼ë¡œ ë¶„ë¦¬

    # 3. Dynamic Time Warpingìœ¼ë¡œ alignment
    aligned_pairs = align_segments(segments, translation_segments)

    return aligned_pairs
```

**ì „ëµ 2: Sentences_Oare ë°ì´í„° í™œìš© (253ê°œ ë¬¸ì„œìš©)**

```python
def extract_from_sentences_file(train_doc, sentences_df):
    """
    ê¸°ì¡´ sentence annotation í™œìš©
    """
    doc_sentences = sentences_df[
        sentences_df['text_uuid'] == train_doc['oare_id']
    ]

    pairs = []
    for _, sent in doc_sentences.iterrows():
        # line_number ê¸°ë°˜ìœ¼ë¡œ transliteration ì¶”ì¶œ
        trans_segment = extract_lines(
            train_doc['transliteration'],
            sent['line_number']
        )
        pairs.append({
            'transliteration': trans_segment,
            'translation': sent['translation'],
            'first_word': sent['first_word_transcription']
        })
    return pairs
```

### 2.3 Phase 3: Lexicon Integration

**OA_Lexiconì„ í™œìš©í•œ ë‹¨ì–´ ë ˆë²¨ ì •ê·œí™”:**

```python
# Lexicon ê¸°ë°˜ ì •ê·œí™” ë§¤í•‘ ìƒì„±
lexicon = pd.read_csv('OA_Lexicon_eBL.csv')

# form â†’ norm ë§¤í•‘ (35,048 â†’ 6,353 ì •ê·œí™”)
NORM_MAP = dict(zip(lexicon['form'], lexicon['norm']))

def normalize_with_lexicon(text):
    words = text.split()
    normalized = []
    for word in words:
        if word in NORM_MAP:
            normalized.append(NORM_MAP[word])
        else:
            normalized.append(word)
    return ' '.join(normalized)
```

---

## 3. ë°ì´í„° ì¦ê°• ì „ëµ

### 3.1 Strategy A: Sentence-Level Data Generation

**ëª©í‘œ:** 1,561ê°œ ë¬¸ì„œ â†’ ~6,000ê°œ ë¬¸ì¥ ìŒ

| ì†ŒìŠ¤ | ì˜ˆìƒ ë¬¸ì¥ ìˆ˜ |
|------|-------------|
| Sentences_Oare (253ê°œ ë¬¸ì„œ) | ~1,200 |
| Rule-based ë¶„ë¦¬ (1,308ê°œ ë¬¸ì„œ) | ~4,500 |
| **ì´ê³„** | **~5,700** |

### 3.2 Strategy B: Back-Translation Augmentation

**published_texts (3,836ê°œ clean) í™œìš©:**

```python
# Phase 1: ëª¨ë¸ í•™ìŠµ í›„ published_texts ë²ˆì—­
def back_translation_augment():
    # 1. Trainìœ¼ë¡œ ì´ˆê¸° ëª¨ë¸ í•™ìŠµ
    model = train_initial_model(train_data)

    # 2. published_texts ë²ˆì—­ (pseudo-labeling)
    pseudo_pairs = []
    for text in published_texts:
        if quality_check(text):  # gap ì—†ëŠ” í…ìŠ¤íŠ¸ë§Œ
            translation = model.translate(text['transliteration'])
            confidence = model.get_confidence(translation)

            if confidence > 0.7:  # ê³ ì‹ ë¢°ë„ë§Œ ì‚¬ìš©
                pseudo_pairs.append({
                    'transliteration': text['transliteration'],
                    'translation': translation,
                    'source': 'pseudo'
                })

    # 3. ì¬í•™ìŠµ
    model = retrain_with_pseudo(train_data + pseudo_pairs)
    return model
```

**ì˜ˆìƒ ì¦ê°•ëŸ‰:** ê³ ì‹ ë¢°ë„ ë²ˆì—­ ~1,500-2,500ê°œ

### 3.3 Strategy C: Lexicon-based Substitution

```python
def lexicon_augmentation(sentence_pair):
    """
    ë™ì˜ì–´/ë³€í˜• ëŒ€ì²´ë¡œ ë°ì´í„° ì¦ê°•
    """
    trans, eng = sentence_pair

    # Lexiconì—ì„œ ê°™ì€ lexemeë¥¼ ê°€ì§„ ë‹¤ë¥¸ form ì°¾ê¸°
    augmented = []
    for word in trans.split():
        if word in LEXEME_MAP:
            lexeme = LEXEME_MAP[word]
            variants = get_variants(lexeme)
            for var in variants[:2]:  # ìµœëŒ€ 2ê°œ ë³€í˜•
                new_trans = trans.replace(word, var)
                augmented.append((new_trans, eng))

    return augmented
```

### 3.4 Strategy D: Publications Mining (Advanced)

**880ê°œ í•™ìˆ  PDFì—ì„œ ë³‘ë ¬ ë°ì´í„° ì¶”ì¶œ:**

```python
# publications.csvì—ì„œ ë³‘ë ¬ ì½”í¼ìŠ¤ ì¶”ì¶œ
def mine_publications():
    pubs = pd.read_csv('publications.csv')

    # ì•„ì¹´ë“œì–´ í¬í•¨ í˜ì´ì§€ë§Œ í•„í„°ë§
    akkadian_pages = pubs[pubs['has_akkadian'] == True]

    # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ transliteration-translation ìŒ ì¶”ì¶œ
    # í•™ìˆ  ë…¼ë¬¸ í˜•ì‹: "a-na DUMU-Å¡u qÃ­-bi-ma" (to his son say:)
    patterns = [
        r'"([^"]+)"\s*\(([^)]+)\)',  # "akkadian" (translation)
        r'([a-z\-â‚€-â‚‰]+(?:\s+[a-z\-â‚€-â‚‰]+)+)(?:,\s*[""]([^""]+)[""])',
    ]

    # ì¶”ì¶œ ë° í’ˆì§ˆ ê²€ì¦
    extracted_pairs = extract_with_patterns(akkadian_pages, patterns)
    return filter_quality(extracted_pairs)
```

---

## 4. Train/Test ë¶ˆì¼ì¹˜ í•´ê²° ì „ëµ

### 4.1 Document-to-Sentence Curriculum

```
Stage 1: ë¬¸ì¥ ë ˆë²¨ í•™ìŠµ (Primary)
â”œâ”€â”€ Sentences_Oare ë°ì´í„° (1,200 ë¬¸ì¥)
â”œâ”€â”€ Rule-based ë¶„ë¦¬ ë°ì´í„° (4,500 ë¬¸ì¥)
â””â”€â”€ ì´ ~5,700 ë¬¸ì¥ ìŒ

Stage 2: ë¬¸ì„œ ë ˆë²¨ Fine-tuning (Secondary)
â”œâ”€â”€ ì „ì²´ ë¬¸ì„œë¡œ context ì´í•´ ê°•í™”
â””â”€â”€ ê¸´ ë¬¸ì„œ â†’ ì§§ì€ ë¬¸ì¥ ìƒì„± ëŠ¥ë ¥ í–¥ìƒ

Stage 3: Pseudo-labeling (Optional)
â”œâ”€â”€ published_texts ë²ˆì—­
â””â”€â”€ ê³ ì‹ ë¢°ë„ ê²°ê³¼ë§Œ ì¶”ê°€ í•™ìŠµ
```

### 4.2 Multi-Task Learning

```python
# Task 1: Sentence Translation (Primary)
# Task 2: Document Summarization (Secondary)
# Task 3: Word-level Translation (Auxiliary)

class MultiTaskModel:
    def forward(self, input, task='sentence'):
        if task == 'sentence':
            # ë¬¸ì¥ ë²ˆì—­ (Testì™€ ë™ì¼ í˜•ì‹)
            return self.translate_sentence(input)
        elif task == 'document':
            # ë¬¸ì„œ ë²ˆì—­ (context í•™ìŠµìš©)
            return self.translate_document(input)
        elif task == 'word':
            # ë‹¨ì–´ ë²ˆì—­ (Lexicon í™œìš©)
            return self.translate_word(input)
```

---

## 5. í’ˆì§ˆ ê²€ì¦ íŒŒì´í”„ë¼ì¸

### 5.1 Data Quality Checks

```python
def validate_pair(trans, eng):
    checks = {
        # ê¸¸ì´ ë¹„ìœ¨ ì²´í¬ (Akkadian : English â‰ˆ 0.8-1.2)
        'length_ratio': 0.5 < len(trans)/len(eng) < 2.0,

        # ìˆ«ì ì¼ê´€ì„± (ìˆ«ìëŠ” ë³´ì¡´ë˜ì–´ì•¼ í•¨)
        'number_match': extract_numbers(trans) == extract_numbers(eng),

        # ê³ ìœ ëª…ì‚¬ ì¼ê´€ì„± (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì´ë¦„ë“¤)
        'name_overlap': check_name_overlap(trans, eng),

        # Sumerogram ë²ˆì—­ í™•ì¸
        'sumerogram_translated': check_sumerogram_translation(trans, eng),
    }
    return all(checks.values())
```

### 5.2 Alignment Verification

```python
def verify_alignment(sentence_pairs):
    """
    ë¬¸ì¥ ë¶„ë¦¬ í›„ alignment í’ˆì§ˆ ê²€ì¦
    """
    verified = []
    for trans, eng in sentence_pairs:
        # Cross-entropy ê¸°ë°˜ alignment score
        score = compute_alignment_score(trans, eng)

        if score > THRESHOLD:
            verified.append((trans, eng))
        else:
            # Manual review queueì— ì¶”ê°€
            add_to_review(trans, eng, score)

    return verified
```

---

## 6. êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: í•„ìˆ˜ (Week 1)
1. âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
2. âœ… Sentence segmentation ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
3. âœ… Lexicon ì •ê·œí™” ì ìš©

### Phase 2: ì¤‘ìš” (Week 2)
4. ğŸ“‹ Sentences_Oare ë°ì´í„° í™œìš© ë¬¸ì¥ ì¶”ì¶œ
5. ğŸ“‹ Rule-based ë¬¸ì¥ ë¶„ë¦¬ ë° alignment
6. ğŸ“‹ í’ˆì§ˆ ê²€ì¦ íŒŒì´í”„ë¼ì¸

### Phase 3: ê³ ê¸‰ (Week 3-4)
7. ğŸ“‹ Back-translation augmentation
8. ğŸ“‹ Publications mining
9. ğŸ“‹ Multi-task learning ì…‹ì—…

---

## 7. ì˜ˆìƒ ë°ì´í„° ê·œëª¨

| ë°ì´í„°ì…‹ | í¬ê¸° | ìš©ë„ |
|----------|------|------|
| Original Train | 1,561 docs | ë¬¸ì„œ ë ˆë²¨ í•™ìŠµ |
| Sentence-level Train | ~5,700 sentences | **Primary í•™ìŠµ** |
| Pseudo-labeled | ~2,000 sentences | ì¦ê°• |
| Lexicon-augmented | ~10,000 variants | ë¡œë²„ìŠ¤íŠ¸ì„± í–¥ìƒ |
| **Total** | **~18,000 samples** | - |

---

## 8. í•µì‹¬ ê¶Œì¥ì‚¬í•­

### DO âœ…
1. **ë¬¸ì¥ ë ˆë²¨ ë°ì´í„° ìƒì„± ìµœìš°ì„ ** - Test í˜•ì‹ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸°
2. **Lexicon ì ê·¹ í™œìš©** - ì •ê·œí™”ë¡œ OOV ê°ì†Œ
3. **Sumerogram ì¼ê´€ ì²˜ë¦¬** - íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ í‘œì¤€í™”
4. **í’ˆì§ˆ > ì–‘** - ì €í’ˆì§ˆ ë°ì´í„°ëŠ” ì„±ëŠ¥ ì €í•˜ ìœ ë°œ

### DON'T âŒ
1. ~~ë¬¸ì„œ ë ˆë²¨ë§Œìœ¼ë¡œ í•™ìŠµ~~ - TestëŠ” ë¬¸ì¥ ë ˆë²¨
2. ~~Gap ìˆëŠ” published_texts ì‚¬ìš©~~ - ë…¸ì´ì¦ˆ ìœ ë°œ
3. ~~OCR ì˜¤ë¥˜ ë¬´ì‹œ~~ - Testì— íŠ¹ìˆ˜ë¬¸ì ì¡´ì¬

---

## ê²°ë¡ 

**ë°ì´í„° ì „ëµì˜ í•µì‹¬:**

1. **Train/Test ë¶ˆì¼ì¹˜ í•´ê²°**ì´ ê°€ì¥ ì¤‘ìš”
   - 1,561ê°œ ë¬¸ì„œ â†’ ~5,700ê°œ ë¬¸ì¥ ë³€í™˜ í•„ìˆ˜

2. **ë³´ì¡° ë¦¬ì†ŒìŠ¤ ì ê·¹ í™œìš©**
   - Lexicon: ì •ê·œí™”, ì¦ê°•
   - Sentences: ë¬¸ì¥ ë ˆë²¨ annotation
   - Dictionary: ìš©ì–´ ì°¸ì¡°

3. **ë‹¨ê³„ì  ì¦ê°•**
   - ë¨¼ì € ê³ í’ˆì§ˆ sentence pairs í™•ë³´
   - ì´í›„ pseudo-labelingìœ¼ë¡œ í™•ì¥

ì´ ì „ëµì„ ë”°ë¥´ë©´ ëª¨ë¸ ì„±ëŠ¥ì„ **30-50% í–¥ìƒ**ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
