# V2 Training Debug Report

**Date**: 2025-01-31
**Issue**: Training loss explosion (7,129,515) and NaN validation loss
**Status**: âœ… Root cause identified

---

## 1. ë¬¸ì œ í˜„ìƒ

### 1.1 ì¦ìƒ
```
Epoch 1: Training Loss: 7129515.3131, Validation Loss: nan, BLEU: 0.00, chrF++: 0.00
Epoch 2: Training Loss: 0.0000, Validation Loss: nan, BLEU: 0.00, chrF++: 0.00
```

- 1 ì—í¬í¬: Lossê°€ **7ë°±ë§Œ ì´ìƒ**ìœ¼ë¡œ í­ë°œ
- 2 ì—í¬í¬: Lossê°€ **0.0**ìœ¼ë¡œ ë¶•ê´´ (gradient vanishing)
- ëª¨ë“  ì—í¬í¬: Validation Loss = **NaN**
- í‰ê°€ ë©”íŠ¸ë¦­: BLEU = 0, chrF++ = 0

### 1.2 ì´ì „ ì‹œë„ (ì‹¤íŒ¨)
ì²« ë²ˆì§¸ í•™ìŠµ ì‹œë„ì—ì„œë„ ìœ ì‚¬í•œ íŒ¨í„´:
```
Epoch 1: Training Loss: 341142, Validation Loss: nan, BLEU: 0.00, chrF++: 0.00
Epoch 2: Training Loss: 0.0000, Validation Loss: nan, BLEU: 0.00, chrF++: 0.00
```

---

## 2. ë””ë²„ê¹… ê³¼ì •

### 2.1 ë°ì´í„° ê²€ì¦ âœ…
```python
# ê²€ì¦ ê²°ê³¼
- ì´ í•™ìŠµ ë°ì´í„°: 2,565í–‰
- NaN ê°’: 0ê°œ
- ë¹ˆ ë¬¸ìì—´: 0ê°œ
- Train/Val ì¤‘ë³µ ID: 0ê°œ
```

**ê²°ë¡ **: ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ì•„ë‹˜

### 2.2 ë¹„ì •ìƒ ë¬¸ì ê²€ì‚¬

Target(ì˜ì–´ ë²ˆì—­)ì—ì„œ ë¹„ASCII ë¬¸ì ë°œê²¬:
```
ì „ì²´ í–‰: 2565
ë¹„ASCII íƒ€ê²Ÿ í–‰: 2377 (92.7%)
ì•„ì¹´ë“œì–´ íŠ¹ìˆ˜ë¬¸ì í¬í•¨ í–‰: 2372 (92.5%)
```

ë°œê²¬ëœ ë¬¸ìë“¤:
- `Å¡`, `Å ` (shin)
- `Ä`, `Ä“`, `Ä«`, `Å«` (ì¥ëª¨ìŒ)
- `á¹£`, `á¹¢`, `á¹­` (emphatic consonants)

**ë¶„ì„ ê²°ê³¼**: ì´ ë¬¸ìë“¤ì€ **ê³ ìœ ëª…ì‚¬ì˜ í•™ìˆ ì  ì „ì‚¬**ì— ì‚¬ìš©ë¨
```
ì˜ˆì‹œ: "Seal of Mannum-balum-AÅ¡Å¡ur son of á¹¢illi-Adad"
      "From Å u-TammuzÄ«, Elaya, Ennam-AÅ¡Å¡ur..."
```

**ê²°ë¡ **: ì •ìƒì ì¸ í•™ìˆ  ë²ˆì—­ ë°ì´í„°, ë¬¸ì œ ì•„ë‹˜

### 2.3 ì½”ë“œ ë¹„êµ ë¶„ì„ ğŸ”

ë‘ íŒŒì¼ ê°„ ì„¤ì • ì°¨ì´ ë°œê²¬:

| ì„¤ì • | `akka_v2_train.py` | `akka_v2_train.ipynb` |
|------|--------------------|-----------------------|
| **fp16** | `False` âœ… | `True` âŒ |
| max_source_length | 256 | 512 |
| learning_rate | 1e-4 | 3e-4 |
| warmup_ratio | 0.1 | 0.05 |

---

## 3. ê·¼ë³¸ ì›ì¸

### ğŸš¨ FP16 (Half Precision) + ByT5 = Numerical Instability

**ByT5ëŠ” FP16ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**

#### ì´ìœ :
1. **Byte-level processing**: ByT5ëŠ” ë¬¸ìê°€ ì•„ë‹Œ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œí€€ìŠ¤ê°€ ë§¤ìš° ê¸¸ì–´ì§
2. **ê¸´ ì‹œí€€ìŠ¤ + FP16**: Attention score ê³„ì‚° ì‹œ ìˆ˜ì¹˜ overflow ë°œìƒ
3. **Gradient explosion**: Lossê°€ ìˆ˜ë°±ë§Œê¹Œì§€ í­ë°œ
4. **NaN propagation**: í•œë²ˆ NaNì´ ë°œìƒí•˜ë©´ ì „ì²´ gradientì— ì „íŒŒ

#### ê¸°ìˆ ì  ì„¤ëª…:
```
FP16 ë²”ìœ„: Â±65,504 (ìµœëŒ€ê°’)
FP32 ë²”ìœ„: Â±3.4 Ã— 10^38

ByT5 attention ê³„ì‚°:
- ì‹œí€€ìŠ¤ ê¸¸ì´ 512 bytes
- Attention scores: softmax(QK^T / âˆšd)
- ê¸´ ì‹œí€€ìŠ¤ì—ì„œ QK^T ê°’ì´ FP16 ë²”ìœ„ ì´ˆê³¼ ê°€ëŠ¥
â†’ Overflow â†’ Inf â†’ NaN
```

#### ê´€ë ¨ ì´ìŠˆ:
- [Hugging Face Issue #12039](https://github.com/huggingface/transformers/issues/12039)
- T5/ByT5 ê³„ì—´ ëª¨ë¸ì˜ ì•Œë ¤ì§„ FP16 ë¶ˆì•ˆì •ì„±

---

## 4. í•´ê²° ë°©ë²•

### 4.1 í•„ìˆ˜ ìˆ˜ì • (Config í´ë˜ìŠ¤)

```python
@dataclass
class Config:
    # ... ê¸°ì¡´ ì„¤ì • ...

    # âš ï¸ í•µì‹¬ ìˆ˜ì •: FP16 ë¹„í™œì„±í™”
    fp16: bool = False      # True â†’ False
    bf16: bool = False      # A100ì—ì„œëŠ” True ê°€ëŠ¥í•˜ë‚˜ ì•ˆì „í•˜ê²Œ False

    # ì¶”ê°€ ì•ˆì •í™” ì„¤ì •
    max_source_length: int = 256    # 512 â†’ 256 (overflow ë°©ì§€)
    max_target_length: int = 256    # 512 â†’ 256
    learning_rate: float = 1e-4     # 3e-4 â†’ 1e-4 (ì•ˆì •ì )
    warmup_ratio: float = 0.1       # 0.05 â†’ 0.1 (ì ì§„ì  í•™ìŠµ)
```

### 4.2 ëŒ€ì•ˆ: BF16 ì‚¬ìš© (A100/H100ë§Œ)

A100 GPUëŠ” BF16 (Brain Float 16)ì„ ì§€ì›í•©ë‹ˆë‹¤:
```python
fp16: bool = False
bf16: bool = True  # A100ì—ì„œë§Œ!
```

BF16ì€ FP16ë³´ë‹¤ ë” ë„“ì€ ì§€ìˆ˜ ë²”ìœ„ë¥¼ ê°€ì ¸ overflowì— ê°•í•©ë‹ˆë‹¤.

**ì£¼ì˜**: T4, V100 ë“±ì€ BF16ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ

### 4.3 ë©”ëª¨ë¦¬ ê³ ë ¤ì‚¬í•­

FP32ëŠ” FP16ì˜ 2ë°° ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ (ì¶”ì •) |
|------|-------------------|
| FP16, batch=2, seq=512 | ~12GB |
| FP32, batch=2, seq=512 | ~24GB |
| FP32, batch=2, seq=256 | ~12GB |

A100 (40GB/80GB)ì—ì„œëŠ” ì¶©ë¶„í•˜ì§€ë§Œ, ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ 256ìœ¼ë¡œ ì¤„ì´ë©´ ë” ì•ˆì „í•©ë‹ˆë‹¤.

---

## 5. ìˆ˜ì •ëœ ì„¤ì • (ê¶Œì¥)

```python
@dataclass
class Config:
    """Training configuration - FP16 disabled for ByT5 stability."""

    # Model
    model_name: str = "google/byt5-base"

    # Paths
    data_dir: Path = None
    output_dir: Path = Path("/content/drive/MyDrive/akkadian/v2")

    # Sequence lengths (reduced for stability)
    max_source_length: int = 256
    max_target_length: int = 256

    # Training hyperparameters
    seed: int = 42
    batch_size: int = 4
    gradient_accumulation_steps: int = 4  # effective batch = 16
    epochs: int = 10
    learning_rate: float = 1e-4
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01

    # Hardware - CRITICAL: FP16 must be False for ByT5!
    fp16: bool = False
    bf16: bool = False  # Set True only on A100/H100
    gradient_checkpointing: bool = True
    dataloader_num_workers: int = 2
```

---

## 6. ì˜ˆìƒ ê²°ê³¼

ìˆ˜ì • í›„ ì˜ˆìƒë˜ëŠ” ì •ìƒì ì¸ í•™ìŠµ ë¡œê·¸:
```
Epoch 1: Training Loss: 2.5-4.0, Validation Loss: 2.0-3.5, BLEU: 0-5, chrF++: 5-15
Epoch 2: Training Loss: 1.5-2.5, Validation Loss: 1.5-2.5, BLEU: 5-15, chrF++: 15-25
...
Epoch 10: Training Loss: 0.3-0.8, Validation Loss: 0.5-1.0, BLEU: 20-40, chrF++: 40-60
```

---

## 7. ì²´í¬ë¦¬ìŠ¤íŠ¸

ìˆ˜ì • ì „ í™•ì¸ì‚¬í•­:

- [ ] `fp16: bool = False` ì„¤ì • í™•ì¸
- [ ] `bf16: bool = False` ë˜ëŠ” A100ì´ë©´ `True`
- [ ] `max_source_length` â‰¤ 256 (ê¶Œì¥)
- [ ] `learning_rate` = 1e-4 (ê¶Œì¥)
- [ ] `warmup_ratio` â‰¥ 0.1 (ê¶Œì¥)
- [ ] Colabì—ì„œ **ipynb íŒŒì¼**ì˜ Configê°€ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
- [ ] GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ì¸ (FP32ëŠ” 2ë°° ë©”ëª¨ë¦¬ í•„ìš”)

---

## 8. ì°¸ê³  ìë£Œ

- [ByT5 Paper](https://arxiv.org/abs/2105.13626)
- [Transformers FP16 Training Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one#fp16-training)
- [Mixed Precision Training Best Practices](https://pytorch.org/docs/stable/notes/amp_examples.html)
- [BF16 vs FP16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)

---

**Report generated by Claude**
