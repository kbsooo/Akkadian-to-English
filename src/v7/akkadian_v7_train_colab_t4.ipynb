{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507d56fc",
   "metadata": {},
   "source": [
    "# V7 Training — ByT5-small Akkadian→English (Colab T4 16GB)\n",
    "\n",
    "**Environment:** Google Colab Free/Pro, single T4 16GB\n",
    "\n",
    "### T4 Optimization Strategy\n",
    "| Constraint | Solution |\n",
    "|------------|----------|\n",
    "| 16GB VRAM | Gradient checkpointing ON (~30% VRAM savings) |\n",
    "| No BF16 | FP32 only — ByT5 NaN with FP16, T4 lacks BF16 |\n",
    "| Slower matmul | Batch=8 + grad_accum=2 → effective=16 |\n",
    "| Slow eval | Greedy decoding + short gen length (eval ≠ final inference) |\n",
    "| Colab timeout | Early stopping patience=5, ~4-5h total |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42fc82",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaed3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_USERNAME = \"your-username\"  # EDIT THIS\n",
    "V7_DATA_DATASET = f\"{KAGGLE_USERNAME}/akkadian-v7-data\"\n",
    "MODEL_NAME = \"google/byt5-small\"  # ~300M params (d_model=1472, 12+4 layers)\n",
    "OUTPUT_DIR = \"outputs_v7\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 384\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 8        # T4 16GB + FP32 + gradient checkpointing → batch=8 safe\n",
    "GRAD_ACCUM = 2        # effective batch = 8 × 2 = 16\n",
    "LR = 5e-5             # standard for effective batch=16\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.005\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LABEL_SMOOTHING = 0.1\n",
    "SEED = 42\n",
    "\n",
    "print(\"Configuration (Colab T4 16GB):\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Max length: {MAX_SOURCE_LENGTH}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch: {BATCH_SIZE} × {GRAD_ACCUM} accum = {BATCH_SIZE * GRAD_ACCUM} effective\")\n",
    "print(f\"  LR: {LR}\")\n",
    "print(f\"  Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"  Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd70db8",
   "metadata": {},
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c422e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                       \"kagglehub\", \"transformers\", \"datasets\", \"sacrebleu\", \"accelerate\"])\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27962cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddc980",
   "metadata": {},
   "source": [
    "## Download V7 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "try:\n",
    "    data_path = kagglehub.dataset_download(V7_DATA_DATASET)\n",
    "    print(f\"Data from Kaggle: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Kaggle download failed: {e}\")\n",
    "    print(\"Trying Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = \"/content/drive/MyDrive/akkadian/data_v7\"\n",
    "\n",
    "print(\"\\nData files:\")\n",
    "for f in sorted(os.listdir(data_path)):\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae3a50",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{data_path}/v7_train.csv\")\n",
    "val_df = pd.read_csv(f\"{data_path}/v7_val.csv\")\n",
    "train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val:   {len(val_df):,}\")\n",
    "print(f\"\\nSource breakdown:\")\n",
    "print(train_df[\"source\"].value_counts().to_string())\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  src: {train_df['src'].iloc[0][:100]}...\")\n",
    "print(f\"  tgt: {train_df['tgt'].iloc[0][:100]}...\")\n",
    "\n",
    "pct_over = (train_df[\"src\"].str.len() > MAX_SOURCE_LENGTH).mean() * 100\n",
    "print(f\"\\nSources > {MAX_SOURCE_LENGTH} chars: {pct_over:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e81523",
   "metadata": {},
   "source": [
    "## V7 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e864b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_V7_TRANS_TABLE = str.maketrans({\n",
    "    \"Ḫ\": \"H\", \"ḫ\": \"h\",\n",
    "    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\",\n",
    "    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\",\n",
    "    \"„\": '\"', \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\", \"ʾ\": \"'\", \"ʿ\": \"'\",\n",
    "})\n",
    "\n",
    "def normalize_transliteration_v7(text: str) -> str:\n",
    "    \"\"\"V7 normalization: preserves š, ṣ, ṭ and vowel accents. Only Ḫ→H.\"\"\"\n",
    "    if not text or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "    text = text.replace(\"<gap>\", \"\\x00GAP\\x00\")\n",
    "    text = text.replace(\"<big_gap>\", \"\\x00BIGGAP\\x00\")\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\[\\s*…+\\s*…*\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = text.replace(\"…\", \" \\x00BIGGAP\\x00 \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" \\x00GAP\\x00 \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "    for c in \"‹›⌈⌉⌊⌋˹˺\":\n",
    "        text = text.replace(c, \"\")\n",
    "    text = text.translate(_V7_TRANS_TABLE)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    text = re.sub(r\"(?<![a-zA-Z\\x00])\\bx\\b(?![a-zA-Z])\", \" \\x00GAP\\x00 \", text)\n",
    "    text = text.replace(\"\\x00GAP\\x00\", \"<gap>\")\n",
    "    text = text.replace(\"\\x00BIGGAP\\x00\", \"<big_gap>\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"V7 normalization defined (preserves š, ṣ, ṭ, vowel accents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b2860",
   "metadata": {},
   "source": [
    "## Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba72eec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# FP32: ByT5 byte embeddings cause NaN under FP16 (5-bit exponent, max 65504)\n",
    "# T4 lacks BF16 support, so FP32 is the only safe option\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {n_params:,}\")\n",
    "print(f\"Dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dda331",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    # text_target ensures decoder-side tokenization with proper BOS token handling\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"], max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True, padding=False,\n",
    "        text_target=examples[\"tgt\"])\n",
    "    labels = model_inputs[\"labels\"]\n",
    "    model_inputs[\"labels\"] = [l[:MAX_TARGET_LENGTH] for l in labels]\n",
    "    return model_inputs\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"],\n",
    "                        num_proc=2)\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"],\n",
    "                    num_proc=2)\n",
    "\n",
    "print(f\"Tokenized — Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "print(f\"Sample input_ids len: {len(train_ds[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b0bde",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe2764",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bleu_metric = BLEU()\n",
    "chrf_metric = CHRF(word_order=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = [p.strip() for p in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "    decoded_labels = [[l.strip()] for l in tokenizer.batch_decode(labels, skip_special_tokens=True)]\n",
    "    bleu = bleu_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    chrf = chrf_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    geo = (bleu * chrf) ** 0.5 if bleu > 0 and chrf > 0 else 0.0\n",
    "    return {\"bleu\": bleu, \"chrf\": chrf, \"geo_mean\": geo}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49879a01",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7214e75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            avg = sum(self.losses) / len(self.losses)\n",
    "            print(f\"\\n--- Epoch {int(state.epoch)} avg train loss: {avg:.4f} ---\")\n",
    "            self.losses = []\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"  BLEU: {metrics.get('eval_bleu', 0):.2f}  \"\n",
    "                  f\"chrF: {metrics.get('eval_chrf', 0):.2f}  \"\n",
    "                  f\"Geo: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "\n",
    "\n",
    "class SampleCallback(TrainerCallback):\n",
    "    \"\"\"Generate sample translations after each eval.\"\"\"\n",
    "    def __init__(self, tokenizer, samples):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = samples[:3]\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        print(\"\\nSample translations:\")\n",
    "        for i, src in enumerate(self.samples):\n",
    "            inputs = self.tokenizer(src, return_tensors=\"pt\", truncation=True,\n",
    "                                    max_length=MAX_SOURCE_LENGTH)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_length=128, num_beams=1)\n",
    "            trans = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            print(f\"  [{i}] {src[:60]}...\")\n",
    "            print(f\"      -> {trans[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30399c",
   "metadata": {},
   "source": [
    "## Training Arguments (T4 16GB Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da381e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "# Steps per epoch (for eval scheduling)\n",
    "steps_per_epoch = len(train_ds) // (BATCH_SIZE * GRAD_ACCUM)\n",
    "# Eval every 2 epochs to minimize ByT5's slow autoregressive eval generation\n",
    "eval_interval = steps_per_epoch * 2\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # eval uses less memory (no gradients)\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    label_smoothing_factor=LABEL_SMOOTHING,\n",
    "    max_grad_norm=1.0,\n",
    "    # ByT5 requires FP32 — FP16 causes NaN, T4 lacks BF16\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    # Gradient checkpointing: trades ~20% speed for ~30% VRAM savings\n",
    "    # Essential for batch=8 on T4 16GB with ByT5's long byte sequences\n",
    "    gradient_checkpointing=True,\n",
    "    # Eval every ~2 epochs: ByT5 byte-level generation is the dominant time cost\n",
    "    # (up to 384 autoregressive steps per sample). Training forward/backward is fast;\n",
    "    # eval generation is memory-bandwidth-bound and barely benefits from faster GPUs.\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_interval,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=eval_interval,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    # Generation during eval — greedy + short length to keep eval fast\n",
    "    # Final inference uses beam search separately; eval only needs metric tracking\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    "    # Colab has 2 CPU cores — 2 workers for data loading\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(\"Training args configured (T4 16GB)\")\n",
    "print(f\"  Effective batch: {BATCH_SIZE} × {GRAD_ACCUM} accum = {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"  Gradient checkpointing: ON\")\n",
    "print(f\"  Precision: FP32\")\n",
    "print(f\"  Eval every: ~2 epochs ({eval_interval} steps)\")\n",
    "print(f\"  Eval generation: greedy, max_length=128\")\n",
    "print(f\"  LR: {LR} (cosine → 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1630d76",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aced248",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    LogCallback(),\n",
    "    SampleCallback(tokenizer, val_df[\"src\"].tolist()),\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE),\n",
    "]\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"Starting training on T4 16GB...\")\n",
    "print(f\"  Train samples: {len(train_ds):,}\")\n",
    "print(f\"  Steps/epoch: ~{steps_per_epoch}\")\n",
    "print(f\"  Max epochs: {EPOCHS} (early stop patience={EARLY_STOPPING_PATIENCE})\")\n",
    "print(f\"  Estimated time: ~4-5 hours\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4cc90",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"\\nFinal evaluation:\")\n",
    "print(f\"  BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"  chrF: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"  Geo:  {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00215e",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = f\"{OUTPUT_DIR}/final\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "trainer.model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "config_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_source_length\": MAX_SOURCE_LENGTH,\n",
    "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "    \"epochs_trained\": int(trainer.state.epoch),\n",
    "    \"effective_batch_size\": BATCH_SIZE * GRAD_ACCUM,\n",
    "    \"learning_rate\": LR,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"normalization\": \"v7_preserve_diacritics\",\n",
    "    \"gpu\": \"T4 16GB\",\n",
    "    \"precision\": \"FP32\",\n",
    "}\n",
    "with open(f\"{final_dir}/v7_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel saved to {final_dir}/\")\n",
    "for fname in sorted(os.listdir(final_dir)):\n",
    "    size = os.path.getsize(f\"{final_dir}/{fname}\") / 1e6\n",
    "    print(f\"  {fname} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551dc69",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "device0 = next(model.parameters()).device\n",
    "\n",
    "test_input = \"um-ma ka-ru-um\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device0)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_length=50, num_beams=4)\n",
    "translation = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  Input:  '{test_input}'\")\n",
    "print(f\"  Output: '{translation}'\")\n",
    "assert translation.strip() != \"\", \"Empty output!\"\n",
    "print(\"  OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb8ab2",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Upload `outputs_v7/final/` to Kaggle as Dataset `akkadian-v7-model`.\n",
    "\n",
    "Required files:\n",
    "- config.json, model.safetensors\n",
    "- tokenizer.json, tokenizer_config.json, special_tokens_map.json\n",
    "- v7_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1301bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Best geo_mean: {results.get('eval_geo_mean', 0):.2f}\")\n",
    "print(f\"  BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"  chrF: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"  Epochs: {int(trainer.state.epoch)}\")\n",
    "print(f\"  GPU: T4 16GB (FP32)\")\n",
    "print(f\"\\n  Output: {final_dir}/\")\n",
    "print(f\"  -> Upload to Kaggle Dataset 'akkadian-v7-model'\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
