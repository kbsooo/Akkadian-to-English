{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d60cd37",
   "metadata": {},
   "source": [
    "# V7 Training — ByT5-small Akkadian→English (Colab A100 40GB)\n",
    "\n",
    "### A100 Optimizations vs T4\n",
    "| Feature | T4 16GB | A100 40GB |\n",
    "|---------|---------|-----------|\n",
    "| Precision | FP32 only | **BF16** (same range as FP32, safe for ByT5) |\n",
    "| Batch | 8 | **32** (4× larger, fewer steps) |\n",
    "| Grad checkpointing | ON (saves VRAM) | **OFF** (no recomputation, ~20% faster) |\n",
    "| TF32 matmul | N/A | **ON** (3× faster FP32 matmuls via Tensor Cores) |\n",
    "| torch.compile | N/A | **ON** (graph fusion, ~10-15% speedup) |\n",
    "| **Net speedup** | baseline | **~5-8× faster** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92829d",
   "metadata": {},
   "source": [
    "Configuration for V7 training — A100 optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_USERNAME = \"your-username\"  # EDIT THIS\n",
    "V7_DATA_DATASET = f\"{KAGGLE_USERNAME}/akkadian-v7-data\"\n",
    "MODEL_NAME = \"google/byt5-small\"\n",
    "OUTPUT_DIR = \"outputs_v7\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 384\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32      # A100 40GB + BF16 + no grad ckpt → batch=32 fits easily\n",
    "GRAD_ACCUM = 1       # effective batch = 32 × 1 = 32\n",
    "LR = 7e-5            # sqrt-scaled from 5e-5: LR × sqrt(32/16) ≈ 7e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.005\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LABEL_SMOOTHING = 0.1\n",
    "SEED = 42\n",
    "\n",
    "print(\"Configuration (A100 40GB):\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Max length: {MAX_SOURCE_LENGTH}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch: {BATCH_SIZE} × {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM} effective\")\n",
    "print(f\"  LR: {LR} (sqrt-scaled for batch=32)\")\n",
    "print(f\"  Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"  Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b17f3",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kagglehub\", \"transformers\", \"datasets\", \"sacrebleu\", \"accelerate\"])\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702f514",
   "metadata": {},
   "source": [
    "Import libraries and setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_mem:.1f} GB\")\n",
    "    # TF32: A100 Tensor Cores can do FP32 matmuls at TF32 precision (~3× faster)\n",
    "    # Negligible accuracy impact (19-bit mantissa precision vs 23-bit)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 matmul: ENABLED (A100 Tensor Cores)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3415c5",
   "metadata": {},
   "source": [
    "Download V7 data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5799347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "try:\n",
    "    data_path = kagglehub.dataset_download(V7_DATA_DATASET)\n",
    "    print(f\"Data from Kaggle: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Kaggle download failed: {e}\")\n",
    "    print(\"Trying Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = \"/content/drive/MyDrive/akkadian/data_v7\"\n",
    "\n",
    "print(\"\\nData files:\")\n",
    "for f in sorted(os.listdir(data_path)):\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360ec61",
   "metadata": {},
   "source": [
    "Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64482aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{data_path}/v7_train.csv\")\n",
    "val_df = pd.read_csv(f\"{data_path}/v7_val.csv\")\n",
    "train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"\\nSource breakdown (train):\")\n",
    "print(train_df[\"source\"].value_counts())\n",
    "print(f\"\\nSample src: {train_df['src'].iloc[0][:120]}\")\n",
    "print(f\"Sample tgt: {train_df['tgt'].iloc[0][:120]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02072bef",
   "metadata": {},
   "source": [
    "V7 Normalization (exact - must match build and infer scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_V7_TRANS_TABLE = str.maketrans({\n",
    "    \"Ḫ\": \"H\", \"ḫ\": \"h\",\n",
    "    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\",\n",
    "    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\",\n",
    "    \"„\": '\"', \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\", \"ʾ\": \"'\", \"ʿ\": \"'\",\n",
    "})\n",
    "\n",
    "def normalize_transliteration_v7(text: str) -> str:\n",
    "    \"\"\"V7 normalization: preserves š, ṣ, ṭ and vowel accents. Only Ḫ→H.\"\"\"\n",
    "    if not text or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Standardize determinative braces: {} → () to match test format\n",
    "    text = text.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "    # Protect existing gap tokens\n",
    "    text = text.replace(\"<gap>\", \"\\x00GAP\\x00\")\n",
    "    text = text.replace(\"<big_gap>\", \"\\x00BIGGAP\\x00\")\n",
    "    # Remove apostrophe line numbers (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "    # Remove angle-bracket content markers (keep content)\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "    # Large gaps\n",
    "    text = re.sub(r\"\\[\\s*…+\\s*…*\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = text.replace(\"…\", \" \\x00BIGGAP\\x00 \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    # Single gap: [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" \\x00GAP\\x00 \", text, flags=re.IGNORECASE)\n",
    "    # Strip square brackets, keep content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "    # Half brackets / editorial marks\n",
    "    for c in \"‹›⌈⌉⌊⌋˹˺\":\n",
    "        text = text.replace(c, \"\")\n",
    "    # Character map (diacritics preserved except Ḫ→H)\n",
    "    text = text.translate(_V7_TRANS_TABLE)\n",
    "    # Scribal notations\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    # Standalone x → gap\n",
    "    text = re.sub(r\"(?<![a-zA-Z\\x00])\\bx\\b(?![a-zA-Z])\", \" \\x00GAP\\x00 \", text)\n",
    "    # Restore gap tokens\n",
    "    text = text.replace(\"\\x00GAP\\x00\", \"<gap>\")\n",
    "    text = text.replace(\"\\x00BIGGAP\\x00\", \"<big_gap>\")\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"V7 normalization function defined\")\n",
    "print(\"  Preserves: š, ṣ, ṭ, vowel accents\")\n",
    "print(\"  Only removes: Ḫ→H, smart quotes, special punct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf52633",
   "metadata": {},
   "source": [
    "Check data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source length stats (chars):\")\n",
    "print(train_df[\"src\"].str.len().describe())\n",
    "print(f\"\\nSources > 384 chars: {(train_df['src'].str.len() > 384).sum()} ({(train_df['src'].str.len() > 384).mean()*100:.1f}%)\")\n",
    "print(f\"\\nTarget length stats (chars):\")\n",
    "print(train_df[\"tgt\"].str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ae373",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079d8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Load in BF16 directly — halves memory from the start, A100 computes BF16 natively\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {n_params:,}\")\n",
    "print(f\"Dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer vocab: {len(tokenizer)}\")\n",
    "\n",
    "# torch.compile: fuses ops, reduces kernel launch overhead (~10-15% speedup)\n",
    "# \"reduce-overhead\" mode best for small models with many short sequences\n",
    "try:\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    print(\"torch.compile: ENABLED (reduce-overhead)\")\n",
    "except Exception as e:\n",
    "    print(f\"torch.compile: SKIPPED ({e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9eb8e",
   "metadata": {},
   "source": [
    "Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    # text_target ensures decoder-side tokenization (adds decoder_start_token_id prefix)\n",
    "    # Without it, labels lack proper BOS handling for T5's decoder\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"], max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True, padding=False,\n",
    "        text_target=examples[\"tgt\"])\n",
    "    # text_target already populates model_inputs[\"labels\"] correctly\n",
    "    # Truncate target side separately\n",
    "    labels = model_inputs[\"labels\"]\n",
    "    model_inputs[\"labels\"] = [l[:MAX_TARGET_LENGTH] for l in labels]\n",
    "    return model_inputs\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "print(f\"Train dataset: {len(train_ds)}\")\n",
    "print(f\"Val dataset: {len(val_ds)}\")\n",
    "print(f\"Train sample keys: {train_ds[0].keys()}\")\n",
    "print(f\"Train sample input_ids length: {len(train_ds[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8975b",
   "metadata": {},
   "source": [
    "Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211833",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bleu_metric = BLEU()\n",
    "chrf_metric = CHRF(word_order=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = [p.strip() for p in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "    decoded_labels = [[l.strip()] for l in tokenizer.batch_decode(labels, skip_special_tokens=True)]\n",
    "    bleu_score = bleu_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    chrf_score = chrf_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    geo = (bleu_score * chrf_score) ** 0.5 if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "    return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo}\n",
    "\n",
    "print(\"Metrics defined: BLEU, chrF++ (word_order=2), geo_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34a97e",
   "metadata": {},
   "source": [
    "Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            avg = sum(self.losses) / len(self.losses)\n",
    "            print(f\"\\n--- Epoch {int(state.epoch)} avg train loss: {avg:.4f} ---\")\n",
    "            self.losses = []\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"  BLEU: {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"  chrF: {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"  Geo:  {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "\n",
    "class SampleCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, samples, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = samples[:3]\n",
    "        self.device = device\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "        model.eval()\n",
    "        print(\"\\nSample translations:\")\n",
    "        for i, src in enumerate(self.samples):\n",
    "            inputs = self.tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=384)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "            trans = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            print(f\"  [{i}] {src[:60]}...\")\n",
    "            print(f\"      -> {trans[:80]}\")\n",
    "\n",
    "print(\"Callbacks defined: LogCallback, SampleCallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd40fda",
   "metadata": {},
   "source": [
    "Configure training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # 64: no grad → fits easily\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    label_smoothing_factor=LABEL_SMOOTHING,\n",
    "    max_grad_norm=1.0,\n",
    "    # BF16: same exponent range as FP32 (8 bits), so ByT5's byte embeddings don't overflow\n",
    "    # Unlike FP16 (5-bit exponent, max 65504) which causes NaN with ByT5\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    # No gradient checkpointing: A100 40GB has plenty of VRAM at batch=32 + BF16\n",
    "    # Skipping saves ~20% training time by avoiding activation recomputation\n",
    "    gradient_checkpointing=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=4,\n",
    "    # Colab A100 has 12 CPU cores — use more workers for data loading\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    logging_steps=25,  # fewer steps per epoch at batch=32, log more frequently\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    # torch.compile wraps forward as (*args, **kwargs), which can break\n",
    "    # Trainer's signature-based column pruning. Keep dataset columns as-is.\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured (A100)\")\n",
    "print(f\"  Effective batch: {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"  BF16: ON (A100 native)\")\n",
    "print(f\"  Gradient checkpointing: OFF (40GB sufficient)\")\n",
    "print(f\"  TF32 matmul: ON\")\n",
    "print(f\"  torch.compile: ON\")\n",
    "print(f\"  LR: {LR} (cosine → 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370072d",
   "metadata": {},
   "source": [
    "Create trainer and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a45c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    LogCallback(),\n",
    "    SampleCallback(tokenizer, val_df[\"src\"].tolist(), device),\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE),\n",
    "]\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,  # renamed from `tokenizer` in transformers >= 4.46\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e4e75",
   "metadata": {},
   "source": [
    "Evaluate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c934c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"\\nFinal evaluation:\")\n",
    "print(f\"  BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"  chrF: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"  Geo:  {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e644c",
   "metadata": {},
   "source": [
    "Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = f\"{OUTPUT_DIR}/final\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "trainer.model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "# Save config for reference\n",
    "config_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_source_length\": MAX_SOURCE_LENGTH,\n",
    "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"effective_batch\": BATCH_SIZE * GRAD_ACCUM,\n",
    "    \"learning_rate\": LR,\n",
    "    \"bf16\": True,\n",
    "    \"normalization\": \"v7_preserve_diacritics\",\n",
    "}\n",
    "with open(f\"{final_dir}/v7_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {final_dir}/\")\n",
    "for f in sorted(os.listdir(final_dir)):\n",
    "    size = os.path.getsize(f\"{final_dir}/{f}\") / 1e6\n",
    "    print(f\"  {f} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdc1b8",
   "metadata": {},
   "source": [
    "Sanity check: Generate a sample translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = \"um-ma ka-ru-um\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_length=50, num_beams=4)\n",
    "translation = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"Sanity check:\")\n",
    "print(f\"  Input: '{test_input}'\")\n",
    "print(f\"  Output: '{translation}'\")\n",
    "assert translation.strip() != \"\", \"WARNING: Empty output!\"\n",
    "print(\"  OK - model produces non-empty output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2d37d",
   "metadata": {},
   "source": [
    "Upload instructions\n",
    "\n",
    "Upload the `outputs_v7/final/` folder to Kaggle as a Model or Dataset named `akkadian-v7-model`.\n",
    "\n",
    "Required files:\n",
    "- config.json\n",
    "- model.safetensors\n",
    "- tokenizer.json\n",
    "- tokenizer_config.json\n",
    "- special_tokens_map.json\n",
    "- v7_config.json"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
