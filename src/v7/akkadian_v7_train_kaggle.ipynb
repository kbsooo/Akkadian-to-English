{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a1e975",
   "metadata": {},
   "source": [
    "# V7 Training — ByT5-small Akkadian→English (Kaggle T4×2)\n",
    "\n",
    "**Environment:** Kaggle Notebook, GPU T4 × 2, 9h limit\n",
    "**Data:** `/kaggle/input/data-v7/` (pre-built V7 dataset)\n",
    "**Output:** `/kaggle/working/outputs_v7/final/` → upload as `akkadian-v7-model`\n",
    "\n",
    "### Multi-GPU Strategy\n",
    "HuggingFace Trainer uses DataParallel automatically when 2+ GPUs detected.\n",
    "- `per_device_train_batch_size` applies **per GPU**\n",
    "- Effective batch = per_device × n_gpus × grad_accum = 8 × 2 × 1 = 16\n",
    "- 2× throughput vs single-GPU with identical training dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d4162",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fee2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/byt5-small\"  # ~300M params (d_model=1472, 12+4 layers)\n",
    "DATA_DIR = \"/kaggle/input/data-v7\"\n",
    "OUTPUT_DIR = \"/kaggle/working/outputs_v7\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 384\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 8       # per GPU — with gradient checkpointing, fits in T4 16GB\n",
    "GRAD_ACCUM = 1       # effective batch = 8 × 2 GPUs × 1 = 16\n",
    "LR = 5e-5            # lower LR prevents overfitting on ~8K data\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.005\n",
    "EARLY_STOPPING_PATIENCE = 5  # cosine LR converges slowly, needs patience\n",
    "LABEL_SMOOTHING = 0.1\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbeaea",
   "metadata": {},
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5796093",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                       \"sacrebleu\", \"accelerate\"])\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep transformers on PyTorch path only, and reduce TF/XLA startup noise.\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38928b",
   "metadata": {},
   "source": [
    "## GPU Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"  [{i}] {name} — {mem:.1f} GB\")\n",
    "\n",
    "effective_batch = BATCH_SIZE * n_gpus * GRAD_ACCUM\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  per_device_batch = {BATCH_SIZE}\")\n",
    "print(f\"  n_gpus = {n_gpus}\")\n",
    "print(f\"  grad_accum = {GRAD_ACCUM}\")\n",
    "print(f\"  → effective batch = {effective_batch}\")\n",
    "print(f\"  epochs = {EPOCHS}, LR = {LR}\")\n",
    "print(f\"  label_smoothing = {LABEL_SMOOTHING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b21dd6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d833672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{DATA_DIR}/v7_train.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/v7_val.csv\")\n",
    "train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val:   {len(val_df):,}\")\n",
    "print(f\"\\nSource breakdown:\")\n",
    "print(train_df[\"source\"].value_counts().to_string())\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  src: {train_df['src'].iloc[0][:100]}...\")\n",
    "print(f\"  tgt: {train_df['tgt'].iloc[0][:100]}...\")\n",
    "\n",
    "# Check length distribution\n",
    "pct_over = (train_df[\"src\"].str.len() > MAX_SOURCE_LENGTH).mean() * 100\n",
    "print(f\"\\nSources > {MAX_SOURCE_LENGTH} chars: {pct_over:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3854bd7",
   "metadata": {},
   "source": [
    "## Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af911dca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29c0b5",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    # text_target ensures decoder-side tokenization with proper BOS token handling\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"], max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True, padding=False,\n",
    "        text_target=examples[\"tgt\"])\n",
    "    labels = model_inputs[\"labels\"]\n",
    "    model_inputs[\"labels\"] = [l[:MAX_TARGET_LENGTH] for l in labels]\n",
    "    return model_inputs\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"],\n",
    "                        num_proc=2)  # parallel tokenization on Kaggle's 4-core CPU\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"],\n",
    "                    num_proc=2)\n",
    "\n",
    "print(f\"Tokenized — Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "print(f\"Sample input_ids len: {len(train_ds[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac5318",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63cab3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bleu_metric = BLEU()\n",
    "chrf_metric = CHRF(word_order=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = [p.strip() for p in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "    decoded_labels = [[l.strip()] for l in tokenizer.batch_decode(labels, skip_special_tokens=True)]\n",
    "    bleu = bleu_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    chrf = chrf_metric.corpus_score(decoded_preds, decoded_labels).score\n",
    "    geo = (bleu * chrf) ** 0.5 if bleu > 0 and chrf > 0 else 0.0\n",
    "    return {\"bleu\": bleu, \"chrf\": chrf, \"geo_mean\": geo}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53573c4",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bcaf6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            avg = sum(self.losses) / len(self.losses)\n",
    "            print(f\"\\n--- Epoch {int(state.epoch)} avg train loss: {avg:.4f} ---\")\n",
    "            self.losses = []\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"  BLEU: {metrics.get('eval_bleu', 0):.2f}  \"\n",
    "                  f\"chrF: {metrics.get('eval_chrf', 0):.2f}  \"\n",
    "                  f\"Geo: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "\n",
    "\n",
    "class SampleCallback(TrainerCallback):\n",
    "    \"\"\"Generate sample translations on GPU 0 after each eval.\"\"\"\n",
    "    def __init__(self, tokenizer, samples):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = samples[:3]\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            return\n",
    "        # When Trainer wraps in DataParallel, unwrap to get the actual model\n",
    "        raw_model = model.module if hasattr(model, \"module\") else model\n",
    "        raw_model.eval()\n",
    "        device = next(raw_model.parameters()).device\n",
    "        print(\"\\nSample translations:\")\n",
    "        for i, src in enumerate(self.samples):\n",
    "            inputs = self.tokenizer(src, return_tensors=\"pt\", truncation=True,\n",
    "                                    max_length=MAX_SOURCE_LENGTH)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = raw_model.generate(**inputs, max_length=128, num_beams=4)\n",
    "            trans = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            print(f\"  [{i}] {src[:60]}...\")\n",
    "            print(f\"      → {trans[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f62ec",
   "metadata": {},
   "source": [
    "## Training Arguments (T4×2 Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fa170",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # eval uses less memory (no gradients)\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    label_smoothing_factor=LABEL_SMOOTHING,\n",
    "    max_grad_norm=1.0,\n",
    "    # ByT5 requires FP32 — FP16 causes NaN losses due to byte-level embeddings\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    # Gradient checkpointing: trades ~20% speed for ~30% VRAM savings\n",
    "    # Critical for batch=8 on T4 with ByT5's 384-token byte sequences\n",
    "    gradient_checkpointing=True,\n",
    "    # Eval & save\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    # Generation during eval\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=4,\n",
    "    # Kaggle has 4 CPU cores — use 2 workers per GPU for data loading\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    # Misc\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    # Multi-GPU: Trainer auto-detects DataParallel when CUDA_VISIBLE_DEVICES has 2+ GPUs\n",
    "    # No ddp_* args needed for Kaggle's simple 2-GPU setup\n",
    ")\n",
    "\n",
    "print(\"Training args configured\")\n",
    "print(f\"  Effective batch: {BATCH_SIZE} × {n_gpus} GPU × {GRAD_ACCUM} accum = {effective_batch}\")\n",
    "print(f\"  Gradient checkpointing: ON\")\n",
    "print(f\"  LR: {LR} (cosine → 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4a0e0",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d83a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    LogCallback(),\n",
    "    SampleCallback(tokenizer, val_df[\"src\"].tolist()),\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE),\n",
    "]\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"Starting training on {n_gpus} GPU(s)...\")\n",
    "print(f\"  Train samples: {len(train_ds):,}\")\n",
    "print(f\"  Steps/epoch: ~{len(train_ds) // effective_batch}\")\n",
    "print(f\"  Max epochs: {EPOCHS} (early stop patience={EARLY_STOPPING_PATIENCE})\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c523bd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf77424",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"\\nFinal evaluation:\")\n",
    "print(f\"  BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"  chrF: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"  Geo:  {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323988d0",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = f\"{OUTPUT_DIR}/final\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "# Unwrap DataParallel if needed\n",
    "save_model = trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model\n",
    "save_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "# Save training config for reproducibility\n",
    "config_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_source_length\": MAX_SOURCE_LENGTH,\n",
    "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "    \"epochs_trained\": int(trainer.state.epoch),\n",
    "    \"effective_batch_size\": effective_batch,\n",
    "    \"learning_rate\": LR,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"best_metric\": results.get(\"eval_geo_mean\", 0),\n",
    "    \"normalization\": \"v7_preserve_diacritics\",\n",
    "    \"n_gpus\": n_gpus,\n",
    "}\n",
    "with open(f\"{final_dir}/v7_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel saved to {final_dir}/\")\n",
    "for fname in sorted(os.listdir(final_dir)):\n",
    "    size = os.path.getsize(f\"{final_dir}/{fname}\") / 1e6\n",
    "    print(f\"  {fname} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd8693",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98305b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick generation test on the saved model\n",
    "save_model.eval()\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "save_model = save_model.to(device0)\n",
    "\n",
    "test_input = \"um-ma ka-ru-um\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device0)\n",
    "with torch.no_grad():\n",
    "    out = save_model.generate(**inputs, max_length=50, num_beams=4)\n",
    "translation = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  Input:  '{test_input}'\")\n",
    "print(f\"  Output: '{translation}'\")\n",
    "assert translation.strip() != \"\", \"Empty output!\"\n",
    "print(\"  OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa497cd",
   "metadata": {},
   "source": [
    "## Verify Output Files\n",
    "\n",
    "After training, create a **New Dataset** on Kaggle:\n",
    "1. Go to kaggle.com/datasets/new\n",
    "2. Upload all files from `/kaggle/working/outputs_v7/final/`\n",
    "3. Name it `akkadian-v7-model`\n",
    "4. Use as input in the inference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc527bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Best geo_mean: {results.get('eval_geo_mean', 0):.2f}\")\n",
    "print(f\"  BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"  chrF: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"  Epochs: {int(trainer.state.epoch)}\")\n",
    "print(f\"  GPUs used: {n_gpus}\")\n",
    "print(f\"\\n  Output: {final_dir}/\")\n",
    "print(f\"  Files:\")\n",
    "for fname in sorted(os.listdir(final_dir)):\n",
    "    print(f\"    - {fname}\")\n",
    "print(f\"\\n  → Upload '{final_dir}/' as Kaggle Dataset 'akkadian-v7-model'\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
