{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b860912",
   "metadata": {},
   "source": [
    "# V7 Inference — Akkadian→English (Kaggle T4×2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9675ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_DATASET = \"akkadian-v7-model\"  # Kaggle dataset name containing model weights\n",
    "ONOMASTICON_DATASET = \"deeppast/old-assyrian-grammars-and-other-resources\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 384\n",
    "MBR_N_CANDIDATES = 8  # Number of beam candidates for MBR decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadedb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and GPU setup\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "import difflib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"GPUs available: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    mem = torch.cuda.get_device_properties(i).total_mem / 1e9\n",
    "    print(f\"         VRAM: {mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find paths (robust for Kaggle)\n",
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "KAGGLE_WORKING = Path(\"/kaggle/working\")\n",
    "\n",
    "# Find competition data\n",
    "COMP_DIR = None\n",
    "for d in KAGGLE_INPUT.iterdir():\n",
    "    if (d / \"test.csv\").exists():\n",
    "        COMP_DIR = d\n",
    "        break\n",
    "assert COMP_DIR is not None, f\"Competition data not found in {KAGGLE_INPUT}\"\n",
    "print(f\"Competition data: {COMP_DIR}\")\n",
    "\n",
    "# Find model\n",
    "MODEL_DIR = None\n",
    "for d in KAGGLE_INPUT.iterdir():\n",
    "    if MODEL_DATASET.replace(\"-\", \"\") in d.name.replace(\"-\", \"\"):\n",
    "        # Search for config.json\n",
    "        if (d / \"config.json\").exists():\n",
    "            MODEL_DIR = d\n",
    "        else:\n",
    "            for sub in d.rglob(\"config.json\"):\n",
    "                MODEL_DIR = sub.parent\n",
    "                break\n",
    "        if MODEL_DIR:\n",
    "            break\n",
    "assert MODEL_DIR is not None, f\"Model not found in {KAGGLE_INPUT}\"\n",
    "print(f\"Model: {MODEL_DIR}\")\n",
    "print(f\"Model files: {sorted(os.listdir(MODEL_DIR))}\")\n",
    "\n",
    "# Find Onomasticon\n",
    "ONOMASTICON_PATH = None\n",
    "for d in KAGGLE_INPUT.iterdir():\n",
    "    for f in d.rglob(\"Onomasticon*\"):\n",
    "        ONOMASTICON_PATH = f\n",
    "        break\n",
    "    if ONOMASTICON_PATH:\n",
    "        break\n",
    "print(f\"Onomasticon: {ONOMASTICON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR), local_files_only=True)\n",
    "print(f\"  Tokenizer vocab: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR), local_files_only=True)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Model vocab: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multi-GPU\n",
    "model.eval()\n",
    "\n",
    "if n_gpus >= 2:\n",
    "    print(\"Setting up dual-GPU inference...\")\n",
    "    model_0 = model.to('cuda:0')\n",
    "    # Reload from disk instead of deepcopy — avoids doubling CPU memory peak\n",
    "    # and is more reliable across different model architectures\n",
    "    model_1 = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR), local_files_only=True)\n",
    "    model_1 = model_1.eval().to('cuda:1')\n",
    "    print(f\"  Model on cuda:0: OK\")\n",
    "    print(f\"  Model on cuda:1: OK (loaded from disk)\")\n",
    "    del model  # Free CPU memory\n",
    "else:\n",
    "    print(\"Single GPU mode\")\n",
    "    model_0 = model.to('cuda:0')\n",
    "    model_1 = None\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(n_gpus):\n",
    "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    print(f\"  GPU {i} memory allocated: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "test_input = \"um-ma ka-ru-um\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to('cuda:0')\n",
    "with torch.no_grad():\n",
    "    out = model_0.generate(**inputs, max_length=50, num_beams=4)\n",
    "test_output = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(f\"Sanity check:\")\n",
    "print(f\"  Input: '{test_input}'\")\n",
    "print(f\"  Output: '{test_output}'\")\n",
    "assert test_output.strip() != \"\", \"Model produces empty output!\"\n",
    "print(\"  OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5711a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "print(f\"text_ids: {test_df['text_id'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebd9ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# V7 Normalization function (EXACT)\n",
    "_V7_TRANS_TABLE = str.maketrans({\n",
    "    \"Ḫ\": \"H\", \"ḫ\": \"h\",\n",
    "    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\",\n",
    "    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\",\n",
    "    \"„\": '\"', \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\", \"ʾ\": \"'\", \"ʿ\": \"'\",\n",
    "})\n",
    "\n",
    "def normalize_transliteration_v7(text: str) -> str:\n",
    "    \"\"\"V7 normalization: preserves š, ṣ, ṭ and vowel accents. Only Ḫ→H.\"\"\"\n",
    "    if not text or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Standardize determinative braces: {} → () to match test format\n",
    "    text = text.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "    # Protect existing gap tokens\n",
    "    text = text.replace(\"<gap>\", \"\\x00GAP\\x00\")\n",
    "    text = text.replace(\"<big_gap>\", \"\\x00BIGGAP\\x00\")\n",
    "    # Remove apostrophe line numbers (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "    # Remove angle-bracket content markers (keep content)\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "    # Large gaps\n",
    "    text = re.sub(r\"\\[\\s*…+\\s*…*\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\]\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    text = text.replace(\"…\", \" \\x00BIGGAP\\x00 \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" \\x00BIGGAP\\x00 \", text)\n",
    "    # Single gap: [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" \\x00GAP\\x00 \", text, flags=re.IGNORECASE)\n",
    "    # Strip square brackets, keep content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "    # Half brackets / editorial marks\n",
    "    for c in \"‹›⌈⌉⌊⌋˹˺\":\n",
    "        text = text.replace(c, \"\")\n",
    "    # Character map (diacritics preserved except Ḫ→H)\n",
    "    text = text.translate(_V7_TRANS_TABLE)\n",
    "    # Scribal notations\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    # Standalone x → gap\n",
    "    text = re.sub(r\"(?<![a-zA-Z\\x00])\\bx\\b(?![a-zA-Z])\", \" \\x00GAP\\x00 \", text)\n",
    "    # Restore gap tokens\n",
    "    text = text.replace(\"\\x00GAP\\x00\", \"<gap>\")\n",
    "    text = text.replace(\"\\x00BIGGAP\\x00\", \"<big_gap>\")\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2609881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize test data\n",
    "normalized = [normalize_transliteration_v7(t) for t in test_df[\"transliteration\"]]\n",
    "print(f\"Normalized {len(normalized)} samples\")\n",
    "for i, (orig, norm) in enumerate(zip(test_df[\"transliteration\"], normalized)):\n",
    "    if i < 3:  # Show first 3\n",
    "        print(f\"  [{i}] Original: {str(orig)[:80]}...\")\n",
    "        print(f\"       Normalized: {norm[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a6cc9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define MBR decoding\n",
    "from sacrebleu.metrics import CHRF\n",
    "_chrf_scorer = CHRF(word_order=2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mbr_decode(model, tokenizer, source_text, n=8, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    Generate n diverse candidates via sampling, pick consensus translation.\n",
    "\n",
    "    Beam search produces near-identical candidates (low diversity).\n",
    "    Sampling with temperature creates genuinely different translations,\n",
    "    which is essential for MBR consensus to work effectively.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(source_text, return_tensors=\"pt\", truncation=True,\n",
    "                       max_length=MAX_SOURCE_LENGTH).to(device)\n",
    "\n",
    "    try:\n",
    "        # Sampling-based generation for diverse candidates\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=n,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        # OOM fallback: reduce candidates, use greedy\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"  OOM in MBR, falling back to greedy for: '{source_text[:40]}...'\")\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                num_beams=1,\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    candidates = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "    # Remove empty/duplicate candidates\n",
    "    candidates = list(dict.fromkeys([c for c in candidates if c.strip()]))\n",
    "    if len(candidates) == 0:\n",
    "        return \"\"\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "\n",
    "    # Score each candidate against all others using chrF++\n",
    "    # Precompute all pairwise scores to avoid O(n²) redundant calls\n",
    "    n_cand = len(candidates)\n",
    "    scores = np.zeros(n_cand)\n",
    "    for i in range(n_cand):\n",
    "        for j in range(n_cand):\n",
    "            if i != j:\n",
    "                scores[i] += _chrf_scorer.corpus_score([candidates[i]], [[candidates[j]]]).score\n",
    "        scores[i] /= (n_cand - 1)\n",
    "\n",
    "    return candidates[int(np.argmax(scores))]\n",
    "\n",
    "# Test MBR\n",
    "test_mbr = mbr_decode(model_0, tokenizer, normalized[0], n=MBR_N_CANDIDATES, device='cuda:0')\n",
    "print(f\"MBR test: '{test_mbr[:100]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with multi-GPU\n",
    "def translate_chunk(model, tokenizer, texts, device, desc=\"\"):\n",
    "    \"\"\"Translate a list of texts using MBR decoding on a specific GPU.\"\"\"\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=desc):\n",
    "        trans = mbr_decode(model, tokenizer, text, n=MBR_N_CANDIDATES, device=device)\n",
    "        results.append(trans)\n",
    "    return results\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if model_1 is not None and len(normalized) > 1:\n",
    "    # Split data for dual-GPU\n",
    "    mid = len(normalized) // 2\n",
    "    chunk_0 = normalized[:mid]\n",
    "    chunk_1 = normalized[mid:]\n",
    "    \n",
    "    print(f\"Dual-GPU inference: {len(chunk_0)} + {len(chunk_1)} samples\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        future_0 = executor.submit(translate_chunk, model_0, tokenizer, chunk_0, 'cuda:0', \"GPU-0\")\n",
    "        future_1 = executor.submit(translate_chunk, model_1, tokenizer, chunk_1, 'cuda:1', \"GPU-1\")\n",
    "        results_0 = future_0.result()\n",
    "        results_1 = future_1.result()\n",
    "    \n",
    "    translations = results_0 + results_1\n",
    "else:\n",
    "    print(f\"Single-GPU inference: {len(normalized)} samples\")\n",
    "    translations = translate_chunk(model_0, tokenizer, normalized, 'cuda:0', \"Translating\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nInference done in {elapsed:.0f}s ({elapsed/60:.1f}min)\")\n",
    "print(f\"Avg per sample: {elapsed/len(normalized):.1f}s\")\n",
    "\n",
    "# Show raw translations\n",
    "for i, t in enumerate(translations[:3]):\n",
    "    print(f\"  [{i}] {t[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70527b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model_0\n",
    "if model_1 is not None:\n",
    "    del model_1\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376d705",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Rule-based post-processing\n",
    "def rule_based_postprocess(translation: str) -> str:\n",
    "    if not translation:\n",
    "        return \"\"\n",
    "    # Gap marker normalization\n",
    "    translation = re.sub(r'\\b(?:gap)\\b(?!\\s*>)', '<gap>', translation, flags=re.IGNORECASE)\n",
    "    translation = re.sub(r'\\[(gap)\\]', '<gap>', translation, flags=re.IGNORECASE)\n",
    "    translation = re.sub(r'\\b(?:big[_ ]gap|large[_ ]gap)\\b', '<big_gap>', translation, flags=re.IGNORECASE)\n",
    "    translation = re.sub(r'(<gap>\\s*){2,}', '<big_gap> ', translation)\n",
    "    # Number normalization\n",
    "    translation = re.sub(r'\\bone[- ]third\\b', '0.33333', translation, flags=re.IGNORECASE)\n",
    "    translation = re.sub(r'\\btwo[- ]thirds?\\b', '0.66666', translation, flags=re.IGNORECASE)\n",
    "    translation = re.sub(r'\\bone[- ]half\\b', '0.5', translation, flags=re.IGNORECASE)\n",
    "    # Whitespace/punctuation\n",
    "    translation = re.sub(r'\\s+([.,;:!?)])', r'\\1', translation)\n",
    "    translation = re.sub(r'([(\\[])\\s+', r'\\1', translation)\n",
    "    translation = re.sub(r'\\s{2,}', ' ', translation)\n",
    "    translation = re.sub(r'\"{2,}', '\"', translation)\n",
    "    return translation.strip()\n",
    "\n",
    "translations = [rule_based_postprocess(t) for t in translations]\n",
    "print(\"Rule-based post-processing done\")\n",
    "for i, t in enumerate(translations[:3]):\n",
    "    print(f\"  [{i}] {t[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53553e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Onomasticon name repair\n",
    "def load_onomasticon(path):\n",
    "    if path is None:\n",
    "        return {}\n",
    "    df = pd.read_csv(path)\n",
    "    # Try common column names\n",
    "    col = df.columns[0]\n",
    "    names = df[col].dropna().str.strip().unique().tolist()\n",
    "    return {n.lower(): n for n in names if isinstance(n, str) and len(n) > 1}\n",
    "\n",
    "def repair_names(translation, onomasticon):\n",
    "    if not onomasticon or not translation:\n",
    "        return translation\n",
    "    words = translation.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if not word or not word[0].isupper():\n",
    "            result.append(word)\n",
    "            continue\n",
    "        stripped = word.rstrip('.,;:!?\"\\')]')\n",
    "        suffix = word[len(stripped):]\n",
    "        low = stripped.lower()\n",
    "        if low in onomasticon:\n",
    "            result.append(onomasticon[low] + suffix)\n",
    "            continue\n",
    "        matches = difflib.get_close_matches(low, onomasticon.keys(), n=1, cutoff=0.85)\n",
    "        if matches:\n",
    "            result.append(onomasticon[matches[0]] + suffix)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "onomasticon = load_onomasticon(ONOMASTICON_PATH)\n",
    "print(f\"Onomasticon loaded: {len(onomasticon)} names\")\n",
    "if onomasticon:\n",
    "    sample_items = dict(list(onomasticon.items())[:5])\n",
    "    print(f\"  Sample: {sample_items}\")\n",
    "\n",
    "translations = [repair_names(t, onomasticon) for t in translations]\n",
    "print(\"Name repair done\")\n",
    "for i, t in enumerate(translations[:3]):\n",
    "    print(f\"  [{i}] {t[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a97a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document consistency\n",
    "def enforce_consistency(translations, text_ids):\n",
    "    doc_groups = defaultdict(list)\n",
    "    for i, tid in enumerate(text_ids):\n",
    "        doc_groups[tid].append(i)\n",
    "    \n",
    "    for tid, indices in doc_groups.items():\n",
    "        if len(indices) <= 1:\n",
    "            continue\n",
    "        name_counts = Counter()\n",
    "        for idx in indices:\n",
    "            for w in translations[idx].split():\n",
    "                if w and w[0].isupper() and len(w) > 2:\n",
    "                    name_counts[w] += 1\n",
    "        canonical = {}\n",
    "        used = set()\n",
    "        for name in sorted(name_counts, key=name_counts.get, reverse=True):\n",
    "            if name.lower() in used:\n",
    "                continue\n",
    "            for other in name_counts:\n",
    "                if other.lower() != name.lower() and other.lower() not in used:\n",
    "                    if difflib.SequenceMatcher(None, name.lower(), other.lower()).ratio() > 0.8:\n",
    "                        canonical[other] = name\n",
    "                        used.add(other.lower())\n",
    "            used.add(name.lower())\n",
    "        for idx in indices:\n",
    "            for wrong, right in canonical.items():\n",
    "                # Word boundary matching to avoid partial replacement (e.g. \"Aššur\" inside \"Puzur-Aššur\")\n",
    "                translations[idx] = re.sub(r'\\b' + re.escape(wrong) + r'\\b', right, translations[idx])\n",
    "    \n",
    "    return translations\n",
    "\n",
    "text_ids = test_df[\"text_id\"].tolist()\n",
    "translations = enforce_consistency(translations, text_ids)\n",
    "print(\"Document consistency enforced\")\n",
    "for i, t in enumerate(translations[:3]):\n",
    "    print(f\"  [{i}] {t[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4933857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation and save submission\n",
    "# Replace any empty translations\n",
    "for i, t in enumerate(translations):\n",
    "    if not t or t.strip() == \"\":\n",
    "        translations[i] = \"<gap>\"\n",
    "        print(f\"  WARNING: Empty translation at index {i}, replaced with <gap>\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Validate\n",
    "assert len(submission) == len(test_df), f\"Row count mismatch: {len(submission)} vs {len(test_df)}\"\n",
    "assert submission[\"translation\"].notna().all(), \"NaN values found!\"\n",
    "assert (submission[\"translation\"].str.len() > 0).all(), \"Empty translations found!\"\n",
    "print(\"Validation passed\")\n",
    "\n",
    "output_path = KAGGLE_WORKING / \"submission.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "print(f\"\\nSubmission saved to {output_path}\")\n",
    "print(f\"  Rows: {len(submission)}\")\n",
    "print(submission)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
