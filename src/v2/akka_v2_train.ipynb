{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e502fbfc",
   "metadata": {},
   "source": [
    "# Akkadian V2 Training (Colab Version)\n",
    "\n",
    "**Key Changes from V1:**\n",
    "- Unified ASCII normalization (Train/Test style mismatch fixed)\n",
    "- All diacritics converted to ASCII (≈°‚Üís, √†‚Üía, etc.)\n",
    "\n",
    "**Environment**: Google Colab with GPU\n",
    "\n",
    "**Output**: Saved to Google Drive `/content/drive/MyDrive/akkadian/v2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dc8f6",
   "metadata": {},
   "source": [
    "## 0. Setup: Kaggle Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01045e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub login and data download\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Kaggle\n",
    "kbsooo_akkadian_v2_data_path = kagglehub.dataset_download('kbsooo/akkadian-v2-data')\n",
    "print(f'Data downloaded to: {kbsooo_akkadian_v2_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2116d",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04edef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc53d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for Colab GPU.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = \"google/byt5-base\"\n",
    "    \n",
    "    # Paths (Colab + Google Drive)\n",
    "    data_dir: Path = None  # Set after kagglehub download\n",
    "    output_dir: Path = Path(\"/content/drive/MyDrive/akkadian/v2\")\n",
    "\n",
    "    # Data selection\n",
    "    data_variant: str = \"sentence\"  # \"sentence\" (recommended) or \"document\"\n",
    "    sentence_train_file: str = \"v2_sentence_train.csv\"\n",
    "    sentence_val_file: str = \"v2_sentence_val.csv\"\n",
    "    doc_train_file: str = \"v2_train_augmented_clean.csv\"\n",
    "    doc_val_file: str = \"v2_val.csv\"\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    max_source_length: int = 256   # Reduced from 512 to prevent overflow\n",
    "    max_target_length: int = 256   # Reduced from 512\n",
    "    batch_size: int = 4            # Increased from 2\n",
    "    gradient_accumulation_steps: int = 4  # Reduced from 8\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-4    # Reduced from 3e-4\n",
    "    warmup_ratio: float = 0.1      # Increased from 0.05\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Hardware - FP16 DISABLED to prevent ByT5 overflow!\n",
    "    fp16: bool = False             # Changed from True - ByT5 is unstable with FP16\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# Set data directory from kagglehub download\n",
    "CFG.data_dir = Path(kbsooo_akkadian_v2_data_path)\n",
    "\n",
    "# Ensure output directory exists\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Data directory: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output directory: {CFG.output_dir}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproducibility\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf18dd",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_data_paths() -> tuple[Path, Path]:\n",
    "    \"\"\"Choose sentence-level data if available; fall back to document-level.\"\"\"\n",
    "    if CFG.data_variant == \"sentence\":\n",
    "        train_path = CFG.data_dir / CFG.sentence_train_file\n",
    "        val_path = CFG.data_dir / CFG.sentence_val_file\n",
    "        if train_path.exists() and val_path.exists():\n",
    "            return train_path, val_path\n",
    "        print(\"‚ö†Ô∏è Sentence-level files not found. Falling back to document-level.\")\n",
    "    return CFG.data_dir / CFG.doc_train_file, CFG.data_dir / CFG.doc_val_file\n",
    "\n",
    "\n",
    "print(\"üìñ Loading preprocessed data...\")\n",
    "train_path, val_path = resolve_data_paths()\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "\n",
    "required_cols = {\"src\", \"tgt\"}\n",
    "if not required_cols.issubset(train_df.columns):\n",
    "    missing = required_cols - set(train_df.columns)\n",
    "    raise ValueError(f\"Missing columns in train data: {missing}\")\n",
    "if not required_cols.issubset(val_df.columns):\n",
    "    missing = required_cols - set(val_df.columns)\n",
    "    raise ValueError(f\"Missing columns in val data: {missing}\")\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"   Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"   src: {train_df.iloc[0]['src'][:80]}...\")\n",
    "print(f\"   tgt: {train_df.iloc[0]['tgt'][:80]}...\")\n",
    "\n",
    "# Truncation risk check\n",
    "src_over = (train_df[\"src\"].str.len() > CFG.max_source_length).mean()\n",
    "tgt_over = (train_df[\"tgt\"].str.len() > CFG.max_target_length).mean()\n",
    "print(f\"\\n‚ö†Ô∏è Truncation risk (train): src>{CFG.max_source_length}: {src_over:.1%}, tgt>{CFG.max_target_length}: {tgt_over:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80a90e",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40a41e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"ü§ñ Loading model: {CFG.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ‚úÖ Gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467a96e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    # ByT5: encoder/decoder share same vocab, no as_target_tokenizer() needed\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"], desc=\"Tokenizing train\")\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"], desc=\"Tokenizing val\")\n",
    "\n",
    "print(f\"   Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537585e5",
   "metadata": {},
   "source": [
    "## 4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485b312",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "        \n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ea020",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f825bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for cleaner training logs.\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if state.log_history:\n",
    "            last_log = state.log_history[-1]\n",
    "            epoch = last_log.get('epoch', 0)\n",
    "            train_loss = last_log.get('loss', 'N/A')\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"üìä Epoch {int(epoch)} Complete\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f}\" if isinstance(train_loss, float) else f\"   Train Loss: {train_loss}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\nüìà Validation Results:\")\n",
    "            print(f\"   Loss: {metrics.get('eval_loss', 'N/A'):.4f}\" if metrics.get('eval_loss') else \"   Loss: N/A\")\n",
    "            print(f\"   BLEU: {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF++: {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo Mean: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "            print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    fp16=CFG.fp16 and torch.cuda.is_available(),\n",
    "    bf16=CFG.bf16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_bleu\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=100,  # Less frequent logging\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    "    disable_tqdm=False,  # Keep progress bar\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèãÔ∏è Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Google Drive\n",
    "final_dir = CFG.output_dir / \"final\"\n",
    "print(f\"\\nüíæ Saving to Google Drive: {final_dir}\")\n",
    "trainer.save_model(str(final_dir))\n",
    "tokenizer.save_pretrained(str(final_dir))\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìà Final evaluation:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"   BLEU: {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"   chrF++: {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"   Geo Mean: {results.get('eval_geo_mean', 0):.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"üìÅ Model saved to: {final_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a52bb",
   "metadata": {},
   "source": [
    "## 6. Create Model Archive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = CFG.output_dir / \"akkadian_v2_model\"\n",
    "shutil.make_archive(str(zip_path), 'zip', final_dir)\n",
    "print(f\"üì¶ Model archived: {zip_path}.zip\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
