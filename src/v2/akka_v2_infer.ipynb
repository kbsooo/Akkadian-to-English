{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80bcfaf",
   "metadata": {},
   "source": [
    "# Akkadian V2 Inference\n",
    "\n",
    "**Key Changes from V1:**\n",
    "- Unified ASCII normalization (same as training)\n",
    "- All diacritics converted to ASCII (≈°‚Üís, √†‚Üía, etc.)\n",
    "- Test data now uses same character set as training\n",
    "\n",
    "**Environment**: Kaggle T4 GPU x2\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v2/akka_v2_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741cc12",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bd857",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Inference params\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = True\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e20751",
   "metadata": {},
   "source": [
    "## 2. Normalization (MUST match training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78b965",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Vowels with diacritics ‚Üí base vowels\n",
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "# Akkadian consonants ‚Üí ASCII\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈°, ≈†\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£, ·π¢\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠, ·π¨\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´, ·∏™\n",
    "}\n",
    "\n",
    "# OCR artifacts\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "    '\\u2308': '[', '\\u2309': ']', '\\u230a': '[', '\\u230b': ']',\n",
    "}\n",
    "\n",
    "# Subscripts\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text):\n",
    "    \"\"\"Normalize to ASCII - MUST match training preprocessing.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06511c91",
   "metadata": {},
   "source": [
    "## 3. Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15147296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return Path(\"/kaggle/input\").exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    if not is_kaggle():\n",
    "        return Path(\"data\")\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"test.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_model_dir():\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"outputs/akkadian_v2/final\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "    \n",
    "    # Kaggle: find model dataset\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"config.json\").exists():\n",
    "            return d\n",
    "        for sub in d.glob(\"**/config.json\"):\n",
    "            return sub.parent\n",
    "    raise FileNotFoundError(\"Model not found in /kaggle/input\")\n",
    "\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model_dir()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779907a4",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dbaac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"ü§ñ Loading model from {MODEL_DIR}\")\n",
    "\n",
    "# ByT5 tokenizer with extra_ids to match training\n",
    "tokenizer = ByT5Tokenizer(extra_ids=125)\n",
    "print(f\"   Tokenizer len: {len(tokenizer)}\")  # Should be 384\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "\n",
    "# Ensure vocab sizes match\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Resized to: {model.config.vocab_size}\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if CFG.fp16 and device.type == \"cuda\":\n",
    "    model = model.half()\n",
    "    print(\"   ‚úÖ Using FP16\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943fb43",
   "metadata": {},
   "source": [
    "## 5. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4dc3ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translations.extend(generate_batch(batch))\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ff50b",
   "metadata": {},
   "source": [
    "## 6. Load Test Data & Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe24f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df)}\")\n",
    "\n",
    "# Normalize (CRITICAL: same as training)\n",
    "print(\"üîß Normalizing (ASCII conversion)...\")\n",
    "normalized = [normalize_transliteration(t) for t in test_df[\"transliteration\"]]\n",
    "\n",
    "print(\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(2, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfc9a5",
   "metadata": {},
   "source": [
    "## 7. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "assert len(submission) == len(test_df)\n",
    "assert submission[\"translation\"].notna().all()\n",
    "\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {output_path}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaa34f",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Submit `submission.csv` to the competition."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
