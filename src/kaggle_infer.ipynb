{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40859278",
   "metadata": {},
   "source": [
    "Kaggle Inference Template (use trained MPS model)\n",
    "\n",
    "Goal: load a trained checkpoint from Kaggle Dataset and create submission.csv\n",
    "\n",
    "Steps (Kaggle UI):\n",
    "1) Upload your trained model folder as a Kaggle Dataset\n",
    "2) Add it as an Input to this notebook\n",
    "3) Run all cells â†’ /kaggle/working/submission.csv\n",
    "\n",
    "If your model dataset name is, e.g., \"akkadian-byt5-mps\", set MODEL_DATASET_NAME below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e791fe",
   "metadata": {},
   "source": [
    "## 1. Paths & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f336c2b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "WORK_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# Set this to your Kaggle Dataset name that contains the trained model\n",
    "MODEL_DATASET_NAME = None  # e.g., \"akkadian-byt5-mps\"\n",
    "TOKENIZER_DIR: Optional[Path] = None  # set if you uploaded tokenizer separately\n",
    "\n",
    "# If None, we try to auto-detect a checkpoint inside /kaggle/input\n",
    "MODEL_DIR: Optional[Path] = None\n",
    "\n",
    "MAX_SRC_LEN = 256\n",
    "MAX_TGT_LEN = 256\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb26bb3",
   "metadata": {},
   "source": [
    "## 2. Locate competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de407db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_competition_data_dir() -> Path:\n",
    "    if not KAGGLE_INPUT.exists():\n",
    "        raise FileNotFoundError(\"/kaggle/input not found\")\n",
    "    for d in KAGGLE_INPUT.iterdir():\n",
    "        if (d / \"test.csv\").exists() and (d / \"sample_submission.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Could not locate competition dataset\")\n",
    "\n",
    "\n",
    "def find_model_dir() -> Path:\n",
    "    if MODEL_DATASET_NAME:\n",
    "        d = KAGGLE_INPUT / MODEL_DATASET_NAME\n",
    "        if not d.exists():\n",
    "            raise FileNotFoundError(f\"Model dataset not found: {d}\")\n",
    "        return d\n",
    "\n",
    "    # Auto-detect: look for config.json + model weights\n",
    "    for d in KAGGLE_INPUT.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        # check root\n",
    "        if (d / \"config.json\").exists():\n",
    "            return d\n",
    "        # check subdirs\n",
    "        for sub in d.glob(\"**/config.json\"):\n",
    "            return sub.parent\n",
    "\n",
    "    raise FileNotFoundError(\"Could not auto-detect model directory in /kaggle/input\")\n",
    "\n",
    "\n",
    "COMP_DATA_DIR = find_competition_data_dir()\n",
    "MODEL_DIR = MODEL_DIR or find_model_dir()\n",
    "\n",
    "print(\"Competition data:\", COMP_DATA_DIR)\n",
    "print(\"Model dir:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9818b",
   "metadata": {},
   "source": [
    "## 3. Normalization (match preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360881a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    \"\\u2080\": \"0\",\n",
    "    \"\\u2081\": \"1\",\n",
    "    \"\\u2082\": \"2\",\n",
    "    \"\\u2083\": \"3\",\n",
    "    \"\\u2084\": \"4\",\n",
    "    \"\\u2085\": \"5\",\n",
    "    \"\\u2086\": \"6\",\n",
    "    \"\\u2087\": \"7\",\n",
    "    \"\\u2088\": \"8\",\n",
    "    \"\\u2089\": \"9\",\n",
    "    \"\\u2093\": \"x\",\n",
    "})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace(\"\\u1E2A\", \"H\").replace(\"\\u1E2B\", \"h\")\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace(\"\\u2026\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", \" <gap> \", text)\n",
    "    text = re.sub(r\"\\bx\\b\", \" <unk_sign> \", text)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\" \\1 \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801ec36",
   "metadata": {},
   "source": [
    "## 4. Load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac03227",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Prefer tokenizer from model/tokenizer dir if present; fallback to ByT5 class.\n",
    "try:\n",
    "    if TOKENIZER_DIR and TOKENIZER_DIR.exists():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, local_files_only=True)\n",
    "    elif (MODEL_DIR / \"tokenizer_config.json\").exists() or (MODEL_DIR / \"tokenizer.json\").exists():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    else:\n",
    "        # If no tokenizer files, use ByT5 tokenizer with defaults (no internet needed)\n",
    "        tokenizer = ByT5Tokenizer()\n",
    "except Exception as exc:\n",
    "    print(f\"Tokenizer load failed ({exc}); falling back to ByT5Tokenizer().\")\n",
    "    tokenizer = ByT5Tokenizer()\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if device.type == \"cuda\":\n",
    "    model = model.half()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315dc565",
   "metadata": {},
   "source": [
    "## 5. Predict on test and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_generate(texts, batch_size=BATCH_SIZE, max_len=MAX_TGT_LEN):\n",
    "    outputs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SRC_LEN,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(**inputs, max_length=max_len)\n",
    "        outputs.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(COMP_DATA_DIR / \"test.csv\")\n",
    "assert {\"id\", \"transliteration\"}.issubset(test_df.columns)\n",
    "\n",
    "norm_texts = [normalize_transliteration(t) for t in test_df[\"transliteration\"].tolist()]\n",
    "\n",
    "preds = batch_generate(norm_texts)\n",
    "\n",
    "sub = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": preds})\n",
    "sub_path = WORK_DIR / \"submission.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "\n",
    "print(\"Saved:\", sub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6b511",
   "metadata": {},
   "source": [
    "## 6. Quick sanity check\n",
    "\n",
    "```\n",
    "!head /kaggle/working/submission.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
