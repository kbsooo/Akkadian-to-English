{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b61439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Akkadian V6 Training Script (Colab T4)\n",
    "=======================================\n",
    "Based on V5d with CRITICAL CHANGE: Tokenizer saved with model for Kaggle offline\n",
    "- Model: ByT5-small (300M params)\n",
    "- Data: V5d verified data (2,854 train + 319 val)\n",
    "- Device: CUDA (Colab T4)\n",
    "- CRITICAL: tokenizer.save_pretrained() added for Kaggle offline compatibility\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d09e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ce50e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4bd49",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Configuration\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Paths - ADJUST FOR COLAB\n",
    "    data_dir: Path = Path(\"data/v6\")  # V6 expanded data (8,308 samples)\n",
    "    output_dir: Path = Path(\"outputs/v6\")\n",
    "\n",
    "    # Model\n",
    "    model_name: str = \"google/byt5-small\"\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 15  # More data = more epochs\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4  # effective batch = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 3\n",
    "\n",
    "    # Sequence lengths\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "\n",
    "    # Glossary - ENABLED for V6 (domain shift mitigation)\n",
    "    use_glossary: bool = True  # Re-enabled for better public score\n",
    "    glossary_max_items: int = 8\n",
    "    glossary_drop_prob: float = 0.5\n",
    "\n",
    "    # Misc\n",
    "    seed: int = 42\n",
    "    logging_steps: int = 50\n",
    "    save_total_limit: int = 3\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c783f",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Device Setup\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get best available device: CUDA > MPS > CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96172c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V6 Training (Colab T4)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Data: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üéÆ Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif DEVICE.type == \"mps\":\n",
    "    print(\"   Apple Silicon MPS acceleration\")\n",
    "print(f\"üß† Glossary: {'Enabled' if CFG.use_glossary else 'Disabled'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d0913",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b98697",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Normalization (same as V5d - MUST match infer!)\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOWEL_MAP = {\n",
    "    \"√†\": \"a\", \"√°\": \"a\", \"√¢\": \"a\", \"ƒÅ\": \"a\", \"√§\": \"a\",\n",
    "    \"√Ä\": \"A\", \"√Å\": \"A\", \"√Ç\": \"A\", \"ƒÄ\": \"A\", \"√Ñ\": \"A\",\n",
    "    \"√®\": \"e\", \"√©\": \"e\", \"√™\": \"e\", \"ƒì\": \"e\", \"√´\": \"e\",\n",
    "    \"√à\": \"E\", \"√â\": \"E\", \"√ä\": \"E\", \"ƒí\": \"E\", \"√ã\": \"E\",\n",
    "    \"√¨\": \"i\", \"√≠\": \"i\", \"√Æ\": \"i\", \"ƒ´\": \"i\", \"√Ø\": \"i\",\n",
    "    \"√å\": \"I\", \"√ç\": \"I\", \"√é\": \"I\", \"ƒ™\": \"I\", \"√è\": \"I\",\n",
    "    \"√≤\": \"o\", \"√≥\": \"o\", \"√¥\": \"o\", \"≈ç\": \"o\", \"√∂\": \"o\",\n",
    "    \"√í\": \"O\", \"√ì\": \"O\", \"√î\": \"O\", \"≈å\": \"O\", \"√ñ\": \"O\",\n",
    "    \"√π\": \"u\", \"√∫\": \"u\", \"√ª\": \"u\", \"≈´\": \"u\", \"√º\": \"u\",\n",
    "    \"√ô\": \"U\", \"√ö\": \"U\", \"√õ\": \"U\", \"≈™\": \"U\", \"√ú\": \"U\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a82bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONSONANT_MAP = {\n",
    "    \"≈°\": \"s\", \"≈†\": \"S\",\n",
    "    \"·π£\": \"s\", \"·π¢\": \"S\",\n",
    "    \"·π≠\": \"t\", \"·π¨\": \"T\",\n",
    "    \"·∏´\": \"h\", \"·∏™\": \"H\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_QUOTE_MAP = {\n",
    "    \"‚Äû\": '\"', \"\"\": '\"', \"\"\": '\"',\n",
    "    \"'\": \"'\", \"'\": \"'\", \"‚Äö\": \"'\",\n",
    "    \" æ\": \"'\", \" ø\": \"'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SUBSCRIPT_MAP = {\n",
    "    \"‚ÇÄ\": \"0\", \"‚ÇÅ\": \"1\", \"‚ÇÇ\": \"2\", \"‚ÇÉ\": \"3\", \"‚ÇÑ\": \"4\",\n",
    "    \"‚ÇÖ\": \"5\", \"‚ÇÜ\": \"6\", \"‚Çá\": \"7\", \"‚Çà\": \"8\", \"‚Çâ\": \"9\",\n",
    "    \"‚Çì\": \"x\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62dedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all character maps\n",
    "_ALL_CHAR_MAP = {**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP, **_SUBSCRIPT_MAP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1bd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build translation table safely\n",
    "_TRANS_TABLE = {}\n",
    "for k, v in _ALL_CHAR_MAP.items():\n",
    "    if isinstance(k, str) and len(k) == 1:\n",
    "        _TRANS_TABLE[ord(k)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfaf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration - MUST match infer exactly!\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Protect literal gap tokens\n",
    "    text = text.replace(\"<gap>\", \"__LIT_GAP__\")\n",
    "    text = text.replace(\"<big_gap>\", \"__LIT_BIG_GAP__\")\n",
    "\n",
    "    # Remove apostrophe line numbers only (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "\n",
    "    # Remove <content> blocks first\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "\n",
    "    # Large gaps\n",
    "    text = re.sub(r\"\\[\\s*‚Ä¶+\\s*‚Ä¶*\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # Ellipsis\n",
    "    text = text.replace(\"‚Ä¶\", \" __BIG_GAP__ \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # [content] -> content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "\n",
    "    # Half brackets\n",
    "    for char in \"‚Äπ‚Ä∫‚åà‚åâ‚åä‚åãÀπÀ∫\":\n",
    "        text = text.replace(char, \"\")\n",
    "\n",
    "    # Character maps\n",
    "    text = text.translate(_TRANS_TABLE)\n",
    "\n",
    "    # Scribal notations / word divider\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "\n",
    "    # Standalone x\n",
    "    text = re.sub(r\"\\bx\\b\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Convert placeholders\n",
    "    text = text.replace(\"__GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Restore literal tokens\n",
    "    text = text.replace(\"__LIT_GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__LIT_BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Cleanup\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d48183",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Glossary (optional - disabled by default)\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d907e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_SPLIT_RE = re.compile(r\"[\\s\\-]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_src(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return [t for t in SRC_SPLIT_RE.split(str(text)) if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fe312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glossary(path: Path) -> dict[str, list[str]]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {k: list(v) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glossary_prompt(\n",
    "    src: str,\n",
    "    glossary: dict[str, list[str]],\n",
    "    max_items: int,\n",
    "    drop_prob: float,\n",
    "    rng: random.Random,\n",
    ") -> str:\n",
    "    \"\"\"Build glossary-augmented prompt for training.\"\"\"\n",
    "    if not glossary:\n",
    "        return src\n",
    "    if drop_prob > 0 and rng.random() < drop_prob:\n",
    "        return src\n",
    "\n",
    "    items = []\n",
    "    used = set()\n",
    "    for tok in tokenize_src(src):\n",
    "        if tok in used:\n",
    "            continue\n",
    "        tgts = glossary.get(tok)\n",
    "        if not tgts:\n",
    "            continue\n",
    "        items.append(f\"{tok}={tgts[0]}\")\n",
    "        used.add(tok)\n",
    "        if len(items) >= max_items:\n",
    "            break\n",
    "\n",
    "    if not items:\n",
    "        return src\n",
    "\n",
    "    return \"GLOSSARY: \" + \"; \".join(items) + \" ||| \" + src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31a1f3",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Data Loading\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if not {\"src\", \"tgt\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing src/tgt columns: {path}\")\n",
    "    df = df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, glossary: dict, drop_prob: float, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"Normalize and add glossary prompts.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize source (V5d data already normalized, but re-normalize for safety)\n",
    "    df[\"src\"] = df[\"src\"].apply(normalize_transliteration)\n",
    "\n",
    "    # Add glossary prompts\n",
    "    if CFG.use_glossary and glossary:\n",
    "        df[\"src\"] = [\n",
    "            build_glossary_prompt(src, glossary, CFG.glossary_max_items, drop_prob, rng)\n",
    "            for src in df[\"src\"]\n",
    "        ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420af280",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Metrics\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "\n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "\n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c5d49",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Callbacks\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.losses = []\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Epoch {self.epoch + 1}/{args.num_train_epochs}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            print(f\"\\nüìâ Train Loss: {sum(self.losses)/len(self.losses):.4f}\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"üìà Validation\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "            print(f\"   BLEU: {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF: {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo:  {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "            print(\"‚îÄ\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc788a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleOutputCallback(TrainerCallback):\n",
    "    \"\"\"Print sample translations during validation.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, val_samples: list[str], device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.val_samples = val_samples[:3]  # First 3 samples\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None or not self.val_samples:\n",
    "            return\n",
    "\n",
    "        model.eval()\n",
    "        print(\"\\nüìù Sample Translations:\")\n",
    "\n",
    "        for i, src in enumerate(self.val_samples):\n",
    "            inputs = self.tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "\n",
    "            translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"   [{i}] {src[:50]}...\")\n",
    "            print(f\"       ‚Üí {translation[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29abf1",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "Main Training\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\nüìñ Loading data...\")\n",
    "\n",
    "    train_df = load_data(CFG.data_dir / \"v6_train.csv\")\n",
    "    val_df = load_data(CFG.data_dir / \"v6_val.csv\")\n",
    "\n",
    "    print(f\"   Train: {len(train_df):,}\")\n",
    "    print(f\"   Val: {len(val_df):,}\")\n",
    "\n",
    "    # Load glossary (optional)\n",
    "    glossary = {}\n",
    "    if CFG.use_glossary:\n",
    "        glossary = load_glossary(CFG.data_dir / \"v6_glossary.json\")\n",
    "        print(f\"   Glossary: {len(glossary):,} entries\")\n",
    "    else:\n",
    "        print(\"   Glossary: Disabled\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_df = prepare_data(train_df, glossary, CFG.glossary_drop_prob, CFG.seed)\n",
    "    val_df = prepare_data(val_df, glossary, 0.0, CFG.seed)  # No drop for val\n",
    "\n",
    "    # Sample prompts\n",
    "    print(\"\\nüìù Sample prompts:\")\n",
    "    for i in range(min(2, len(train_df))):\n",
    "        print(f\"   [{i}] {train_df.iloc[i]['src'][:100]}...\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"\\nü§ñ Loading model: {CFG.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "    print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "    print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"src\"],\n",
    "            max_length=CFG.max_source_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            examples[\"tgt\"],\n",
    "            max_length=CFG.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    def to_dataset(df: pd.DataFrame) -> Dataset:\n",
    "        ds = Dataset.from_pandas(df[[\"src\", \"tgt\"]])\n",
    "        return ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "    print(\"\\nüîß Tokenizing...\")\n",
    "    train_ds = to_dataset(train_df)\n",
    "    val_ds = to_dataset(val_df)\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "    # Training arguments\n",
    "    # Force FP32 for ByT5 stability\n",
    "    use_fp16 = False  # ByT5 requires FP32\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "        num_train_epochs=CFG.epochs,\n",
    "        per_device_train_batch_size=CFG.batch_size,\n",
    "        per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "        gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        warmup_ratio=CFG.warmup_ratio,\n",
    "        max_grad_norm=CFG.max_grad_norm,\n",
    "        fp16=use_fp16,\n",
    "        bf16=False,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=CFG.save_total_limit,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_geo_mean\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CFG.max_target_length,\n",
    "        dataloader_num_workers=2,  # Colab T4 can use workers\n",
    "        logging_steps=CFG.logging_steps,\n",
    "        report_to=\"none\",\n",
    "        seed=CFG.seed,\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        LogCallback(),\n",
    "        SampleOutputCallback(tokenizer, val_df[\"src\"].tolist(), DEVICE),\n",
    "        EarlyStoppingCallback(early_stopping_patience=CFG.early_stopping_patience),\n",
    "    ]\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=build_compute_metrics(tokenizer),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"\\nüèÅ Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # ‚≠ê CRITICAL CHANGE: Save BOTH model AND tokenizer\n",
    "    final_dir = CFG.output_dir / \"final\"\n",
    "    final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    trainer.model.save_pretrained(str(final_dir))\n",
    "\n",
    "    # ‚≠ê CRITICAL: Save tokenizer for Kaggle offline\n",
    "    tokenizer.save_pretrained(str(final_dir))\n",
    "\n",
    "    # Save config for reference\n",
    "    config_info = {\n",
    "        \"model_name\": CFG.model_name,\n",
    "        \"max_source_length\": CFG.max_source_length,\n",
    "        \"max_target_length\": CFG.max_target_length,\n",
    "        \"use_glossary\": CFG.use_glossary,\n",
    "    }\n",
    "    with (final_dir / \"v6_config.json\").open(\"w\") as f:\n",
    "        json.dump(config_info, f, indent=2)\n",
    "\n",
    "    print(f\"\\nüíæ Model and tokenizer saved: {final_dir}\")\n",
    "    print(f\"   ‚úÖ Model saved\")\n",
    "    print(f\"   ‚úÖ Tokenizer saved (for Kaggle offline)\")\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: Upload ENTIRE '{final_dir}' folder to Kaggle Dataset\")\n",
    "    print(f\"   Required files:\")\n",
    "    print(f\"   - config.json\")\n",
    "    print(f\"   - model.safetensors (or pytorch_model.bin)\")\n",
    "    print(f\"   - tokenizer.json ‚≠ê\")\n",
    "    print(f\"   - special_tokens_map.json ‚≠ê\")\n",
    "    print(f\"   - tokenizer_config.json ‚≠ê\")\n",
    "\n",
    "    # Final evaluation\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"\\nüìà Final: BLEU={results.get('eval_bleu',0):.2f}, chrF={results.get('eval_chrf',0):.2f}, Geo={results.get('eval_geo_mean',0):.2f}\")\n",
    "\n",
    "    # Quick sanity check\n",
    "    print(\"\\nüîç Sanity check...\")\n",
    "    model.eval()\n",
    "    test_input = \"um-ma ka-ru-um\"\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50, num_beams=4)\n",
    "\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"   Input: '{test_input}'\")\n",
    "    print(f\"   Output: '{translation}'\")\n",
    "\n",
    "    if not translation or translation == \"\":\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Empty output! Model may not have trained properly.\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Model produces non-empty output\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"‚úÖ V6 Training Complete!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
