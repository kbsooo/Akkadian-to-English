{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd274871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64200741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "V5c ByT5-small inference (Kaggle internet-off safe):\n",
    "- Never downloads anything from HF at runtime.\n",
    "- Deterministic tokenizer derived from model.config.vocab_size.\n",
    "- Retrieval + glossary prompting if assets provided.\n",
    "- Non-empty guarantee: never outputs empty/`...`/punct-only strings.\n",
    "\n",
    "Kaggle usage:\n",
    "  python src/v5c/akkadian_v5c_small_infer.py --model-dir /kaggle/input/<your-model>/pytorch/default/1 --assets-dir /kaggle/input/<your-assets>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2798614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716e3c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb2c9f",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "V5 normalization (inline)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOWEL_MAP = {\n",
    "    \"\\u00e0\": \"a\",\n",
    "    \"\\u00e1\": \"a\",\n",
    "    \"\\u00e2\": \"a\",\n",
    "    \"\\u0101\": \"a\",\n",
    "    \"\\u00e4\": \"a\",\n",
    "    \"\\u00c0\": \"A\",\n",
    "    \"\\u00c1\": \"A\",\n",
    "    \"\\u00c2\": \"A\",\n",
    "    \"\\u0100\": \"A\",\n",
    "    \"\\u00c4\": \"A\",\n",
    "    \"\\u00e8\": \"e\",\n",
    "    \"\\u00e9\": \"e\",\n",
    "    \"\\u00ea\": \"e\",\n",
    "    \"\\u0113\": \"e\",\n",
    "    \"\\u00eb\": \"e\",\n",
    "    \"\\u00c8\": \"E\",\n",
    "    \"\\u00c9\": \"E\",\n",
    "    \"\\u00ca\": \"E\",\n",
    "    \"\\u0112\": \"E\",\n",
    "    \"\\u00cb\": \"E\",\n",
    "    \"\\u00ec\": \"i\",\n",
    "    \"\\u00ed\": \"i\",\n",
    "    \"\\u00ee\": \"i\",\n",
    "    \"\\u012b\": \"i\",\n",
    "    \"\\u00ef\": \"i\",\n",
    "    \"\\u00cc\": \"I\",\n",
    "    \"\\u00cd\": \"I\",\n",
    "    \"\\u00ce\": \"I\",\n",
    "    \"\\u012a\": \"I\",\n",
    "    \"\\u00cf\": \"I\",\n",
    "    \"\\u00f2\": \"o\",\n",
    "    \"\\u00f3\": \"o\",\n",
    "    \"\\u00f4\": \"o\",\n",
    "    \"\\u014d\": \"o\",\n",
    "    \"\\u00f6\": \"o\",\n",
    "    \"\\u00d2\": \"O\",\n",
    "    \"\\u00d3\": \"O\",\n",
    "    \"\\u00d4\": \"O\",\n",
    "    \"\\u014c\": \"O\",\n",
    "    \"\\u00d6\": \"O\",\n",
    "    \"\\u00f9\": \"u\",\n",
    "    \"\\u00fa\": \"u\",\n",
    "    \"\\u00fb\": \"u\",\n",
    "    \"\\u016b\": \"u\",\n",
    "    \"\\u00fc\": \"u\",\n",
    "    \"\\u00d9\": \"U\",\n",
    "    \"\\u00da\": \"U\",\n",
    "    \"\\u00db\": \"U\",\n",
    "    \"\\u016a\": \"U\",\n",
    "    \"\\u00dc\": \"U\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36231ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONSONANT_MAP = {\n",
    "    \"\\u0161\": \"s\",\n",
    "    \"\\u0160\": \"S\",\n",
    "    \"\\u1e63\": \"s\",\n",
    "    \"\\u1e62\": \"S\",\n",
    "    \"\\u1e6d\": \"t\",\n",
    "    \"\\u1e6c\": \"T\",\n",
    "    \"\\u1e2b\": \"h\",\n",
    "    \"\\u1e2a\": \"H\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21592ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_QUOTE_MAP = {\n",
    "    \"\\u201e\": '\"',\n",
    "    \"\\u201c\": '\"',\n",
    "    \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\",\n",
    "    \"\\u2019\": \"'\",\n",
    "    \"\\u201a\": \"'\",\n",
    "    \"\\u02be\": \"'\",\n",
    "    \"\\u02bf\": \"'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0135ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SUBSCRIPT_MAP = str.maketrans(\n",
    "    {\n",
    "        \"\\u2080\": \"0\",\n",
    "        \"\\u2081\": \"1\",\n",
    "        \"\\u2082\": \"2\",\n",
    "        \"\\u2083\": \"3\",\n",
    "        \"\\u2084\": \"4\",\n",
    "        \"\\u2085\": \"5\",\n",
    "        \"\\u2086\": \"6\",\n",
    "        \"\\u2087\": \"7\",\n",
    "        \"\\u2088\": \"8\",\n",
    "        \"\\u2089\": \"9\",\n",
    "        \"\\u2093\": \"x\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d286ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transliteration(text) -> str:\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Protect literal tokens\n",
    "    text = text.replace(\"<gap>\", \"__LIT_GAP__\").replace(\"<big_gap>\", \"__LIT_BIG_GAP__\")\n",
    "\n",
    "    # Remove apostrophe line numbers only (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "\n",
    "    # <content> blocks\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "\n",
    "    # large gaps\n",
    "    text = re.sub(r\"\\[\\s*\\u2026+\\s*\\u2026*\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = text.replace(\"\\u2026\", \" __BIG_GAP__ \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # [content] -> content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "\n",
    "    # Half brackets\n",
    "    text = text.replace(\"\\u2039\", \"\").replace(\"\\u203A\", \"\")\n",
    "    text = text.replace(\"\\u2308\", \"\").replace(\"\\u2309\", \"\")\n",
    "    text = text.replace(\"\\u230A\", \"\").replace(\"\\u230B\", \"\")\n",
    "    text = text.replace(\"\\u02F9\", \"\").replace(\"\\u02FA\", \"\")\n",
    "\n",
    "    # Character maps\n",
    "    text = text.translate(_FULL_MAP).translate(_SUBSCRIPT_MAP)\n",
    "\n",
    "    # Scribal notations / divider\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "\n",
    "    # Standalone x\n",
    "    text = re.sub(r\"\\bx\\b\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Convert placeholders and restore literals\n",
    "    text = text.replace(\"__GAP__\", \"<gap>\").replace(\"__BIG_GAP__\", \"<big_gap>\")\n",
    "    text = text.replace(\"__LIT_GAP__\", \"<gap>\").replace(\"__LIT_BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbdd22",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "Prompting + Retrieval\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_SPLIT_RE = re.compile(r\"[\\s\\-]+\")\n",
    "TGT_TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z'\\-]*|\\d+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c87442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_src(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return [t for t in SRC_SPLIT_RE.split(str(text)) if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tgt(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return TGT_TOKEN_RE.findall(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce321f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bad_output(s: str) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    if t in {\"...\", \"‚Ä¶\"}:\n",
    "        return True\n",
    "    if re.fullmatch(r\"[.\\s‚Ä¶\\-‚Äì‚Äî,;:!?\\\"'()\\[\\]{}<>/\\\\]+\", t):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_ngrams(text: str, n: int = 3) -> list[str]:\n",
    "    text = f\" {text} \"\n",
    "    if len(text) < n:\n",
    "        return [text]\n",
    "    return [text[i : i + n] for i in range(len(text) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca81fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardRetriever:\n",
    "    def __init__(self, texts: list[str], *, n: int = 3, max_candidates: int = 500):\n",
    "        self.texts = texts\n",
    "        self.n = n\n",
    "        self.max_candidates = max_candidates\n",
    "        self.grams = [set(char_ngrams(t, n)) for t in texts]\n",
    "        self.inv: dict[str, list[int]] = {}\n",
    "        for i, gs in enumerate(self.grams):\n",
    "            for g in gs:\n",
    "                self.inv.setdefault(g, []).append(i)\n",
    "\n",
    "    def retrieve(self, query: str, k: int) -> list[int]:\n",
    "        qg = set(char_ngrams(query, self.n))\n",
    "        freq: Counter[int] = Counter()\n",
    "        for g in qg:\n",
    "            for idx in self.inv.get(g, []):\n",
    "                freq[idx] += 1\n",
    "        if not freq:\n",
    "            # No overlap: return first k (stable)\n",
    "            return list(range(min(k, len(self.texts))))\n",
    "        candidates = [idx for idx, _ in freq.most_common(self.max_candidates)]\n",
    "        scored = []\n",
    "        for idx in candidates:\n",
    "            inter = len(qg & self.grams[idx])\n",
    "            union = len(qg) + len(self.grams[idx]) - inter\n",
    "            scored.append((inter / union if union else 0.0, idx))\n",
    "        scored.sort(key=lambda x: (-x[0], x[1]))\n",
    "        return [idx for _, idx in scored[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7836bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> Any:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec323e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tm_pairs(path: Path, *, max_rows: int | None = None) -> list[dict]:\n",
    "    pairs = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows is not None and i >= max_rows:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            pairs.append(json.loads(line))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d488f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_retrieval(\n",
    "    src: str,\n",
    "    *,\n",
    "    tm_pairs: list[dict],\n",
    "    retriever: JaccardRetriever | None,\n",
    "    glossary: dict[str, list[str]] | None,\n",
    "    tm_k: int,\n",
    "    max_items: int,\n",
    "    max_prompt_chars: int,\n",
    ") -> str:\n",
    "    if not tm_pairs or retriever is None:\n",
    "        return src\n",
    "\n",
    "    idxs = retriever.retrieve(src, k=tm_k)\n",
    "    neighbors = [tm_pairs[i] for i in idxs if 0 <= i < len(tm_pairs)]\n",
    "\n",
    "    q_tokens = tokenize_src(src)\n",
    "    local_counts: dict[str, Counter[str]] = {t: Counter() for t in q_tokens}\n",
    "    for nb in neighbors:\n",
    "        nb_src_tokens = set(tokenize_src(nb.get(\"src\", \"\")))\n",
    "        nb_tgt_tokens = tokenize_tgt(nb.get(\"tgt\", \"\"))\n",
    "        if not nb_src_tokens or not nb_tgt_tokens:\n",
    "            continue\n",
    "        for tok in q_tokens:\n",
    "            if tok in nb_src_tokens:\n",
    "                local_counts[tok].update(nb_tgt_tokens)\n",
    "\n",
    "    items: list[str] = []\n",
    "    used: set[str] = set()\n",
    "    for tok in q_tokens:\n",
    "        if tok in used:\n",
    "            continue\n",
    "        tgt = None\n",
    "        if local_counts.get(tok) and local_counts[tok]:\n",
    "            tgt = local_counts[tok].most_common(1)[0][0]\n",
    "        elif glossary and tok in glossary and glossary[tok]:\n",
    "            tgt = glossary[tok][0]\n",
    "        if tgt:\n",
    "            items.append(f\"{tok}={tgt}\")\n",
    "            used.add(tok)\n",
    "        if len(items) >= max_items:\n",
    "            break\n",
    "\n",
    "    if not items:\n",
    "        return src\n",
    "\n",
    "    prompt = \"GLOSSARY: \" + \"; \".join(items) + \" ||| \" + src\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        return src\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm_fallback_translation(\n",
    "    src: str,\n",
    "    *,\n",
    "    tm_pairs: list[dict],\n",
    "    retriever: JaccardRetriever | None,\n",
    ") -> str | None:\n",
    "    if not tm_pairs or retriever is None:\n",
    "        return None\n",
    "    idxs = retriever.retrieve(src, k=1)\n",
    "    if not idxs:\n",
    "        return None\n",
    "    tgt = tm_pairs[idxs[0]].get(\"tgt\", \"\")\n",
    "    if tgt and not is_bad_output(tgt):\n",
    "        return str(tgt).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53db745",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferArgs:\n",
    "    model_dir: Path | None\n",
    "    assets_dir: Path | None\n",
    "    num_beams: int\n",
    "    max_source_len: int\n",
    "    max_new_tokens: int\n",
    "    min_new_tokens: int\n",
    "    batch_size: int\n",
    "    tm_k: int\n",
    "    glossary_max_items: int\n",
    "    max_prompt_chars: int\n",
    "    max_candidates: int\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e33af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> InferArgs:\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--model-dir\", type=Path, default=None)\n",
    "    p.add_argument(\"--assets-dir\", type=Path, default=None)\n",
    "    p.add_argument(\"--num-beams\", type=int, default=4)\n",
    "    p.add_argument(\"--max-source-len\", type=int, default=256)\n",
    "    p.add_argument(\"--max-new-tokens\", type=int, default=256)\n",
    "    p.add_argument(\"--min-new-tokens\", type=int, default=8)\n",
    "    p.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    p.add_argument(\"--tm-k\", type=int, default=5)\n",
    "    p.add_argument(\"--glossary-max-items\", type=int, default=8)\n",
    "    p.add_argument(\"--max-prompt-chars\", type=int, default=512)\n",
    "    p.add_argument(\"--max-candidates\", type=int, default=500)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    # Notebook-friendly: ignore ipykernel argv like `-f ...`\n",
    "    a, _unknown = p.parse_known_args()\n",
    "    return InferArgs(\n",
    "        model_dir=a.model_dir,\n",
    "        assets_dir=a.assets_dir,\n",
    "        num_beams=a.num_beams,\n",
    "        max_source_len=a.max_source_len,\n",
    "        max_new_tokens=a.max_new_tokens,\n",
    "        min_new_tokens=a.min_new_tokens,\n",
    "        batch_size=a.batch_size,\n",
    "        tm_k=a.tm_k,\n",
    "        glossary_max_items=a.glossary_max_items,\n",
    "        max_prompt_chars=a.max_prompt_chars,\n",
    "        max_candidates=a.max_candidates,\n",
    "        seed=a.seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle() -> bool:\n",
    "    return Path(\"/kaggle/input\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_data_dir() -> Path:\n",
    "    if not is_kaggle():\n",
    "        if Path(\"data/test.csv\").exists():\n",
    "            return Path(\"data\")\n",
    "        raise FileNotFoundError(\"test.csv not found\")\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    for d in base.iterdir():\n",
    "        if (d / \"test.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Competition data dir not found under /kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a61bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assets_dir() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        for d in [Path(\"assets\"), Path(\"data\"), Path(\"models\")]:\n",
    "            if (d / \"v5c_tm_pairs.jsonl\").exists() or (d / \"v5c_glossary.json\").exists():\n",
    "                return d\n",
    "        return None\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    for d in base.iterdir():\n",
    "        if (d / \"v5c_tm_pairs.jsonl\").exists() or (d / \"v5c_glossary.json\").exists():\n",
    "            return d\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_dir() -> Path:\n",
    "    # Allow overriding from env (useful in notebooks).\n",
    "    env = os.environ.get(\"V5C_MODEL_DIR\")\n",
    "    if env:\n",
    "        p = Path(env)\n",
    "        if (p / \"config.json\").exists():\n",
    "            return p\n",
    "\n",
    "    if not is_kaggle():\n",
    "        raise FileNotFoundError(\"Please provide --model-dir when running locally.\")\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    preferred = base / \"akkadian-v5c-small/pytorch/default/1\"\n",
    "    if (preferred / \"config.json\").exists():\n",
    "        return preferred\n",
    "    # common layout for Kaggle Model\n",
    "    for d in base.iterdir():\n",
    "        cand = d / \"pytorch/default/1\"\n",
    "        if (cand / \"config.json\").exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\"Model dir not found. Pass --model-dir explicitly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_once(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    texts: list[str],\n",
    "    *,\n",
    "    num_beams: int,\n",
    "    max_source_len: int,\n",
    "    max_new_tokens: int,\n",
    "    min_new_tokens: int,\n",
    ") -> list[str]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_source_len,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        do_sample=False,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b781848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable: list[Any], n: int) -> Iterable[list[Any]]:\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "\n",
    "    # Make sampling retry reproducible (best-effort).\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    comp_dir = find_comp_data_dir()\n",
    "    model_dir = args.model_dir or find_model_dir()\n",
    "    assets_dir = args.assets_dir or find_assets_dir()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ V5c ByT5-small INFER\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÅ Competition: {comp_dir}\")\n",
    "    print(f\"ü§ñ Model dir:   {model_dir}\")\n",
    "    print(f\"üß± Assets dir:  {assets_dir if assets_dir else 'not found'}\")\n",
    "    print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load model (local only)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(str(model_dir), local_files_only=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Deterministic tokenizer derived from model vocab size (no network, no files)\n",
    "    extra_ids = int(model.config.vocab_size) - 259\n",
    "    if extra_ids < 0:\n",
    "        raise ValueError(f\"Unexpected vocab_size={model.config.vocab_size} for ByT5\")\n",
    "    tokenizer = ByT5Tokenizer(extra_ids=extra_ids)\n",
    "    assert len(tokenizer) == model.config.vocab_size, \"Tokenizer/model vocab mismatch!\"\n",
    "    print(f\"üî§ Tokenizer vocab: {len(tokenizer)} (extra_ids={extra_ids})\")\n",
    "\n",
    "    # Load assets (optional)\n",
    "    tm_pairs: list[dict] = []\n",
    "    glossary: dict[str, list[str]] | None = None\n",
    "    retriever: JaccardRetriever | None = None\n",
    "\n",
    "    if assets_dir:\n",
    "        tm_path = assets_dir / \"v5c_tm_pairs.jsonl\"\n",
    "        gl_path = assets_dir / \"v5c_glossary.json\"\n",
    "        if tm_path.exists():\n",
    "            tm_pairs = load_tm_pairs(tm_path)\n",
    "            print(f\"üß† TM pairs: {len(tm_pairs):,}\")\n",
    "        if gl_path.exists():\n",
    "            glossary = load_json(gl_path)\n",
    "            glossary = {k: list(v) for k, v in glossary.items()}\n",
    "            print(f\"üß† Glossary size: {len(glossary):,}\")\n",
    "        if tm_pairs:\n",
    "            retriever = JaccardRetriever([p.get(\"src\", \"\") for p in tm_pairs], max_candidates=args.max_candidates)\n",
    "\n",
    "    # Load test\n",
    "    test_df = pd.read_csv(comp_dir / \"test.csv\")\n",
    "    print(f\"üìÑ Test rows: {len(test_df):,}\")\n",
    "\n",
    "    # Normalize\n",
    "    normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "    # Build prompts\n",
    "    if tm_pairs and retriever:\n",
    "        prompts = [\n",
    "            build_prompt_with_retrieval(\n",
    "                s,\n",
    "                tm_pairs=tm_pairs,\n",
    "                retriever=retriever,\n",
    "                glossary=glossary,\n",
    "                tm_k=args.tm_k,\n",
    "                max_items=args.glossary_max_items,\n",
    "                max_prompt_chars=args.max_prompt_chars,\n",
    "            )\n",
    "            for s in tqdm(normalized, desc=\"Prompting\")\n",
    "        ]\n",
    "    else:\n",
    "        prompts = normalized\n",
    "\n",
    "    print(\"üìù Prompt sample:\")\n",
    "    for i in range(min(2, len(prompts))):\n",
    "        print(f\"   [{i}] {prompts[i][:160]}...\")\n",
    "\n",
    "    # Generate with retries (model-only; no output substitution).\n",
    "    translations: list[str] = [\"\"] * len(prompts)\n",
    "    remaining = list(range(len(prompts)))\n",
    "\n",
    "    def run_attempt(\n",
    "        attempt_name: str,\n",
    "        *,\n",
    "        num_beams: int,\n",
    "        do_sample: bool,\n",
    "        temperature: float | None = None,\n",
    "        top_p: float | None = None,\n",
    "    ) -> list[int]:\n",
    "        nonlocal translations\n",
    "        if not remaining:\n",
    "            return []\n",
    "        print(f\"üöÄ Generation attempt: {attempt_name} (remaining={len(remaining)})\")\n",
    "        new_remaining: list[int] = []\n",
    "        for chunk in tqdm(list(chunked(remaining, args.batch_size)), desc=f\"Gen:{attempt_name}\", unit=\"batch\"):\n",
    "            texts = [prompts[i] for i in chunk]\n",
    "            inputs = tokenizer(\n",
    "                texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=args.max_source_len,\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            gen_kwargs: dict[str, Any] = dict(\n",
    "                num_beams=num_beams,\n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                min_new_tokens=args.min_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            if do_sample:\n",
    "                gen_kwargs[\"temperature\"] = float(temperature or 0.8)\n",
    "                gen_kwargs[\"top_p\"] = float(top_p or 0.95)\n",
    "\n",
    "            out = model.generate(**inputs, **gen_kwargs)\n",
    "            decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "            for idx, s in zip(chunk, decoded):\n",
    "                translations[idx] = s\n",
    "                if is_bad_output(s):\n",
    "                    new_remaining.append(idx)\n",
    "\n",
    "        return new_remaining\n",
    "\n",
    "    # Attempt 1: beam search\n",
    "    remaining = run_attempt(\"beam\", num_beams=args.num_beams, do_sample=False)\n",
    "    # Attempt 2: greedy\n",
    "    if remaining:\n",
    "        remaining = run_attempt(\"greedy\", num_beams=1, do_sample=False)\n",
    "    # Attempt 3: sampling (last resort, still model-only)\n",
    "    if remaining:\n",
    "        remaining = run_attempt(\"sample\", num_beams=1, do_sample=True, temperature=0.9, top_p=0.95)\n",
    "\n",
    "    if remaining:\n",
    "        # Hard-fail instead of substituting outputs: prevents silent garbage submissions.\n",
    "        print(\"‚ùå Failed to produce valid outputs for some rows after retries.\")\n",
    "        for i in remaining[:10]:\n",
    "            print(f\"   [bad idx={i}] src='{normalized[i][:120]}...' out='{translations[i]}'\")\n",
    "        raise RuntimeError(f\"Bad outputs remain after retries: {len(remaining)}\")\n",
    "\n",
    "    print(\"‚úÖ Generation completed without empty/ellipsis-only outputs (after retries).\")\n",
    "\n",
    "    print(\"üìù Output sample:\")\n",
    "    for i in range(min(3, len(translations))):\n",
    "        print(f\"   [{i}] {translations[i][:160]}...\")\n",
    "\n",
    "    sub = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": translations})\n",
    "    out_path = Path(\"/kaggle/working/submission.csv\") if is_kaggle() else Path(\"submission.csv\")\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Saved: {out_path} ({len(sub):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
