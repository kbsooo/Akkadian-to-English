{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fecd5f3",
   "metadata": {},
   "source": [
    "Kaggle Notebook Template: Akkadian → English (Baseline)\n",
    "\n",
    "Pipeline: data load → preprocessing (if needed) → train → eval → submission\n",
    "\n",
    "Notes\n",
    "- Works on Kaggle with 1 or 2 GPUs (T4).\n",
    "- Uses ByT5-base by default (safe for T4).\n",
    "- Multi-GPU uses accelerate.notebook_launcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import inspect\n",
    "\n",
    "# Optional: accelerate for multi-GPU\n",
    "try:\n",
    "    from accelerate import notebook_launcher\n",
    "    _HAS_ACCELERATE = True\n",
    "except Exception:\n",
    "    _HAS_ACCELERATE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363713f",
   "metadata": {},
   "source": [
    "## 0. (Optional) Install dependencies\n",
    "\n",
    "If needed in Kaggle:\n",
    "```\n",
    "!pip -q install transformers datasets sacrebleu sentencepiece accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef24f7b",
   "metadata": {},
   "source": [
    "## 1. Paths & Settings\n",
    "\n",
    "Kaggle folder convention:\n",
    "- inputs: /kaggle/input/<dataset-name>/\n",
    "- outputs: /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81633c8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "WORK_DIR = Path(\"/kaggle/working\")\n",
    "OUTPUTS_DIR = WORK_DIR / \"outputs\"\n",
    "\n",
    "MODEL_NAME = \"google/byt5-base\"  # safe default for T4\n",
    "TIER_FILE = \"sentence_pairs_q70_pattern.csv\"  # Tier3\n",
    "SEED = 42\n",
    "\n",
    "# Training hyperparams (tune as needed)\n",
    "MAX_SRC_LEN = 256\n",
    "MAX_TGT_LEN = 256\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 4\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "WARMUP_RATIO = 0.05\n",
    "\n",
    "USE_FP16 = True if torch.cuda.is_available() else False\n",
    "GRADIENT_CHECKPOINTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308bd06",
   "metadata": {},
   "source": [
    "## 2. Locate competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9125ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_competition_data_dir() -> Path:\n",
    "    # find directory containing train.csv and test.csv\n",
    "    if not KAGGLE_INPUT.exists():\n",
    "        raise FileNotFoundError(\"/kaggle/input not found\")\n",
    "\n",
    "    for d in KAGGLE_INPUT.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if (d / \"train.csv\").exists() and (d / \"test.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Could not locate competition dataset with train.csv/test.csv\")\n",
    "\n",
    "\n",
    "def find_repo_dir() -> Path | None:\n",
    "    # search for repo that contains src/data_preprocessing.py\n",
    "    for d in KAGGLE_INPUT.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        cand = d / \"src\" / \"data_preprocessing.py\"\n",
    "        if cand.exists():\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "\n",
    "COMP_DATA_DIR = find_competition_data_dir()\n",
    "REPO_DIR = find_repo_dir()\n",
    "\n",
    "print(\"Competition data:\", COMP_DATA_DIR)\n",
    "print(\"Repo dir:\", REPO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216267b1",
   "metadata": {},
   "source": [
    "## 3. Preprocessing (if needed)\n",
    "\n",
    "This expects our preprocessing script to be attached as a Kaggle Dataset.\n",
    "If the outputs already exist, it will skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_preprocessing():\n",
    "    if OUTPUTS_DIR.exists() and (OUTPUTS_DIR / TIER_FILE).exists():\n",
    "        print(\"Found preprocessed outputs.\")\n",
    "        return\n",
    "\n",
    "    if REPO_DIR is None:\n",
    "        raise RuntimeError(\"Repo with src/data_preprocessing.py not found in /kaggle/input\")\n",
    "\n",
    "    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    script = REPO_DIR / \"src\" / \"data_preprocessing.py\"\n",
    "    cmd = [\n",
    "        \"python\",\n",
    "        str(script),\n",
    "        \"--data-dir\",\n",
    "        str(COMP_DATA_DIR),\n",
    "        \"--out-dir\",\n",
    "        str(OUTPUTS_DIR),\n",
    "        \"--plot\",\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf7b42",
   "metadata": {},
   "source": [
    "## 4. Load Tier3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc931f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(OUTPUTS_DIR / TIER_FILE)\n",
    "assert {\"oare_id\", \"src_norm\", \"tgt_norm\"}.issubset(train_df.columns)\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93156841",
   "metadata": {},
   "source": [
    "## 5. Grouped train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_split(df: pd.DataFrame, group_col: str, val_frac: float, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    groups = df[group_col].unique().tolist()\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(groups)\n",
    "    n_val = max(1, int(len(groups) * val_frac))\n",
    "    val_groups = set(groups[:n_val])\n",
    "\n",
    "    train_df = df[~df[group_col].isin(val_groups)].reset_index(drop=True)\n",
    "    val_df = df[df[group_col].isin(val_groups)].reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "train_df, val_df = group_split(train_df, \"oare_id\", 0.1, SEED)\n",
    "print(\"Train/Val:\", len(train_df), len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b2210",
   "metadata": {},
   "source": [
    "## 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc617d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src_norm\", \"tgt_norm\"]])\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src_norm\", \"tgt_norm\"]])\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(\n",
    "        batch[\"src_norm\"],\n",
    "        max_length=MAX_SRC_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=batch[\"tgt_norm\"],\n",
    "        max_length=MAX_TGT_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"src_norm\", \"tgt_norm\"])\n",
    "val_ds = val_ds.map(tokenize, batched=True, remove_columns=[\"src_norm\", \"tgt_norm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77838a",
   "metadata": {},
   "source": [
    "## 7. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee9e88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bleu = BLEU()\n",
    "chrf = CHRF(word_order=2)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds == -100, tokenizer.pad_token_id, preds)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    bleu_score = bleu.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "    chrf_score = chrf.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "    score = math.sqrt(max(0.0, bleu_score) * max(0.0, chrf_score))\n",
    "\n",
    "    return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578067b9",
   "metadata": {},
   "source": [
    "## 8. Train (single or multi‑GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainCfg:\n",
    "    batch_size: int = BATCH_SIZE\n",
    "    grad_accum: int = GRAD_ACCUM\n",
    "    epochs: int = EPOCHS\n",
    "    lr: float = LR\n",
    "    warmup_ratio: float = WARMUP_RATIO\n",
    "\n",
    "\n",
    "def train_fn():\n",
    "    set_seed(SEED)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(train_ds) / max(1, BATCH_SIZE))\n",
    "    steps_per_epoch = math.ceil(steps_per_epoch / max(1, GRAD_ACCUM))\n",
    "    total_steps = max(1, steps_per_epoch * EPOCHS)\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    # HF arg compatibility (evaluation_strategy -> eval_strategy)\n",
    "    arg_sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    eval_key = \"evaluation_strategy\" if \"evaluation_strategy\" in arg_sig.parameters else \"eval_strategy\"\n",
    "\n",
    "    args_kwargs = dict(\n",
    "        output_dir=str(WORK_DIR / \"baseline\"),\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=0.01,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=20,\n",
    "        save_total_limit=2,\n",
    "        fp16=USE_FP16,\n",
    "        bf16=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"score\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    args_kwargs[eval_key] = \"epoch\"\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(**args_kwargs)\n",
    "\n",
    "    trainer_kwargs = dict(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer_sig = inspect.signature(Seq2SeqTrainer.__init__)\n",
    "    if \"tokenizer\" in trainer_sig.parameters:\n",
    "        trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "    elif \"processing_class\" in trainer_sig.parameters:\n",
    "        trainer_kwargs[\"processing_class\"] = tokenizer\n",
    "\n",
    "    trainer = Seq2SeqTrainer(**trainer_kwargs)\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    with open(WORK_DIR / \"baseline\" / \"eval_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1 and _HAS_ACCELERATE:\n",
    "    print(\"Multi-GPU detected. Using accelerate.notebook_launcher ...\")\n",
    "    notebook_launcher(train_fn, num_processes=torch.cuda.device_count())\n",
    "else:\n",
    "    train_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31766713",
   "metadata": {},
   "source": [
    "## 9. Predict on Test + Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal normalization for test side (matches preprocessing)\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize_transliteration(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace(\"\\u1E2A\", \"H\").replace(\"\\u1E2B\", \"h\")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n",
    "    text = text.replace(\"\\u2026\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", \" <gap> \", text)\n",
    "    text = re.sub(r\"\\bx\\b\", \" <unk_sign> \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def batch_generate(texts, batch_size=8, max_len=MAX_TGT_LEN):\n",
    "    outputs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(**inputs, max_length=max_len)\n",
    "        outputs.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(COMP_DATA_DIR / \"test.csv\")\n",
    "assert {\"id\", \"transliteration\"}.issubset(test_df.columns)\n",
    "\n",
    "norm_texts = [normalize_transliteration(t) for t in test_df[\"transliteration\"].tolist()]\n",
    "\n",
    "preds = batch_generate(norm_texts, batch_size=8)\n",
    "\n",
    "sub = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": preds})\n",
    "sub_path = WORK_DIR / \"submission.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "\n",
    "print(\"Saved:\", sub_path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
