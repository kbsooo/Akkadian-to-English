{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badcedf0",
   "metadata": {},
   "source": [
    "# Akkadian V5d Inference\n",
    "\n",
    "- Model: ByT5-small (fine-tuned)\n",
    "- Tokenizer: Loaded from Kaggle Models (Internet OFF compatible)\n",
    "- Features: Glossary + TM Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75bfe54",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a1e58",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model paths on Kaggle\n",
    "    model_name: str = \"akkadian-v5d\"\n",
    "    tokenizer_name: str = \"byt5-small\"  # Kaggle model name for tokenizer\n",
    "    \n",
    "    model_path: Path = field(init=False)\n",
    "    tokenizer_path: Path = field(init=False)\n",
    "    \n",
    "    # Inference settings\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 8  # T4 x 2 has enough memory for larger batches\n",
    "    num_beams: int = 4\n",
    "    \n",
    "    # Retrieval + glossary\n",
    "    tm_k: int = 5\n",
    "    glossary_max_items: int = 8\n",
    "    max_prompt_chars: int = 512\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.model_path = self.kaggle_input / f\"{self.model_name}/pytorch/default/1\"\n",
    "        self.tokenizer_path = self.kaggle_input / f\"{self.tokenizer_name}/pytorch/default/1\"\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8210fb1",
   "metadata": {},
   "source": [
    "## 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec58598",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_VOWEL_MAP = {\n",
    "    \"√†\": \"a\", \"√°\": \"a\", \"√¢\": \"a\", \"ƒÅ\": \"a\", \"√§\": \"a\",\n",
    "    \"√Ä\": \"A\", \"√Å\": \"A\", \"√Ç\": \"A\", \"ƒÄ\": \"A\", \"√Ñ\": \"A\",\n",
    "    \"√®\": \"e\", \"√©\": \"e\", \"√™\": \"e\", \"ƒì\": \"e\", \"√´\": \"e\",\n",
    "    \"√à\": \"E\", \"√â\": \"E\", \"√ä\": \"E\", \"ƒí\": \"E\", \"√ã\": \"E\",\n",
    "    \"√¨\": \"i\", \"√≠\": \"i\", \"√Æ\": \"i\", \"ƒ´\": \"i\", \"√Ø\": \"i\",\n",
    "    \"√å\": \"I\", \"√ç\": \"I\", \"√é\": \"I\", \"ƒ™\": \"I\", \"√è\": \"I\",\n",
    "    \"√≤\": \"o\", \"√≥\": \"o\", \"√¥\": \"o\", \"≈ç\": \"o\", \"√∂\": \"o\",\n",
    "    \"√í\": \"O\", \"√ì\": \"O\", \"√î\": \"O\", \"≈å\": \"O\", \"√ñ\": \"O\",\n",
    "    \"√π\": \"u\", \"√∫\": \"u\", \"√ª\": \"u\", \"≈´\": \"u\", \"√º\": \"u\",\n",
    "    \"√ô\": \"U\", \"√ö\": \"U\", \"√õ\": \"U\", \"≈™\": \"U\", \"√ú\": \"U\",\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    \"≈°\": \"s\", \"≈†\": \"S\",\n",
    "    \"·π£\": \"s\", \"·π¢\": \"S\",\n",
    "    \"·π≠\": \"t\", \"·π¨\": \"T\",\n",
    "    \"·∏´\": \"h\", \"·∏™\": \"H\",\n",
    "}\n",
    "\n",
    "_QUOTE_MAP = {\n",
    "    \"‚Äû\": '\"', \"\"\": '\"', \"\"\": '\"',\n",
    "    \"'\": \"'\", \"'\": \"'\", \"‚Äö\": \"'\",\n",
    "    \" æ\": \"'\", \" ø\": \"'\",\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = {\n",
    "    \"‚ÇÄ\": \"0\", \"‚ÇÅ\": \"1\", \"‚ÇÇ\": \"2\", \"‚ÇÉ\": \"3\", \"‚ÇÑ\": \"4\",\n",
    "    \"‚ÇÖ\": \"5\", \"‚ÇÜ\": \"6\", \"‚Çá\": \"7\", \"‚Çà\": \"8\", \"‚Çâ\": \"9\",\n",
    "    \"‚Çì\": \"x\",\n",
    "}\n",
    "\n",
    "# Merge all character maps\n",
    "_ALL_CHAR_MAP = {**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP, **_SUBSCRIPT_MAP}\n",
    "\n",
    "# Build translation table safely\n",
    "_TRANS_TABLE = {}\n",
    "for k, v in _ALL_CHAR_MAP.items():\n",
    "    if isinstance(k, str) and len(k) == 1:\n",
    "        _TRANS_TABLE[ord(k)] = v\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    text = text.replace(\"<gap>\", \"__LIT_GAP__\")\n",
    "    text = text.replace(\"<big_gap>\", \"__LIT_BIG_GAP__\")\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\[\\s*‚Ä¶+\\s*‚Ä¶*\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = text.replace(\"‚Ä¶\", \" __BIG_GAP__ \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "    for char in \"‚Äπ‚Ä∫‚åà‚åâ‚åä‚åãÀπÀ∫\":\n",
    "        text = text.replace(char, \"\")\n",
    "    text = text.translate(_TRANS_TABLE)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\bx\\b\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "    text = text.replace(\"__GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__BIG_GAP__\", \"<big_gap>\")\n",
    "    text = text.replace(\"__LIT_GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__LIT_BIG_GAP__\", \"<big_gap>\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1126845",
   "metadata": {},
   "source": [
    "## 3. Glossary & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db5d58",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "SRC_SPLIT_RE = re.compile(r\"[\\s\\-]+\")\n",
    "TGT_TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z'\\-]*|\\d+\")\n",
    "\n",
    "\n",
    "def tokenize_src(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return [t for t in SRC_SPLIT_RE.split(str(text)) if t]\n",
    "\n",
    "\n",
    "def tokenize_tgt(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return TGT_TOKEN_RE.findall(str(text))\n",
    "\n",
    "\n",
    "def char_ngrams(text: str, n: int = 3) -> list[str]:\n",
    "    text = f\" {text} \"\n",
    "    if len(text) < n:\n",
    "        return [text]\n",
    "    return [text[i : i + n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "\n",
    "class JaccardRetriever:\n",
    "    def __init__(self, texts: list[str], n: int = 3, max_candidates: int = 500):\n",
    "        self.texts = texts\n",
    "        self.n = n\n",
    "        self.max_candidates = max_candidates\n",
    "        self.grams = [set(char_ngrams(t, n)) for t in texts]\n",
    "        self.inverted = {}\n",
    "        for idx, grams in enumerate(self.grams):\n",
    "            for g in grams:\n",
    "                self.inverted.setdefault(g, []).append(idx)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> list[int]:\n",
    "        grams = set(char_ngrams(query, self.n))\n",
    "        freq = Counter()\n",
    "        for g in grams:\n",
    "            for idx in self.inverted.get(g, []):\n",
    "                freq[idx] += 1\n",
    "        if freq:\n",
    "            candidates = [idx for idx, _ in freq.most_common(self.max_candidates)]\n",
    "        else:\n",
    "            candidates = list(range(min(len(self.texts), self.max_candidates)))\n",
    "        scores = []\n",
    "        for idx in candidates:\n",
    "            inter = len(grams & self.grams[idx])\n",
    "            union = len(grams) + len(self.grams[idx]) - inter\n",
    "            score = inter / union if union else 0.0\n",
    "            scores.append((score, idx))\n",
    "        scores.sort(key=lambda x: (-x[0], x[1]))\n",
    "        return [idx for _, idx in scores[:k]]\n",
    "\n",
    "\n",
    "def build_prompt_with_retrieval(\n",
    "    src: str,\n",
    "    tm_pairs: list[dict],\n",
    "    retriever: JaccardRetriever | None,\n",
    "    glossary: dict[str, list[str]] | None,\n",
    "    max_items: int,\n",
    "    max_prompt_chars: int,\n",
    "    tm_k: int,\n",
    ") -> str:\n",
    "    if not tm_pairs or retriever is None:\n",
    "        if not glossary:\n",
    "            return src\n",
    "        items = []\n",
    "        used = set()\n",
    "        for tok in tokenize_src(src):\n",
    "            if tok in used:\n",
    "                continue\n",
    "            tgts = glossary.get(tok)\n",
    "            if tgts:\n",
    "                items.append(f\"{tok}={tgts[0]}\")\n",
    "                used.add(tok)\n",
    "            if len(items) >= max_items:\n",
    "                break\n",
    "        if not items:\n",
    "            return src\n",
    "        return \"GLOSSARY: \" + \"; \".join(items) + \" ||| \" + src\n",
    "\n",
    "    idxs = retriever.retrieve(src, k=tm_k)\n",
    "    neighbors = [tm_pairs[i] for i in idxs]\n",
    "    query_tokens = tokenize_src(src)\n",
    "    local_counts: dict[str, Counter] = {t: Counter() for t in query_tokens}\n",
    "\n",
    "    for nb in neighbors:\n",
    "        nb_src_tokens = set(tokenize_src(nb.get(\"src\", \"\")))\n",
    "        nb_tgt_tokens = tokenize_tgt(nb.get(\"tgt\", \"\"))\n",
    "        if not nb_src_tokens or not nb_tgt_tokens:\n",
    "            continue\n",
    "        for tok in query_tokens:\n",
    "            if tok in nb_src_tokens:\n",
    "                local_counts[tok].update(nb_tgt_tokens)\n",
    "\n",
    "    items = []\n",
    "    used = set()\n",
    "    for tok in query_tokens:\n",
    "        if tok in used:\n",
    "            continue\n",
    "        tgt = None\n",
    "        if local_counts.get(tok):\n",
    "            tgt = local_counts[tok].most_common(1)[0][0]\n",
    "        elif glossary and tok in glossary:\n",
    "            tgt = glossary[tok][0]\n",
    "        if tgt:\n",
    "            items.append(f\"{tok}={tgt}\")\n",
    "            used.add(tok)\n",
    "        if len(items) >= max_items:\n",
    "            break\n",
    "\n",
    "    if not items:\n",
    "        return src\n",
    "    prompt = \"GLOSSARY: \" + \"; \".join(items) + \" ||| \" + src\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        return src\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def load_tm_pairs(path: Path) -> list[dict]:\n",
    "    pairs = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                pairs.append(json.loads(line))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def load_glossary(path: Path) -> dict[str, list[str]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {k: list(v) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a7ed5",
   "metadata": {},
   "source": [
    "## 4. Setup & Path Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d5e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle() -> bool:\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data not found\")\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_assets_dir() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data/v5d\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        return None\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"v5d_glossary.json\").exists() or (d / \"v5d_tm_pairs.jsonl\").exists():\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_model() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"outputs/v5d/model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "    if CFG.model_path.exists():\n",
    "        return CFG.model_path\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v5d\" in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "    raise FileNotFoundError(\"V5d model not found\")\n",
    "\n",
    "\n",
    "def find_tokenizer() -> Path:\n",
    "    \"\"\"Find tokenizer path - must be loaded from Kaggle Models (Internet OFF)\"\"\"\n",
    "    if not is_kaggle():\n",
    "        # Local: use HuggingFace\n",
    "        return Path(\"google/byt5-small\")\n",
    "    \n",
    "    if CFG.tokenizer_path.exists():\n",
    "        return CFG.tokenizer_path\n",
    "    \n",
    "    # Search for byt5 tokenizer in Kaggle inputs\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"byt5\" in d.name.lower():\n",
    "            if (d / \"tokenizer_config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/tokenizer_config.json\"):\n",
    "                return sub.parent\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        \"Tokenizer not found! Upload 'google/byt5-small' to Kaggle Models \"\n",
    "        \"and add it as a data source.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V5d Inference\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model()\n",
    "ASSETS_DIR = find_assets_dir()\n",
    "TOKENIZER_DIR = find_tokenizer()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üî§ Tokenizer: {TOKENIZER_DIR}\")\n",
    "print(f\"üß† Assets: {ASSETS_DIR if ASSETS_DIR else 'not found'}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c0cb4",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üî§ Loading tokenizer from: {TOKENIZER_DIR}\")\n",
    "\n",
    "# Load from local path (Kaggle) or HuggingFace name (local dev)\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(TOKENIZER_DIR), local_files_only=is_kaggle())\n",
    "print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "print(f\"   ‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574de7c",
   "metadata": {},
   "source": [
    "## 6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ü§ñ Loading model from: {MODEL_DIR}\")\n",
    "\n",
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "config = T5Config.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Config tie_word_embeddings: {config.tie_word_embeddings}\")\n",
    "config.tie_word_embeddings = False\n",
    "\n",
    "# Initialize model\n",
    "model = T5ForConditionalGeneration(config)\n",
    "\n",
    "# T5 always ties encoder/decoder embed_tokens to shared - we need to untie them\n",
    "# before loading weights so they can receive their own trained values\n",
    "def ensure_untied_embeddings(model, config):\n",
    "    \"\"\"Create separate embedding modules for encoder/decoder (not tied to shared).\"\"\"\n",
    "    # Create brand new embedding modules\n",
    "    model.encoder.embed_tokens = torch.nn.Embedding(config.vocab_size, config.d_model)\n",
    "    model.decoder.embed_tokens = torch.nn.Embedding(config.vocab_size, config.d_model)\n",
    "    \n",
    "    # Also ensure lm_head is separate\n",
    "    model.lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "    \n",
    "    print(f\"   Created separate encoder/decoder/lm_head embeddings\")\n",
    "\n",
    "ensure_untied_embeddings(model, config)\n",
    "print(f\"   Model initialized with untied embeddings\")\n",
    "\n",
    "# Load weights\n",
    "weights_path = MODEL_DIR / \"model.safetensors\"\n",
    "if weights_path.exists():\n",
    "    state_dict = load_file(str(weights_path))\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(f\"   ‚úÖ Loaded weights from safetensors\")\n",
    "else:\n",
    "    weights_path = MODEL_DIR / \"pytorch_model.bin\"\n",
    "    state_dict = torch.load(str(weights_path), map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(f\"   ‚úÖ Loaded weights from pytorch_model.bin\")\n",
    "\n",
    "print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Verify all 4 embeddings are separate\n",
    "embed_params = {name: p.numel() for name, p in model.named_parameters() \n",
    "                if 'embed' in name or 'lm_head' in name or 'shared' in name}\n",
    "print(f\"   Embedding params: {embed_params}\")\n",
    "\n",
    "if len(embed_params) != 4:\n",
    "    raise RuntimeError(f\"Expected 4 embedding params, got {len(embed_params)}!\")\n",
    "\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    print(f\"   ‚ö†Ô∏è WARNING: Vocab mismatch!\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Vocab match\")\n",
    "\n",
    "# GPU setup (note: generate() doesn't support DataParallel, so we use single GPU)\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"   Available GPUs: {n_gpus}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use first GPU for inference\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")\n",
    "if n_gpus > 1:\n",
    "    print(f\"   ‚ÑπÔ∏è Note: Using larger batch_size={CFG.batch_size} to utilize GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbd8e6",
   "metadata": {},
   "source": [
    "## 7. Load TM & Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_PAIRS = []\n",
    "GLOSSARY = None\n",
    "RETRIEVER = None\n",
    "\n",
    "if ASSETS_DIR:\n",
    "    tm_path = ASSETS_DIR / \"v5d_tm_pairs.jsonl\"\n",
    "    glossary_path = ASSETS_DIR / \"v5d_glossary.json\"\n",
    "\n",
    "    if tm_path.exists():\n",
    "        TM_PAIRS = load_tm_pairs(tm_path)\n",
    "        print(f\"üß† TM pairs: {len(TM_PAIRS):,}\")\n",
    "    else:\n",
    "        print(\"üß† TM pairs: not found\")\n",
    "\n",
    "    if glossary_path.exists():\n",
    "        GLOSSARY = load_glossary(glossary_path)\n",
    "        print(f\"üß† Glossary size: {len(GLOSSARY):,}\")\n",
    "    else:\n",
    "        print(\"üß† Glossary: not found\")\n",
    "\n",
    "if TM_PAIRS:\n",
    "    RETRIEVER = JaccardRetriever([p.get(\"src\", \"\") for p in TM_PAIRS])\n",
    "    print(\"üß† Retriever built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f50cd0",
   "metadata": {},
   "source": [
    "## 8. Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Sanity check...\")\n",
    "test_input = \"um-ma\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=50, num_beams=4)\n",
    "\n",
    "test_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"   Input: '{test_input}'\")\n",
    "print(f\"   Output: '{test_output}'\")\n",
    "\n",
    "if not test_output or test_output.strip() == \"\":\n",
    "    print(\"   ‚ö†Ô∏è WARNING: Empty output!\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Model produces non-empty output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020252ca",
   "metadata": {},
   "source": [
    "## 9. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1ddbf",
   "metadata": {},
   "source": [
    "## 10. Normalize & Build Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a77855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Normalizing...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4e9fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"üß† Building glossary prompts...\")\n",
    "prompts = []\n",
    "for src in tqdm(normalized, desc=\"Glossary\"):\n",
    "    prompt = build_prompt_with_retrieval(\n",
    "        src,\n",
    "        tm_pairs=TM_PAIRS,\n",
    "        retriever=RETRIEVER,\n",
    "        glossary=GLOSSARY,\n",
    "        max_items=CFG.glossary_max_items,\n",
    "        max_prompt_chars=CFG.max_prompt_chars,\n",
    "        tm_k=CFG.tm_k,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "\n",
    "print(f\"\\nüìù Sample prompts:\")\n",
    "for i in range(min(2, len(prompts))):\n",
    "    print(f\"   [{i}] {prompts[i][:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63ffe5",
   "metadata": {},
   "source": [
    "## 11. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df8e17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(texts: list[str]) -> list[str]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Fallback for empty outputs\n",
    "    return [r if r and r.strip() else \"[translation unavailable]\" for r in results]\n",
    "\n",
    "\n",
    "def translate_all(texts: list[str]) -> list[str]:\n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), CFG.batch_size), desc=\"üîÆ Translating\", unit=\"batch\")\n",
    "    for i in pbar:\n",
    "        batch = texts[i : i + CFG.batch_size]\n",
    "        translations.extend(generate_batch(batch))\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd76608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Running inference...\")\n",
    "translations = translate_all(prompts)\n",
    "\n",
    "empty_count = sum(1 for t in translations if t == \"[translation unavailable]\")\n",
    "if empty_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {empty_count} fallback translations!\")\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ea1e0",
   "metadata": {},
   "source": [
    "## 12. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "assert len(submission) == len(test_df), \"Length mismatch!\"\n",
    "assert submission[\"translation\"].notna().all(), \"NaN values!\"\n",
    "\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ V5d Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
