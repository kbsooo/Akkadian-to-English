{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c9ad25",
   "metadata": {},
   "source": [
    "Baseline Training (Seq2Seq)\n",
    "\n",
    "Usage (Tier3 default):\n",
    "  uv run python src/train_baseline.py --data-dir src/outputs --tier tier3\n",
    "\n",
    "Multi-GPU (2x T4 on Kaggle):\n",
    "  torchrun --nproc_per_node=2 src/train_baseline.py --data-dir src/outputs --tier tier3\n",
    "  # or\n",
    "  accelerate launch src/train_baseline.py --data-dir src/outputs --tier tier3\n",
    "\n",
    "Kaggle defaults:\n",
    "- data dir: /kaggle/working/outputs if present, else /kaggle/input/*/outputs\n",
    "- out dir:  /kaggle/working/baseline\n",
    "\n",
    "Optional:\n",
    "  --model-name google/byt5-base\n",
    "  --out-dir src/outputs/baseline_tier3\n",
    "  --epochs 5 --batch-size 4 --lr 3e-4\n",
    "  --max-source-length 256 --max-target-length 256\n",
    "\n",
    "Notes:\n",
    "- Character-level ByT5 is robust to rare diacritics and OCR noise.\n",
    "- Uses grouped split by oare_id to reduce leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704be62",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_name: str = \"google/byt5-base\"\n",
    "    seed: int = 42\n",
    "    val_frac: float = 0.1\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    epochs: int = 5\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.05\n",
    "    eval_strategy: str = \"epoch\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    predict_with_generate: bool = True\n",
    "    gradient_checkpointing: bool = False\n",
    "    dataloader_num_workers: int = 2\n",
    "    save_total_limit: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7e3c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data loading\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def load_tier(data_dir: Path, tier: str) -> pd.DataFrame:\n",
    "    tier_map = {\n",
    "        \"tier1\": \"sentence_pairs_valid.csv\",\n",
    "        \"tier2\": \"sentence_pairs_q70.csv\",\n",
    "        \"tier3\": \"sentence_pairs_q70_pattern.csv\",\n",
    "    }\n",
    "    fname = tier_map.get(tier)\n",
    "    if not fname:\n",
    "        raise ValueError(f\"Unknown tier: {tier}\")\n",
    "    path = data_dir / fname\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def _is_kaggle() -> bool:\n",
    "    return os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "\n",
    "\n",
    "def _default_data_dir() -> Path:\n",
    "    # Kaggle: prefer /kaggle/working/outputs, else search /kaggle/input/*/outputs\n",
    "    if _is_kaggle():\n",
    "        working = Path(\"/kaggle/working/outputs\")\n",
    "        if working.exists():\n",
    "            return working\n",
    "        input_root = Path(\"/kaggle/input\")\n",
    "        if input_root.exists():\n",
    "            for p in input_root.iterdir():\n",
    "                cand = p / \"outputs\"\n",
    "                if cand.exists():\n",
    "                    return cand\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\"src/outputs\")\n",
    "\n",
    "\n",
    "def _default_out_dir() -> Path:\n",
    "    return Path(\"/kaggle/working/baseline\") if _is_kaggle() else Path(\"src/outputs/baseline\")\n",
    "\n",
    "\n",
    "def group_split(df: pd.DataFrame, group_col: str, val_frac: float, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    groups = df[group_col].unique().tolist()\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(groups)\n",
    "    n_val = max(1, int(len(groups) * val_frac))\n",
    "    val_groups = set(groups[:n_val])\n",
    "\n",
    "    train_df = df[~df[group_col].isin(val_groups)].reset_index(drop=True)\n",
    "    val_df = df[df[group_col].isin(val_groups)].reset_index(drop=True)\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89e7b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Tokenization\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def build_datasets(\n",
    "    df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    cfg: TrainConfig,\n",
    "    use_tagged: bool,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    src_col = \"src_tagged\" if use_tagged and \"src_tagged\" in df.columns else \"src_norm\"\n",
    "    tgt_col = \"tgt_norm\"\n",
    "\n",
    "    # Shape safety: required columns\n",
    "    assert src_col in df.columns, f\"Missing {src_col}\"\n",
    "    assert tgt_col in df.columns, f\"Missing {tgt_col}\"\n",
    "\n",
    "    train_df, val_df = group_split(df, \"oare_id\", cfg.val_frac, cfg.seed)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df[[src_col, tgt_col]])\n",
    "    val_ds = Dataset.from_pandas(val_df[[src_col, tgt_col]])\n",
    "\n",
    "    def tokenize(batch):\n",
    "        # Insight: keep consistent max lengths for stable batching.\n",
    "        model_inputs = tokenizer(\n",
    "            batch[src_col],\n",
    "            max_length=cfg.max_source_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            text_target=batch[tgt_col],\n",
    "            max_length=cfg.max_target_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        # Shape safety: enforce maximum lengths\n",
    "        for ids in model_inputs[\"input_ids\"]:\n",
    "            assert len(ids) <= cfg.max_source_length\n",
    "        for ids in model_inputs[\"labels\"]:\n",
    "            assert len(ids) <= cfg.max_target_length\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    train_ds = train_ds.map(tokenize, batched=True, remove_columns=[src_col, tgt_col])\n",
    "    val_ds = val_ds.map(tokenize, batched=True, remove_columns=[src_col, tgt_col])\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca032fb3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def build_metrics(tokenizer):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        preds = np.where(preds == -100, tokenizer.pad_token_id, preds)\n",
    "        labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        bleu_score = bleu.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "        score = math.sqrt(max(0.0, bleu_score) * max(0.0, chrf_score))\n",
    "\n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"chrf\": chrf_score,\n",
    "            \"score\": score,\n",
    "        }\n",
    "\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d3e8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def train_baseline(\n",
    "    data_dir: Path,\n",
    "    out_dir: Path,\n",
    "    tier: str,\n",
    "    cfg: TrainConfig,\n",
    "    use_tagged: bool,\n",
    "    use_fp16: bool,\n",
    "    use_bf16: bool,\n",
    "    use_torch_compile: bool,\n",
    "    gradient_checkpointing: bool,\n",
    ") -> None:\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    df = load_tier(data_dir, tier)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)\n",
    "\n",
    "    # Insight: torch.compile can speed up training on supported backends.\n",
    "    if use_torch_compile and hasattr(torch, \"compile\"):\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if n_gpus > 1 and \"LOCAL_RANK\" not in os.environ:\n",
    "            print(f\"[Info] Detected {n_gpus} GPUs. For full DDP, run with:\")\n",
    "            print(\"  torchrun --nproc_per_node=2 src/train_baseline.py ...\")\n",
    "            print(\"  or accelerate launch src/train_baseline.py ...\")\n",
    "\n",
    "    train_ds, val_ds = build_datasets(df, tokenizer, cfg, use_tagged)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    # Handle HF arg name changes (evaluation_strategy -> eval_strategy)\n",
    "    arg_sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    eval_key = \"evaluation_strategy\" if \"evaluation_strategy\" in arg_sig.parameters else \"eval_strategy\"\n",
    "\n",
    "    # Warmup steps from ratio to avoid deprecated warmup_ratio\n",
    "    steps_per_epoch = math.ceil(len(train_ds) / max(1, cfg.batch_size))\n",
    "    steps_per_epoch = math.ceil(steps_per_epoch / max(1, cfg.gradient_accumulation_steps))\n",
    "    total_steps = max(1, steps_per_epoch * cfg.epochs)\n",
    "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
    "\n",
    "    args_kwargs = dict(\n",
    "        output_dir=str(out_dir),\n",
    "        save_strategy=cfg.save_strategy,\n",
    "        learning_rate=cfg.lr,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        predict_with_generate=cfg.predict_with_generate,\n",
    "        logging_steps=20,\n",
    "        save_total_limit=cfg.save_total_limit,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"score\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        dataloader_num_workers=cfg.dataloader_num_workers,\n",
    "    )\n",
    "    args_kwargs[eval_key] = cfg.eval_strategy\n",
    "\n",
    "    # Multi-GPU safe settings when available\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        if \"ddp_find_unused_parameters\" in arg_sig.parameters:\n",
    "            args_kwargs[\"ddp_find_unused_parameters\"] = False\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(**args_kwargs)\n",
    "\n",
    "    trainer_kwargs = dict(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=build_metrics(tokenizer),\n",
    "    )\n",
    "\n",
    "    # Transformers API compatibility: tokenizer arg was removed in newer versions.\n",
    "    trainer_sig = inspect.signature(Seq2SeqTrainer.__init__)\n",
    "    if \"tokenizer\" in trainer_sig.parameters:\n",
    "        trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "    elif \"processing_class\" in trainer_sig.parameters:\n",
    "        trainer_kwargs[\"processing_class\"] = tokenizer\n",
    "\n",
    "    trainer = Seq2SeqTrainer(**trainer_kwargs)\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_dir / \"eval_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Visual proof: training curves\n",
    "    log_history = trainer.state.log_history\n",
    "    steps, train_loss = [], []\n",
    "    eval_steps, eval_loss, eval_bleu, eval_chrf, eval_score = [], [], [], [], []\n",
    "\n",
    "    for entry in log_history:\n",
    "        if \"loss\" in entry and \"epoch\" in entry and \"eval_loss\" not in entry:\n",
    "            steps.append(entry.get(\"step\", len(steps)))\n",
    "            train_loss.append(entry[\"loss\"])\n",
    "        if \"eval_loss\" in entry:\n",
    "            eval_steps.append(entry.get(\"step\", len(eval_steps)))\n",
    "            eval_loss.append(entry.get(\"eval_loss\"))\n",
    "            eval_bleu.append(entry.get(\"eval_bleu\"))\n",
    "            eval_chrf.append(entry.get(\"eval_chrf\"))\n",
    "            eval_score.append(entry.get(\"eval_score\"))\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].plot(steps, train_loss, label=\"train_loss\")\n",
    "        axes[0].plot(eval_steps, eval_loss, label=\"eval_loss\")\n",
    "        axes[0].set_title(\"Loss Curves\")\n",
    "        axes[0].set_xlabel(\"step\")\n",
    "        axes[0].legend()\n",
    "\n",
    "        axes[1].plot(eval_steps, eval_bleu, label=\"BLEU\")\n",
    "        axes[1].plot(eval_steps, eval_chrf, label=\"chrF\")\n",
    "        axes[1].plot(eval_steps, eval_score, label=\"GeoMean\")\n",
    "        axes[1].set_title(\"Eval Metrics\")\n",
    "        axes[1].set_xlabel(\"step\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out_dir / \"training_curves.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "    except Exception as exc:\n",
    "        print(f\"Plot skipped: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42527e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def build_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"Baseline Seq2Seq training\")\n",
    "    p.add_argument(\"--data-dir\", type=str, default=str(_default_data_dir()))\n",
    "    p.add_argument(\"--tier\", type=str, default=\"tier3\", choices=[\"tier1\", \"tier2\", \"tier3\"])\n",
    "    p.add_argument(\"--model-name\", type=str, default=\"google/byt5-base\")\n",
    "    p.add_argument(\"--out-dir\", type=str, default=str(_default_out_dir()))\n",
    "    p.add_argument(\"--epochs\", type=int, default=5)\n",
    "    p.add_argument(\"--batch-size\", type=int, default=2)\n",
    "    p.add_argument(\"--grad-accum\", type=int, default=4)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    p.add_argument(\"--max-source-length\", type=int, default=256)\n",
    "    p.add_argument(\"--max-target-length\", type=int, default=256)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--val-frac\", type=float, default=0.1)\n",
    "    p.add_argument(\"--use-tagged\", action=\"store_true\")\n",
    "    p.add_argument(\"--no-auto-fp16\", action=\"store_true\")\n",
    "    p.add_argument(\"--fp16\", action=\"store_true\")\n",
    "    p.add_argument(\"--bf16\", action=\"store_true\")\n",
    "    p.add_argument(\"--torch-compile\", action=\"store_true\")\n",
    "    p.add_argument(\"--gradient-checkpointing\", action=\"store_true\")\n",
    "    p.add_argument(\"--num-workers\", type=int, default=2)\n",
    "    p.add_argument(\"--save-total-limit\", type=int, default=2)\n",
    "    return p\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = build_argparser().parse_args()\n",
    "\n",
    "    cfg = TrainConfig(\n",
    "        model_name=args.model_name,\n",
    "        seed=args.seed,\n",
    "        val_frac=args.val_frac,\n",
    "        max_source_length=args.max_source_length,\n",
    "        max_target_length=args.max_target_length,\n",
    "        batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.grad_accum,\n",
    "        epochs=args.epochs,\n",
    "        lr=args.lr,\n",
    "        dataloader_num_workers=args.num_workers,\n",
    "        save_total_limit=args.save_total_limit,\n",
    "    )\n",
    "\n",
    "    out_dir = Path(args.out_dir) / args.tier\n",
    "\n",
    "    auto_fp16 = not args.no_auto_fp16\n",
    "    use_fp16 = args.fp16\n",
    "    if auto_fp16 and torch.cuda.is_available() and not args.bf16:\n",
    "        use_fp16 = True\n",
    "\n",
    "    train_baseline(\n",
    "        data_dir=Path(args.data_dir),\n",
    "        out_dir=out_dir,\n",
    "        tier=args.tier,\n",
    "        cfg=cfg,\n",
    "        use_tagged=args.use_tagged,\n",
    "        use_fp16=use_fp16,\n",
    "        use_bf16=args.bf16,\n",
    "        use_torch_compile=args.torch_compile,\n",
    "        gradient_checkpointing=args.gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
