{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":735819,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560954,"modelId":573579}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b7f1dc22","cell_type":"markdown","source":"# Akkadian â†’ English Translation: Inference (V1)\n\n**Environment**: Kaggle T4 GPU x2\n\n**Model**: ByT5-base (loaded from Kaggle Models)\n\n**Workflow**:\n1. Load trained model from Kaggle Models/Dataset\n2. Load test data from competition\n3. Run inference with batching\n4. Create submission.csv\n\n**Usage (convert to notebook)**:\n```bash\nuv run jupytext --to notebook src/akka_v1_infer.py\n```","metadata":{}},{"id":"414952db","cell_type":"markdown","source":"## 1. Imports & Configuration","metadata":{}},{"id":"31e236b6","cell_type":"code","source":"from __future__ import annotations\n\nimport os\nimport re\nimport unicodedata\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer","metadata":{"lines_to_next_cell":1,"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:24:46.851915Z","iopub.execute_input":"2026-01-30T05:24:46.852259Z","iopub.status.idle":"2026-01-30T05:24:46.857706Z","shell.execute_reply.started":"2026-01-30T05:24:46.852232Z","shell.execute_reply":"2026-01-30T05:24:46.856964Z"}},"outputs":[],"execution_count":13},{"id":"b90bf548","cell_type":"code","source":"# =============================\n# Configuration\n# =============================\n\n@dataclass\nclass Config:\n    \"\"\"Inference configuration for Kaggle T4 x2 environment.\"\"\"\n    # Paths (Kaggle)\n    kaggle_input: Path = Path(\"/kaggle/input\")\n    kaggle_working: Path = Path(\"/kaggle/working\")\n    \n    # Model settings\n    # Option 1: Set model dataset name (uploaded as Kaggle Dataset)\n    model_dataset_name: Optional[str] = None  # e.g., \"akkadian-byt5-v1\"\n    \n    # Option 2: Set model path directly\n    model_path: Optional[Path] = None\n    \n    # Inference\n    max_source_length: int = 256\n    max_target_length: int = 256\n    batch_size: int = 8  # larger batch for inference\n    num_beams: int = 4\n    \n    # Hardware\n    fp16: bool = True  # Use FP16 for faster inference\n\n\nCFG = Config()","metadata":{"lines_to_next_cell":1,"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:24:47.073315Z","iopub.execute_input":"2026-01-30T05:24:47.073986Z","iopub.status.idle":"2026-01-30T05:24:47.080010Z","shell.execute_reply.started":"2026-01-30T05:24:47.073956Z","shell.execute_reply":"2026-01-30T05:24:47.079153Z"}},"outputs":[],"execution_count":14},{"id":"d5218c03","cell_type":"markdown","source":"## 2. Environment Detection","metadata":{}},{"id":"c178a1ac","cell_type":"code","source":"def is_kaggle() -> bool:\n    \"\"\"Check if running on Kaggle.\"\"\"\n    return Path(\"/kaggle/input\").exists()\n\n\ndef find_competition_data() -> Path:\n    \"\"\"Find competition data directory.\"\"\"\n    if not is_kaggle():\n        # Local fallback\n        local_path = Path(\"data\")\n        if local_path.exists():\n            return local_path\n        raise FileNotFoundError(\"Cannot find competition data locally\")\n    \n    # On Kaggle: look for test.csv and sample_submission.csv\n    for d in CFG.kaggle_input.iterdir():\n        if (d / \"test.csv\").exists() and (d / \"sample_submission.csv\").exists():\n            return d\n    raise FileNotFoundError(\"Cannot find competition data in /kaggle/input\")\n\n\ndef find_model_dir() -> Path:\n    \"\"\"Find trained model directory.\"\"\"\n    # Option 1: Explicit path (handle both str and Path)\n    if CFG.model_path:\n        model_path = Path(CFG.model_path) if isinstance(CFG.model_path, str) else CFG.model_path\n        if model_path.exists():\n            return model_path\n    \n    # Option 2: Dataset name\n    if CFG.model_dataset_name:\n        model_path = CFG.kaggle_input / CFG.model_dataset_name\n        if model_path.exists():\n            # Check if config.json is in root or subdirectory\n            if (model_path / \"config.json\").exists():\n                return model_path\n            for sub in model_path.glob(\"**/config.json\"):\n                return sub.parent\n        raise FileNotFoundError(f\"Model dataset not found: {model_path}\")\n    \n    # Option 3: Auto-detect from /kaggle/input\n    if is_kaggle():\n        for d in CFG.kaggle_input.iterdir():\n            if not d.is_dir():\n                continue\n            # Skip competition data\n            if (d / \"test.csv\").exists():\n                continue\n            # Check for model files\n            if (d / \"config.json\").exists():\n                return d\n            for sub in d.glob(\"**/config.json\"):\n                return sub.parent\n    \n    # Option 4: Local trained model\n    local_model = Path(\"outputs/akkadian_v1/final\")\n    if local_model.exists():\n        return local_model\n    \n    raise FileNotFoundError(\n        \"Could not find model directory. \"\n        \"Set CFG.model_dataset_name or CFG.model_path\"\n    )\n\n\nCOMP_DATA_DIR = find_competition_data()\nMODEL_DIR = find_model_dir()\n\nprint(f\"ðŸ“ Competition data: {COMP_DATA_DIR}\")\nprint(f\"ðŸ¤– Model directory: {MODEL_DIR}\")\nprint(f\"ðŸ–¥ï¸ Running on Kaggle: {is_kaggle()}\")\nprint(f\"ðŸŽ® CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:24:47.319338Z","iopub.execute_input":"2026-01-30T05:24:47.319901Z","iopub.status.idle":"2026-01-30T05:24:47.334930Z","shell.execute_reply.started":"2026-01-30T05:24:47.319860Z","shell.execute_reply":"2026-01-30T05:24:47.334124Z"}},"outputs":[{"name":"stdout","text":"ðŸ“ Competition data: /kaggle/input/deep-past-initiative-machine-translation\nðŸ¤– Model directory: /kaggle/input/akkadian-v1-small/pytorch/default/3\nðŸ–¥ï¸ Running on Kaggle: True\nðŸŽ® CUDA available: True\n   GPU count: 2\n   GPU 0: Tesla T4\n   GPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":15},{"id":"c328a120","cell_type":"markdown","source":"## 3. Data Preprocessing (same as training)","metadata":{}},{"id":"68ba4ed0","cell_type":"code","source":"# Subscript conversion map\n_SUBSCRIPT_MAP = str.maketrans({\n    \"\\u2080\": \"0\", \"\\u2081\": \"1\", \"\\u2082\": \"2\", \"\\u2083\": \"3\", \"\\u2084\": \"4\",\n    \"\\u2085\": \"5\", \"\\u2086\": \"6\", \"\\u2087\": \"7\", \"\\u2088\": \"8\", \"\\u2089\": \"9\",\n    \"\\u2093\": \"x\",\n})\n\n\ndef normalize_transliteration(text: str) -> str:\n    \"\"\"Normalize Akkadian transliteration for model input.\n    \n    Important: This must match the preprocessing used during training!\n    \"\"\"\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    \n    # Unicode normalization\n    text = unicodedata.normalize(\"NFC\", text)\n    \n    # Normalize special H character\n    text = text.replace(\"\\u1E2A\", \"H\").replace(\"\\u1E2B\", \"h\")\n    \n    # Convert subscripts to numbers\n    text = text.translate(_SUBSCRIPT_MAP)\n    \n    # Handle gaps and damaged portions\n    text = text.replace(\"\\u2026\", \" <gap> \")  # ellipsis\n    text = re.sub(r\"\\.\\.\\.+\", \" <gap> \", text)\n    text = re.sub(r\"\\[([^\\]]*)\\]\", \" <gap> \", text)  # [damaged text]\n    \n    # Handle unknown signs\n    text = re.sub(r\"\\bx\\b\", \" <unk> \", text)\n    \n    # Remove editorial marks\n    text = re.sub(r\"[!?/]\", \" \", text)\n    \n    # Normalize whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:24:48.867486Z","iopub.execute_input":"2026-01-30T05:24:48.867830Z","iopub.status.idle":"2026-01-30T05:24:48.874285Z","shell.execute_reply.started":"2026-01-30T05:24:48.867801Z","shell.execute_reply":"2026-01-30T05:24:48.873387Z"}},"outputs":[],"execution_count":16},{"id":"3c7ac811","cell_type":"markdown","source":"## 4. Load Model","metadata":{}},{"id":"956b8da7","cell_type":"code","source":"from transformers import ByT5Tokenizer\n\nprint(f\"ðŸ¤– Loading model from {MODEL_DIR}\")\n\n# Load model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n\n# Force ByT5Tokenizer and compute extra_ids from model config.\n# ByT5 vocab = 256 bytes + 3 specials + extra_ids => extra_ids = vocab_size - 259\nvocab_size = getattr(model.config, \"vocab_size\", None)\nextra_ids = 125\nif isinstance(vocab_size, int):\n    extra_ids = max(vocab_size - 259, 0)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\nprint(f\"   âœ… Using ByT5Tokenizer(extra_ids={extra_ids})\")\n\n# Sanity check for potential mismatch\nif isinstance(vocab_size, int) and tokenizer.vocab_size != vocab_size:\n    print(\n        f\"   âš ï¸ Tokenizer vocab_size ({tokenizer.vocab_size}) != model vocab_size ({vocab_size})\"\n    )\n\n# Move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Use FP16 for faster inference\nif CFG.fp16 and device.type == \"cuda\":\n    model = model.half()\n    print(\"   âœ… Using FP16 for inference\")\n\nmodel.eval()\nprint(f\"   âœ… Model loaded on {device}\")","metadata":{"lines_to_next_cell":1,"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:24:49.194495Z","iopub.execute_input":"2026-01-30T05:24:49.194858Z","iopub.status.idle":"2026-01-30T05:24:58.174133Z","shell.execute_reply.started":"2026-01-30T05:24:49.194817Z","shell.execute_reply":"2026-01-30T05:24:58.173196Z"}},"outputs":[{"name":"stdout","text":"ðŸ¤– Loading model from /kaggle/input/akkadian-v1-small/pytorch/default/3\n   âœ… Using ByT5Tokenizer(extra_ids=125)\n   âš ï¸ Tokenizer vocab_size (256) != model vocab_size (384)\n   âœ… Using FP16 for inference\n   âœ… Model loaded on cuda\n","output_type":"stream"}],"execution_count":17},{"id":"9321fc42","cell_type":"markdown","source":"## 5. Inference Functions","metadata":{}},{"id":"ea2548f2","cell_type":"code","source":"@torch.no_grad()\ndef generate_batch(texts: List[str]) -> List[str]:\n    \"\"\"Generate translations for a batch of texts.\"\"\"\n    # Tokenize\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=CFG.max_source_length,\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Generate\n    outputs = model.generate(\n        **inputs,\n        max_length=CFG.max_target_length,\n        num_beams=CFG.num_beams,\n        early_stopping=True,\n    )\n    \n    # Decode\n    translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return translations\n\n\ndef translate_all(texts: List[str], batch_size: int = None) -> List[str]:\n    \"\"\"Translate all texts with batching and progress bar.\"\"\"\n    if batch_size is None:\n        batch_size = CFG.batch_size\n    \n    all_translations = []\n    \n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n        batch = texts[i:i + batch_size]\n        translations = generate_batch(batch)\n        all_translations.extend(translations)\n    \n    return all_translations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:25:02.615214Z","iopub.execute_input":"2026-01-30T05:25:02.616015Z","iopub.status.idle":"2026-01-30T05:25:02.622756Z","shell.execute_reply.started":"2026-01-30T05:25:02.615984Z","shell.execute_reply":"2026-01-30T05:25:02.622101Z"}},"outputs":[],"execution_count":18},{"id":"817e707f","cell_type":"markdown","source":"## 6. Load Test Data & Run Inference","metadata":{}},{"id":"413ece8d","cell_type":"code","source":"print(\"ðŸ“– Loading test data...\")\ntest_df = pd.read_csv(COMP_DATA_DIR / \"test.csv\")\nprint(f\"   Test samples: {len(test_df)}\")\n\n# Check columns\nrequired_cols = {\"id\", \"transliteration\"}\nif not required_cols.issubset(test_df.columns):\n    raise ValueError(f\"Test data missing columns: {required_cols - set(test_df.columns)}\")\n\n# Normalize input\nprint(\"ðŸ”§ Normalizing transliterations...\")\nnormalized_texts = [\n    normalize_transliteration(t) \n    for t in test_df[\"transliteration\"].tolist()\n]\n\n# Show sample\nprint(\"\\nSample input (normalized):\")\nfor i in range(min(2, len(normalized_texts))):\n    print(f\"  [{i}] {normalized_texts[i][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:25:02.889568Z","iopub.execute_input":"2026-01-30T05:25:02.890335Z","iopub.status.idle":"2026-01-30T05:25:02.899293Z","shell.execute_reply.started":"2026-01-30T05:25:02.890305Z","shell.execute_reply":"2026-01-30T05:25:02.898519Z"}},"outputs":[{"name":"stdout","text":"ðŸ“– Loading test data...\n   Test samples: 4\nðŸ”§ Normalizing transliterations...\n\nSample input (normalized):\n  [0] um-ma kÃ -ru-um kÃ -ni-ia-ma a-na aa-qÃ­-il <gap> da-tim aÃ­-ip-ri-ni kÃ -ar kÃ -ar-ma Ãº wa-bar-ra-tim qÃ­-...\n  [1] i-na mup-pÃ¬-im aa a-lim(ki) ia-tÃ¹ uâ€ž-mÃ¬-im a-nim ma-ma-an KÃ™.AN i-aa-Ãº-mu-ni i-na nÃ©-mÃ¬-lim da-aÃ¹r Ãº...\n","output_type":"stream"}],"execution_count":19},{"id":"db779bdd","cell_type":"code","source":"print(\"\\nðŸš€ Running inference...\")\ntranslations = translate_all(normalized_texts)\n\n# Show sample outputs\nprint(\"\\nSample outputs:\")\nfor i in range(min(2, len(translations))):\n    print(f\"  [{i}] {translations[i][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:25:03.006296Z","iopub.execute_input":"2026-01-30T05:25:03.006825Z","iopub.status.idle":"2026-01-30T05:25:06.354811Z","shell.execute_reply.started":"2026-01-30T05:25:03.006782Z","shell.execute_reply":"2026-01-30T05:25:06.353893Z"}},"outputs":[{"name":"stdout","text":"\nðŸš€ Running inference...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Translating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99bc72a9f14a411193a4597601926b5c"}},"metadata":{}},{"name":"stdout","text":"\nSample outputs:\n  [0]  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n  [1]  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n","output_type":"stream"}],"execution_count":20},{"id":"6d863f02","cell_type":"markdown","source":"## 7. Create Submission","metadata":{}},{"id":"11e9fb4e","cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"translation\": translations,\n})\n\n# Validate\nassert len(submission) == len(test_df), \"Submission length mismatch!\"\nassert submission[\"translation\"].notna().all(), \"Found NaN translations!\"\n\n# Save\nsubmission_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"\\nâœ… Submission saved to: {submission_path}\")\nprint(f\"   Total predictions: {len(submission)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:16:20.236703Z","iopub.execute_input":"2026-01-30T05:16:20.236966Z","iopub.status.idle":"2026-01-30T05:16:20.250350Z","shell.execute_reply.started":"2026-01-30T05:16:20.236942Z","shell.execute_reply":"2026-01-30T05:16:20.249569Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Submission saved to: /kaggle/working/submission.csv\n   Total predictions: 4\n","output_type":"stream"}],"execution_count":9},{"id":"36f4a4e0","cell_type":"code","source":"# Preview submission\nprint(\"\\nðŸ“„ Submission preview:\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:16:20.251297Z","iopub.execute_input":"2026-01-30T05:16:20.251749Z","iopub.status.idle":"2026-01-30T05:16:20.268364Z","shell.execute_reply.started":"2026-01-30T05:16:20.251691Z","shell.execute_reply":"2026-01-30T05:16:20.267655Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“„ Submission preview:\n   id                                        translation\n0   0   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n1   1   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n2   2   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n3   3   @!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n","output_type":"stream"}],"execution_count":10},{"id":"ed81c69c","cell_type":"markdown","source":"## 8. Sanity Check","metadata":{}},{"id":"4c51cfc6","cell_type":"code","source":"# Load sample submission for comparison\nsample_sub = pd.read_csv(COMP_DATA_DIR / \"sample_submission.csv\")\n\nprint(\"\\nðŸ” Comparison with sample submission:\")\nprint(f\"   Sample submission shape: {sample_sub.shape}\")\nprint(f\"   Our submission shape: {submission.shape}\")\n\n# Check if IDs match\nif set(submission[\"id\"]) == set(sample_sub[\"id\"]):\n    print(\"   âœ… IDs match!\")\nelse:\n    print(\"   âŒ ID mismatch!\")\n\n# Show comparison\nprint(\"\\nðŸ“Š Side-by-side comparison (first 2):\")\nfor i in range(min(2, len(submission))):\n    print(f\"\\n[ID: {submission.iloc[i]['id']}]\")\n    print(f\"  Sample: {sample_sub.iloc[i]['translation'][:100]}...\")\n    print(f\"  Ours:   {submission.iloc[i]['translation'][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:16:20.269494Z","iopub.execute_input":"2026-01-30T05:16:20.269890Z","iopub.status.idle":"2026-01-30T05:16:20.283986Z","shell.execute_reply.started":"2026-01-30T05:16:20.269858Z","shell.execute_reply":"2026-01-30T05:16:20.283096Z"}},"outputs":[{"name":"stdout","text":"\nðŸ” Comparison with sample submission:\n   Sample submission shape: (4, 2)\n   Our submission shape: (4, 2)\n   âœ… IDs match!\n\nðŸ“Š Side-by-side comparison (first 2):\n\n[ID: 0]\n  Sample: Thus  Kanesh, say to the -payers, our messenger, every single colony, and the trading stations: A le...\n  Ours:    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n\n[ID: 1]\n  Sample: In the letter of the City (it is written): From this day on, whoever buys meteoric iron, (the City o...\n  Ours:    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n","output_type":"stream"}],"execution_count":11},{"id":"a9a83917","cell_type":"markdown","source":"## 9. Done!\n\nYour `submission.csv` is ready for submission to Kaggle.\n\n**Next Steps:**\n1. Click \"Save Version\" in Kaggle\n2. Go to the competition page\n3. Submit the output file","metadata":{}},{"id":"ceacd995","cell_type":"code","source":"print(\"\\n\" + \"=\" * 60)\nprint(\"ðŸŽ‰ Inference complete!\")\nprint(\"=\" * 60)\nprint(f\"\\nðŸ“ Submission file: {submission_path}\")\nprint(\"\\nTo submit:\")\nprint(\"1. Save this notebook version\")\nprint(\"2. Go to competition page â†’ Submit\")\nprint(\"3. Select this notebook's output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T05:16:20.285039Z","iopub.execute_input":"2026-01-30T05:16:20.285639Z","iopub.status.idle":"2026-01-30T05:16:20.290486Z","shell.execute_reply.started":"2026-01-30T05:16:20.285609Z","shell.execute_reply":"2026-01-30T05:16:20.289664Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nðŸŽ‰ Inference complete!\n============================================================\n\nðŸ“ Submission file: /kaggle/working/submission.csv\n\nTo submit:\n1. Save this notebook version\n2. Go to competition page â†’ Submit\n3. Select this notebook's output\n","output_type":"stream"}],"execution_count":12}]}