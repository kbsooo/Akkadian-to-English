{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c02997",
   "metadata": {},
   "source": [
    "# Akkadian V5b Inference (Retrieval + Glossary Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bcf99",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf83c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import tempfile\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "\n",
    "    model_size: str = \"small\"  # \"small\", \"base\" or \"large\"\n",
    "    model_path: Path = field(init=False)\n",
    "\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "\n",
    "    # Retrieval + glossary\n",
    "    tm_k: int = 5\n",
    "    glossary_max_items: int = 8\n",
    "    max_prompt_chars: int = 512\n",
    "    prefer_tfidf: bool = True\n",
    "    jaccard_max_candidates: int = 500\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"small\":\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v5b-small/pytorch/default/1\"\n",
    "        elif self.model_size == \"base\":\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v5b-base/pytorch/default/1\"\n",
    "        else:\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v5b-large/pytorch/default/1\"\n",
    "\n",
    "\n",
    "CFG = Config(model_size=\"small\")\n",
    "\n",
    "# -----------------------------\n",
    "# V5 normalization (inline for Kaggle)\n",
    "# -----------------------------\n",
    "\n",
    "_VOWEL_MAP = {\n",
    "    \"\\u00e0\": \"a\", \"\\u00e1\": \"a\", \"\\u00e2\": \"a\", \"\\u0101\": \"a\", \"\\u00e4\": \"a\",\n",
    "    \"\\u00c0\": \"A\", \"\\u00c1\": \"A\", \"\\u00c2\": \"A\", \"\\u0100\": \"A\", \"\\u00c4\": \"A\",\n",
    "    \"\\u00e8\": \"e\", \"\\u00e9\": \"e\", \"\\u00ea\": \"e\", \"\\u0113\": \"e\", \"\\u00eb\": \"e\",\n",
    "    \"\\u00c8\": \"E\", \"\\u00c9\": \"E\", \"\\u00ca\": \"E\", \"\\u0112\": \"E\", \"\\u00cb\": \"E\",\n",
    "    \"\\u00ec\": \"i\", \"\\u00ed\": \"i\", \"\\u00ee\": \"i\", \"\\u012b\": \"i\", \"\\u00ef\": \"i\",\n",
    "    \"\\u00cc\": \"I\", \"\\u00cd\": \"I\", \"\\u00ce\": \"I\", \"\\u012a\": \"I\", \"\\u00cf\": \"I\",\n",
    "    \"\\u00f2\": \"o\", \"\\u00f3\": \"o\", \"\\u00f4\": \"o\", \"\\u014d\": \"o\", \"\\u00f6\": \"o\",\n",
    "    \"\\u00d2\": \"O\", \"\\u00d3\": \"O\", \"\\u00d4\": \"O\", \"\\u014c\": \"O\", \"\\u00d6\": \"O\",\n",
    "    \"\\u00f9\": \"u\", \"\\u00fa\": \"u\", \"\\u00fb\": \"u\", \"\\u016b\": \"u\", \"\\u00fc\": \"u\",\n",
    "    \"\\u00d9\": \"U\", \"\\u00da\": \"U\", \"\\u00db\": \"U\", \"\\u016a\": \"U\", \"\\u00dc\": \"U\",\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    \"\\u0161\": \"s\", \"\\u0160\": \"S\",\n",
    "    \"\\u1e63\": \"s\", \"\\u1e62\": \"S\",\n",
    "    \"\\u1e6d\": \"t\", \"\\u1e6c\": \"T\",\n",
    "    \"\\u1e2b\": \"h\", \"\\u1e2a\": \"H\",\n",
    "}\n",
    "\n",
    "_QUOTE_MAP = {\n",
    "    \"\\u201e\": '\"', \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\",\n",
    "    \"\\u02be\": \"'\", \"\\u02bf\": \"'\",\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    \"\\u2080\": \"0\", \"\\u2081\": \"1\", \"\\u2082\": \"2\", \"\\u2083\": \"3\", \"\\u2084\": \"4\",\n",
    "    \"\\u2085\": \"5\", \"\\u2086\": \"6\", \"\\u2087\": \"7\", \"\\u2088\": \"8\", \"\\u2089\": \"9\",\n",
    "    \"\\u2093\": \"x\",\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Protect literal gap tokens before removing <content> blocks\n",
    "    text = text.replace(\"<gap>\", \"__LIT_GAP__\")\n",
    "    text = text.replace(\"<big_gap>\", \"__LIT_BIG_GAP__\")\n",
    "\n",
    "    # Remove apostrophe line numbers only (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "\n",
    "    # Remove <content> blocks first\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "\n",
    "    # Large gaps\n",
    "    text = re.sub(r\"\\[\\s*\\u2026+\\s*\\u2026*\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # Ellipsis\n",
    "    text = text.replace(\"\\u2026\", \" __BIG_GAP__ \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # [content] -> content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "\n",
    "    # Half brackets and variants\n",
    "    text = text.replace(\"\\u2039\", \"\").replace(\"\\u203A\", \"\")\n",
    "    text = text.replace(\"\\u2308\", \"\").replace(\"\\u2309\", \"\")\n",
    "    text = text.replace(\"\\u230A\", \"\").replace(\"\\u230B\", \"\")\n",
    "    text = text.replace(\"\\u02F9\", \"\").replace(\"\\u02FA\", \"\")\n",
    "\n",
    "    # Character maps\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "\n",
    "    # Scribal notations / word divider\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "\n",
    "    # Standalone x\n",
    "    text = re.sub(r\"\\bx\\b\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Convert placeholders\n",
    "    text = text.replace(\"__GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Restore literal tokens\n",
    "    text = text.replace(\"__LIT_GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__LIT_BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Cleanup\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenization utilities\n",
    "# -----------------------------\n",
    "\n",
    "SRC_SPLIT_RE = re.compile(r\"[\\s\\-]+\")\n",
    "TGT_TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z'\\-]*|\\d+\")\n",
    "\n",
    "\n",
    "def tokenize_src(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return [t for t in SRC_SPLIT_RE.split(str(text)) if t]\n",
    "\n",
    "\n",
    "def tokenize_tgt(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    return TGT_TOKEN_RE.findall(str(text))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Retrieval\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def char_ngrams(text: str, n: int = 3) -> list[str]:\n",
    "    text = f\" {text} \"\n",
    "    if len(text) < n:\n",
    "        return [text]\n",
    "    return [text[i : i + n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "\n",
    "class JaccardRetriever:\n",
    "    def __init__(self, texts: list[str], n: int = 3, max_candidates: int = 500):\n",
    "        self.texts = texts\n",
    "        self.n = n\n",
    "        self.max_candidates = max_candidates\n",
    "        self.grams = [set(char_ngrams(t, n)) for t in texts]\n",
    "        self.inverted = {}\n",
    "        for idx, grams in enumerate(self.grams):\n",
    "            for g in grams:\n",
    "                self.inverted.setdefault(g, []).append(idx)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> list[int]:\n",
    "        grams = set(char_ngrams(query, self.n))\n",
    "        cand = set()\n",
    "        freq = Counter()\n",
    "        for g in grams:\n",
    "            for idx in self.inverted.get(g, []):\n",
    "                freq[idx] += 1\n",
    "        if freq:\n",
    "            for idx, _ in freq.most_common(self.max_candidates):\n",
    "                cand.add(idx)\n",
    "        else:\n",
    "            cand = set(range(len(self.texts)))\n",
    "\n",
    "        scores = []\n",
    "        for idx in cand:\n",
    "            inter = len(grams & self.grams[idx])\n",
    "            union = len(grams) + len(self.grams[idx]) - inter\n",
    "            score = inter / union if union else 0.0\n",
    "            scores.append((score, idx))\n",
    "        scores.sort(key=lambda x: (-x[0], x[1]))\n",
    "        return [idx for _, idx in scores[:k]]\n",
    "\n",
    "\n",
    "class TfidfRetriever:\n",
    "    def __init__(self, texts: list[str]):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5))\n",
    "        self.matrix = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> list[int]:\n",
    "        q = self.vectorizer.transform([query])\n",
    "        scores = (self.matrix @ q.T).toarray().ravel()\n",
    "        if k >= len(scores):\n",
    "            idxs = np.argsort(-scores)\n",
    "        else:\n",
    "            idxs = np.argpartition(-scores, k - 1)[:k]\n",
    "            idxs = idxs[np.argsort(-scores[idxs])]\n",
    "        return idxs.tolist()\n",
    "\n",
    "\n",
    "def build_retriever(texts: list[str], prefer_tfidf: bool = True) -> object:\n",
    "    if prefer_tfidf:\n",
    "        try:\n",
    "            return TfidfRetriever(texts)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return JaccardRetriever(texts, max_candidates=CFG.jaccard_max_candidates)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Glossary prompt (local + global)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def build_prompt_with_retrieval(\n",
    "    src: str,\n",
    "    tm_pairs: list[dict],\n",
    "    retriever: object | None,\n",
    "    glossary: dict[str, list[str]] | None,\n",
    "    max_items: int,\n",
    "    max_prompt_chars: int,\n",
    "    tm_k: int,\n",
    ") -> str:\n",
    "    if not tm_pairs or retriever is None:\n",
    "        return src\n",
    "\n",
    "    idxs = retriever.retrieve(src, k=tm_k)\n",
    "    neighbors = [tm_pairs[i] for i in idxs]\n",
    "\n",
    "    query_tokens = tokenize_src(src)\n",
    "    local_counts: dict[str, Counter] = {t: Counter() for t in query_tokens}\n",
    "\n",
    "    for nb in neighbors:\n",
    "        nb_src_tokens = set(tokenize_src(nb.get(\"src\", \"\")))\n",
    "        nb_tgt_tokens = tokenize_tgt(nb.get(\"tgt\", \"\"))\n",
    "        if not nb_src_tokens or not nb_tgt_tokens:\n",
    "            continue\n",
    "        for tok in query_tokens:\n",
    "            if tok in nb_src_tokens:\n",
    "                local_counts[tok].update(nb_tgt_tokens)\n",
    "\n",
    "    items = []\n",
    "    used = set()\n",
    "    for tok in query_tokens:\n",
    "        if tok in used:\n",
    "            continue\n",
    "        tgt = None\n",
    "        if local_counts.get(tok):\n",
    "            tgt = local_counts[tok].most_common(1)[0][0]\n",
    "        elif glossary and tok in glossary:\n",
    "            tgt = glossary[tok][0]\n",
    "        if tgt:\n",
    "            items.append(f\"{tok}={tgt}\")\n",
    "            used.add(tok)\n",
    "        if len(items) >= max_items:\n",
    "            break\n",
    "\n",
    "    if not items:\n",
    "        return src\n",
    "\n",
    "    prompt = \"GLOSSARY: \" + \"; \".join(items) + \" ||| \" + src\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        return src\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c02c58",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45acaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data not found\")\n",
    "\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_assets_dir() -> Path | None:\n",
    "    # Prefer a dataset containing v5b assets\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data/v5b\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        return None\n",
    "\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"v5b_glossary.json\").exists() or (d / \"v5b_tm_pairs.jsonl\").exists():\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_model() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(f\"outputs/akkadian_v5b_{CFG.model_size}/model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "\n",
    "    if CFG.model_path.exists():\n",
    "        return CFG.model_path\n",
    "\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v5b\" in d.name.lower() and CFG.model_size in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "\n",
    "    raise FileNotFoundError(f\"V5b-{CFG.model_size} model not found\")\n",
    "\n",
    "\n",
    "def load_tm_pairs(path: Path, max_rows: int | None = None) -> list[dict]:\n",
    "    pairs = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows is not None and i >= max_rows:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            pairs.append(json.loads(line))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def load_glossary(path: Path) -> dict[str, list[str]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {k: list(v) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666c8ef",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe6a78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V5b Inference: {CFG.model_size.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model()\n",
    "ASSETS_DIR = find_assets_dir()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üß† Assets: {ASSETS_DIR if ASSETS_DIR else 'not found'}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71839bdf",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6a152",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model from: {MODEL_DIR}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "def load_autotokenizer_with_fix(model_dir: Path):\n",
    "    \"\"\"Load AutoTokenizer from local files only.\n",
    "    If tokenizer_config.json is malformed (extra_special_tokens as list),\n",
    "    patch it in a temp directory and retry.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return AutoTokenizer.from_pretrained(str(model_dir), use_fast=False, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è AutoTokenizer local load failed: {e}\")\n",
    "        src_cfg = model_dir / \"tokenizer_config.json\"\n",
    "        if not src_cfg.exists():\n",
    "            raise\n",
    "\n",
    "        with src_cfg.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "        extra = cfg.get(\"extra_special_tokens\")\n",
    "        if isinstance(extra, list):\n",
    "            # transformers expects dict here; list triggers AttributeError(keys)\n",
    "            cfg[\"extra_special_tokens\"] = {}\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        tmp_dir = Path(tempfile.mkdtemp(prefix=\"tokfix_\"))\n",
    "        for name in [\"tokenizer_config.json\", \"special_tokens_map.json\", \"config.json\"]:\n",
    "            src = model_dir / name\n",
    "            if src.exists():\n",
    "                shutil.copy2(src, tmp_dir / name)\n",
    "        with (tmp_dir / \"tokenizer_config.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"   ‚Ü™ Patched tokenizer_config.json (extra_special_tokens list -> dict) and retrying AutoTokenizer\")\n",
    "        return AutoTokenizer.from_pretrained(str(tmp_dir), use_fast=False, local_files_only=True)\n",
    "\n",
    "\n",
    "tokenizer = load_autotokenizer_with_fix(MODEL_DIR)\n",
    "\n",
    "print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "print(f\"   Tokenizer class: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "assert len(tokenizer) == model.config.vocab_size, \"Vocab mismatch!\"\n",
    "print(\"   ‚úÖ Vocab match\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b5119",
   "metadata": {},
   "source": [
    "## 5. Load TM + Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dafc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_PAIRS = []\n",
    "GLOSSARY = None\n",
    "RETRIEVER = None\n",
    "\n",
    "if ASSETS_DIR:\n",
    "    tm_path = ASSETS_DIR / \"v5b_tm_pairs.jsonl\"\n",
    "    glossary_path = ASSETS_DIR / \"v5b_glossary.json\"\n",
    "\n",
    "    if tm_path.exists():\n",
    "        TM_PAIRS = load_tm_pairs(tm_path)\n",
    "        print(f\"üß† TM pairs: {len(TM_PAIRS):,}\")\n",
    "    else:\n",
    "        print(\"üß† TM pairs: not found\")\n",
    "\n",
    "    if glossary_path.exists():\n",
    "        GLOSSARY = load_glossary(glossary_path)\n",
    "        print(f\"üß† Glossary size: {len(GLOSSARY):,}\")\n",
    "    else:\n",
    "        print(\"üß† Glossary: not found\")\n",
    "\n",
    "if TM_PAIRS:\n",
    "    RETRIEVER = build_retriever([p.get(\"src\", \"\") for p in TM_PAIRS], prefer_tfidf=CFG.prefer_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f72b9c",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug: bool = False):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "\n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\")\n",
    "    for i in pbar:\n",
    "        batch = texts[i : i + batch_size]\n",
    "        translations.extend(generate_batch(batch))\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb1b75",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13e266",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "print(\"\\nüîß Normalizing (V5b)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(\"\\nüß† Building glossary prompts...\")\n",
    "prompts = []\n",
    "for src in tqdm(normalized, desc=\"Glossary\"):\n",
    "    prompt = build_prompt_with_retrieval(\n",
    "        src,\n",
    "        tm_pairs=TM_PAIRS,\n",
    "        retriever=RETRIEVER,\n",
    "        glossary=GLOSSARY,\n",
    "        max_items=CFG.glossary_max_items,\n",
    "        max_prompt_chars=CFG.max_prompt_chars,\n",
    "        tm_k=CFG.tm_k,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "\n",
    "print(f\"\\nüìù Sample prompts:\")\n",
    "for i in range(min(2, len(prompts))):\n",
    "    print(f\"   [{i}] {prompts[i][:120]}...\")\n",
    "\n",
    "print(\"\\nüöÄ Running inference...\")\n",
    "print(\"\\n[DEBUG] First sample test...\")\n",
    "_test = generate_batch([prompts[0]], debug=True)\n",
    "print(f\"[DEBUG] Translation: '{_test[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(prompts)\n",
    "\n",
    "empty_count = sum(1 for t in translations if not t or not t.strip())\n",
    "if empty_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {empty_count} empty translations!\")\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5bbaa",
   "metadata": {},
   "source": [
    "## 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f24ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "assert len(submission) == len(test_df), \"Length mismatch!\"\n",
    "assert submission[\"translation\"].notna().all(), \"NaN values!\"\n",
    "\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V5b-{CFG.model_size.upper()} Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
