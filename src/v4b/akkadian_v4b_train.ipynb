{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe353b",
   "metadata": {},
   "source": [
    "# Akkadian V4b Training: Competition Guidelines + OCR Noise\n",
    "\n",
    "**Key Changes from V4:**\n",
    "- Competition-compliant preprocessing\n",
    "  - `[content]` ‚Üí content (remove brackets, keep content)\n",
    "  - `[x]` ‚Üí `<gap>` (single broken sign)\n",
    "  - `‚Ä¶` or `[‚Ä¶ ‚Ä¶]` ‚Üí `<big_gap>` (large breaks)\n",
    "  - `<content>` ‚Üí content (scribal insertions)\n",
    "  - `Àπ À∫` removed (partial breaks)\n",
    "- OCR noise augmentation (same as V4)\n",
    "- train.csv only (no published_texts)\n",
    "\n",
    "**Environment**: Google Colab with A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaa96b",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_path = kagglehub.competition_download('deep-past-initiative-machine-translation')\n",
    "print(f'Competition data: {competition_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b4ce2",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde3cfb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"V4b: Competition Guidelines + OCR Noise.\"\"\"\n",
    "    \n",
    "    model_size: str = \"base\"\n",
    "    data_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    \n",
    "    # OCR Augmentation\n",
    "    augment_prob: float = 0.4\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    val_size: float = 0.1\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    epochs: int = 8  # Reduced from 10 (overfitting after 7)\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Hardware\n",
    "    fp16: bool = False\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # Model-specific\n",
    "    model_name: str = field(init=False)\n",
    "    batch_size: int = field(init=False)\n",
    "    gradient_accumulation_steps: int = field(init=False)\n",
    "    learning_rate: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_name = \"google/byt5-base\"\n",
    "            self.batch_size = 4\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.learning_rate = 1e-4\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4b-base\")\n",
    "        else:\n",
    "            self.model_name = \"google/byt5-large\"\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.learning_rate = 5e-5\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4b-large\")\n",
    "\n",
    "\n",
    "CFG = Config(model_size=\"base\")\n",
    "CFG.data_dir = Path(competition_path)\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4b: {CFG.model_size.upper()} + Competition Guidelines\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Data: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üéØ Epochs: {CFG.epochs}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43aeb8",
   "metadata": {},
   "source": [
    "## 2. Competition-Compliant Normalization\n",
    "\n",
    "Based on DATA_INFO.md guidelines:\n",
    "- `[x]` ‚Üí `<gap>` (single broken sign)\n",
    "- `‚Ä¶` or `[‚Ä¶ ‚Ä¶]` ‚Üí `<big_gap>` (large breaks)\n",
    "- `[content]` ‚Üí content (remove brackets only)\n",
    "- `<content>` ‚Üí content (scribal insertions)\n",
    "- `Àπ À∫` removed (partial breaks)\n",
    "- `! ? /` removed (scribal notations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2624c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character maps for diacritics normalization\n",
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈° ‚Üí s\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£ ‚Üí s\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠ ‚Üí t\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´ ‚Üí h\n",
    "}\n",
    "\n",
    "_QUOTE_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Akkadian transliteration following competition guidelines.\n",
    "    \n",
    "    Key rules from DATA_INFO.md:\n",
    "    - [x] ‚Üí <gap> (single broken sign)\n",
    "    - ‚Ä¶ or [‚Ä¶ ‚Ä¶] ‚Üí <big_gap> (large breaks)\n",
    "    - [content] ‚Üí content (remove brackets, keep content)\n",
    "    - <content> ‚Üí content (scribal insertions)\n",
    "    - Àπ À∫ removed (partial breaks)\n",
    "    - ! ? / : removed (scribal notations, word dividers)\n",
    "    \n",
    "    NOTE: Line numbers are NOT removed because train.csv starts with\n",
    "    quantities like \"1 T√öG\", \"17 G√çN\" which are meaningful data.\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # NOTE: Line number removal DISABLED - would delete quantities like \"1 T√öG\"\n",
    "    \n",
    "    # 1. Handle <content> ‚Üí content (scribal insertions) FIRST\n",
    "    #    Do this before gap tokens are created to avoid removing them!\n",
    "    text = re.sub(r'<<([^>]+)>>', r'\\1', text)  # errant signs\n",
    "    text = re.sub(r'<([^>]+)>', r'\\1', text)    # scribal insertions\n",
    "    \n",
    "    # 2. Handle large gaps: [‚Ä¶ ‚Ä¶] or [... ...] ‚Üí __BIG_GAP__\n",
    "    text = re.sub(r'\\[\\s*‚Ä¶+\\s*‚Ä¶*\\s*\\]', ' __BIG_GAP__ ', text)\n",
    "    text = re.sub(r'\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]', ' __BIG_GAP__ ', text)\n",
    "    \n",
    "    # 3. Handle ellipsis ‚Üí __BIG_GAP__\n",
    "    text = text.replace('\\u2026', ' __BIG_GAP__ ')  # ‚Ä¶\n",
    "    text = re.sub(r'\\.\\.\\.+', ' __BIG_GAP__ ', text)\n",
    "    \n",
    "    # 4. Handle [x] ‚Üí __GAP__ (single broken sign)\n",
    "    text = re.sub(r'\\[\\s*x\\s*\\]', ' __GAP__ ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 5. Handle [content] ‚Üí content (remove brackets, keep content)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]', r'\\1', text)\n",
    "    \n",
    "    # 6. Remove half brackets (partial breaks)\n",
    "    text = text.replace('\\u2039', '')  # ‚Äπ\n",
    "    text = text.replace('\\u203a', '')  # ‚Ä∫\n",
    "    text = text.replace('\\u2308', '')  # ‚åà (left ceiling)\n",
    "    text = text.replace('\\u2309', '')  # ‚åâ (right ceiling)\n",
    "    text = text.replace('\\u230a', '')  # ‚åä (left floor)\n",
    "    text = text.replace('\\u230b', '')  # ‚åã (right floor)\n",
    "    text = text.replace('Àπ', '')  # literal\n",
    "    text = text.replace('À∫', '')  # literal\n",
    "    \n",
    "    # 7. Apply character maps (diacritics, consonants, quotes)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    \n",
    "    # 8. Remove scribal notations AND word dividers: ! ? / :\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s*:\\s*', ' ', text)  # : word divider\n",
    "    \n",
    "    # 9. Handle standalone x ‚Üí __GAP__ (x = unknown/broken sign)\n",
    "    text = re.sub(r'\\bx\\b', ' __GAP__ ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 10. Convert placeholders to actual tokens\n",
    "    text = text.replace('__GAP__', '<gap>')\n",
    "    text = text.replace('__BIG_GAP__', '<big_gap>')\n",
    "    \n",
    "    # 11. Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def normalize_translation(text) -> str:\n",
    "    \"\"\"Normalize English translation.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test normalization\n",
    "print(\"\\nüìù Normalization Examples (Competition Guidelines):\")\n",
    "test_cases = [\n",
    "    \"[K√ô.BABBAR]\",        # ‚Üí K√ô.BABBAR (keep content)\n",
    "    \"[x]\",                 # ‚Üí <gap>\n",
    "    \"[‚Ä¶ ‚Ä¶]\",              # ‚Üí <big_gap>\n",
    "    \"‚Ä¶\",                   # ‚Üí <big_gap>\n",
    "    \"<correction>\",        # ‚Üí correction\n",
    "    \"ÀπpartialÀ∫\",           # ‚Üí partial\n",
    "    \"reading!\",            # ‚Üí reading\n",
    "    \"≈°um-ma ·π£a-bi-tam\",   # ‚Üí sum-ma sa-bi-tam\n",
    "]\n",
    "for tc in test_cases:\n",
    "    print(f\"   '{tc}' ‚Üí '{normalize_transliteration(tc)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbdcf9",
   "metadata": {},
   "source": [
    "## 3. OCR Noise Augmentation (Same as V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340a79c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_DIACRITICS_DROP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',\n",
    "    '\\u0101': 'a', '\\u0100': 'A',\n",
    "    '\\u0113': 'e', '\\u0112': 'E',\n",
    "    '\\u012b': 'i', '\\u012a': 'I',\n",
    "    '\\u016b': 'u', '\\u016a': 'U',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_TO_NUM = {\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "}\n",
    "\n",
    "\n",
    "def _drop_diacritics(text: str, prob: float = 0.4) -> str:\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _DIACRITICS_DROP and random.random() < prob:\n",
    "            result.append(_DIACRITICS_DROP[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _vary_subscripts(text: str, prob: float = 0.3) -> str:\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _SUBSCRIPT_TO_NUM and random.random() < prob:\n",
    "            result.append(_SUBSCRIPT_TO_NUM[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _drop_some_hyphens(text: str, prob: float = 0.15) -> str:\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if '-' in word and random.random() < prob:\n",
    "            word = word.replace('-', '')\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def apply_ocr_noise(text: str, prob: float = 0.4) -> str:\n",
    "    \"\"\"Apply OCR-like noise to RAW transliteration.\"\"\"\n",
    "    if not text or random.random() > prob:\n",
    "        return text\n",
    "    \n",
    "    # Protect special tokens\n",
    "    protected = {}\n",
    "    for i, token in enumerate(['<gap>', '<big_gap>', '<GAP>', '<BIG_GAP>']):\n",
    "        placeholder = f\"__P{i}__\"\n",
    "        if token in text:\n",
    "            text = text.replace(token, placeholder)\n",
    "            protected[placeholder] = token\n",
    "    \n",
    "    # Apply noise\n",
    "    noise_funcs = [\n",
    "        lambda t: _drop_diacritics(t, prob=0.35),\n",
    "        lambda t: _vary_subscripts(t, prob=0.25),\n",
    "        lambda t: _drop_some_hyphens(t, prob=0.12),\n",
    "    ]\n",
    "    \n",
    "    n_funcs = random.randint(1, len(noise_funcs))\n",
    "    for func in random.sample(noise_funcs, n_funcs):\n",
    "        text = func(text)\n",
    "    \n",
    "    # Restore tokens\n",
    "    for placeholder, token in protected.items():\n",
    "        text = text.replace(placeholder, token)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb73a4",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c9473",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_file(data_dir: Path, filename: str) -> Path:\n",
    "    if (data_dir / filename).exists():\n",
    "        return data_dir / filename\n",
    "    for p in data_dir.glob(f\"**/{filename}\"):\n",
    "        return p\n",
    "    raise FileNotFoundError(f\"{filename} not found in {data_dir}\")\n",
    "\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Load train.csv with oare_id-based split.\"\"\"\n",
    "    train_path = find_file(CFG.data_dir, \"train.csv\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f\"   train.csv: {len(train_df)} rows\")\n",
    "    \n",
    "    train_df = train_df.rename(columns={\n",
    "        'transliteration': 'src_raw',\n",
    "        'translation': 'tgt_raw'\n",
    "    })\n",
    "    train_df = train_df.dropna(subset=['src_raw', 'tgt_raw']).reset_index(drop=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_by_oare_id(df: pd.DataFrame, val_size: float, seed: int):\n",
    "    \"\"\"Split by oare_id to prevent leakage.\"\"\"\n",
    "    unique_ids = df['oare_id'].unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(unique_ids)\n",
    "    \n",
    "    n_val = int(len(unique_ids) * val_size)\n",
    "    val_ids = set(unique_ids[:n_val])\n",
    "    \n",
    "    train_df = df[~df['oare_id'].isin(val_ids)].reset_index(drop=True)\n",
    "    val_df = df[df['oare_id'].isin(val_ids)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "print(\"üìñ Loading data...\")\n",
    "raw_df = load_raw_data()\n",
    "print(f\"   Total: {len(raw_df)} rows\")\n",
    "\n",
    "print(f\"\\nüîÄ Splitting by oare_id...\")\n",
    "train_df, val_df = split_by_oare_id(raw_df, CFG.val_size, CFG.seed)\n",
    "print(f\"   Train: {len(train_df)} ({train_df['oare_id'].nunique()} docs)\")\n",
    "print(f\"   Val: {len(val_df)} ({val_df['oare_id'].nunique()} docs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d0188",
   "metadata": {},
   "source": [
    "## 5. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0583ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, augment: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Prepare: (Optional) OCR noise ‚Üí Normalize.\"\"\"\n",
    "    prepared = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing\"):\n",
    "        src_raw = row['src_raw']\n",
    "        tgt_raw = row['tgt_raw']\n",
    "        \n",
    "        # Apply noise to RAW source\n",
    "        if augment:\n",
    "            src_noisy = apply_ocr_noise(src_raw, prob=CFG.augment_prob)\n",
    "        else:\n",
    "            src_noisy = src_raw\n",
    "        \n",
    "        # Normalize (competition guidelines)\n",
    "        src = normalize_transliteration(src_noisy)\n",
    "        tgt = normalize_translation(tgt_raw)\n",
    "        \n",
    "        if src and tgt:\n",
    "            prepared.append({'src': src, 'tgt': tgt})\n",
    "    \n",
    "    return pd.DataFrame(prepared)\n",
    "\n",
    "\n",
    "print(\"\\nüîß Preparing training data (with augmentation)...\")\n",
    "train_prepared = prepare_data(train_df, augment=True)\n",
    "print(f\"   Train: {len(train_prepared)} samples\")\n",
    "\n",
    "print(\"\\nüîß Preparing validation data (no augmentation)...\")\n",
    "val_prepared = prepare_data(val_df, augment=False)\n",
    "print(f\"   Val: {len(val_prepared)} samples\")\n",
    "\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"   src: {train_prepared.iloc[0]['src'][:100]}...\")\n",
    "print(f\"   tgt: {train_prepared.iloc[0]['tgt'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6af01",
   "metadata": {},
   "source": [
    "## 6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853333c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model: {CFG.model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "print(f\"   Vocab: {len(tokenizer)}\")\n",
    "print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe4d41",
   "metadata": {},
   "source": [
    "## 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd97053",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"\\nüî§ Tokenizing...\")\n",
    "train_ds = Dataset.from_pandas(train_prepared[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "val_ds = Dataset.from_pandas(val_prepared[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "print(f\"   Train: {len(train_ds)}, Val: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663e9f8",
   "metadata": {},
   "source": [
    "## 8. Metrics & Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9c481",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "        \n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.losses = []\n",
    "        print(f\"\\n{'='*60}\\nüìä Epoch {self.epoch + 1}/{args.num_train_epochs}\\n{'='*60}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            print(f\"\\nüìâ Train Loss: {sum(self.losses)/len(self.losses):.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\\nüìà Validation\\n{'‚îÄ'*40}\")\n",
    "            print(f\"   BLEU: {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF: {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo:  {metrics.get('eval_geo_mean', 0):.2f}\\n{'‚îÄ'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48c9e9",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    max_grad_norm=CFG.max_grad_norm,\n",
    "    fp16=CFG.fp16,\n",
    "    bf16=CFG.bf16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,  # tokenizer deprecated in v5.x\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[LogCallback()],\n",
    ")\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Training: {CFG.epochs} epochs\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d57225",
   "metadata": {},
   "source": [
    "## 10. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1269f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = CFG.output_dir / \"model\"\n",
    "trainer.save_model(str(model_dir))\n",
    "tokenizer.save_pretrained(str(model_dir))\n",
    "print(f\"\\nüíæ Saved: {model_dir}\")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"\\nüìà Final: BLEU={results.get('eval_bleu',0):.2f}, chrF={results.get('eval_chrf',0):.2f}\")\n",
    "\n",
    "import shutil\n",
    "zip_path = CFG.output_dir / f\"akkadian_v4b_{CFG.model_size}\"\n",
    "shutil.make_archive(str(zip_path), 'zip', model_dir)\n",
    "print(f\"üì¶ Archive: {zip_path}.zip\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n‚úÖ V4b-{CFG.model_size.upper()} Complete!\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
