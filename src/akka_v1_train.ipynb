{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d6ff44",
   "metadata": {},
   "source": [
    "# Akkadian ‚Üí English Translation: Training (V1)\n",
    "\n",
    "**Environment**: Kaggle T4 GPU x2\n",
    "\n",
    "**Model**: ByT5-base (byte-level tokenization, good for low-resource languages)\n",
    "\n",
    "**Workflow**:\n",
    "1. Load preprocessed sentence pairs from competition data\n",
    "2. Train ByT5 with grouped train/val split\n",
    "3. Save model to Kaggle Models\n",
    "\n",
    "**Usage (convert to notebook)**:\n",
    "```bash\n",
    "uv run jupytext --to notebook src/akka_v1_train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc69d0",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a637",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dfbcd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for Kaggle T4 x2 environment.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = \"google/byt5-base\"\n",
    "    \n",
    "    # Paths (Kaggle)\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Data\n",
    "    train_file: str = \"train.csv\"  # from competition data\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    val_frac: float = 0.1\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4  # per GPU\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    epochs: int = 5\n",
    "    learning_rate: float = 3e-4\n",
    "    warmup_ratio: float = 0.05\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Hardware\n",
    "    fp16: bool = True  # T4 supports FP16 well\n",
    "    bf16: bool = False  # T4 doesn't support BF16\n",
    "    gradient_checkpointing: bool = True  # save memory\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy: str = \"epoch\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    save_total_limit: int = 2\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_bleu\"\n",
    "    greater_is_better: bool = True\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e611d5",
   "metadata": {},
   "source": [
    "## 2. Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edde16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle() -> bool:\n",
    "    \"\"\"Check if running on Kaggle.\"\"\"\n",
    "    return Path(\"/kaggle/input\").exists()\n",
    "\n",
    "\n",
    "def find_competition_data() -> Path:\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        # Local fallback\n",
    "        local_path = Path(\"data\")\n",
    "        if local_path.exists():\n",
    "            return local_path\n",
    "        raise FileNotFoundError(\"Cannot find competition data locally\")\n",
    "    \n",
    "    # On Kaggle: look for train.csv\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"train.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Cannot find competition data in /kaggle/input\")\n",
    "\n",
    "\n",
    "def get_output_dir() -> Path:\n",
    "    \"\"\"Get output directory for model checkpoints.\"\"\"\n",
    "    if is_kaggle():\n",
    "        return CFG.kaggle_working / \"akkadian_v1\"\n",
    "    return Path(\"outputs/akkadian_v1\")\n",
    "\n",
    "\n",
    "COMP_DATA_DIR = find_competition_data()\n",
    "OUTPUT_DIR = get_output_dir()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DATA_DIR}\")\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"üñ•Ô∏è Running on Kaggle: {is_kaggle()}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440a266",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscript conversion map\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    \"\\u2080\": \"0\", \"\\u2081\": \"1\", \"\\u2082\": \"2\", \"\\u2083\": \"3\", \"\\u2084\": \"4\",\n",
    "    \"\\u2085\": \"5\", \"\\u2086\": \"6\", \"\\u2087\": \"7\", \"\\u2088\": \"8\", \"\\u2089\": \"9\",\n",
    "    \"\\u2093\": \"x\",\n",
    "})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text: str) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration for model input.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # Normalize special H character\n",
    "    text = text.replace(\"\\u1E2A\", \"H\").replace(\"\\u1E2B\", \"h\")\n",
    "    \n",
    "    # Convert subscripts to numbers\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    \n",
    "    # Handle gaps and damaged portions\n",
    "    text = text.replace(\"\\u2026\", \" <gap> \")  # ellipsis\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <gap> \", text)\n",
    "    text = re.sub(r\"\\[([^\\]]*)\\]\", \" <gap> \", text)  # [damaged text]\n",
    "    \n",
    "    # Handle unknown signs\n",
    "    text = re.sub(r\"\\bx\\b\", \" <unk> \", text)\n",
    "    \n",
    "    # Remove editorial marks\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_translation(text: str) -> str:\n",
    "    \"\"\"Normalize English translation for model output.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # Normalize quotes\n",
    "    text = re.sub(r'[\"\"\"]', '\"', text)\n",
    "    text = re.sub(r\"['']\", \"'\", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess training data.\"\"\"\n",
    "    print(f\"üìñ Loading data from {data_path}\")\n",
    "    \n",
    "    train_df = pd.read_csv(data_path / CFG.train_file)\n",
    "    print(f\"   Raw samples: {len(train_df)}\")\n",
    "    \n",
    "    # Check columns\n",
    "    required_cols = {\"transliteration\", \"translation\"}\n",
    "    if not required_cols.issubset(train_df.columns):\n",
    "        raise ValueError(f\"Missing columns: {required_cols - set(train_df.columns)}\")\n",
    "    \n",
    "    # Add oare_id if not present (for grouping)\n",
    "    if \"oare_id\" not in train_df.columns:\n",
    "        train_df[\"oare_id\"] = train_df.index.astype(str)\n",
    "    \n",
    "    # Normalize texts\n",
    "    train_df[\"src\"] = train_df[\"transliteration\"].apply(normalize_transliteration)\n",
    "    train_df[\"tgt\"] = train_df[\"translation\"].apply(normalize_translation)\n",
    "    \n",
    "    # Filter empty samples\n",
    "    mask = (train_df[\"src\"].str.len() > 5) & (train_df[\"tgt\"].str.len() > 5)\n",
    "    train_df = train_df[mask].reset_index(drop=True)\n",
    "    print(f\"   After filtering: {len(train_df)}\")\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd47819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_split(df: pd.DataFrame, group_col: str, val_frac: float, seed: int):\n",
    "    \"\"\"Split data by group to prevent data leakage.\"\"\"\n",
    "    groups = df[group_col].unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(groups)\n",
    "    \n",
    "    n_val = max(1, int(len(groups) * val_frac))\n",
    "    val_groups = set(groups[:n_val])\n",
    "    \n",
    "    train_mask = ~df[group_col].isin(val_groups)\n",
    "    \n",
    "    train_df = df[train_mask].reset_index(drop=True)\n",
    "    val_df = df[~train_mask].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4a0cd",
   "metadata": {},
   "source": [
    "## 4. Tokenization & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297eb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(train_df: pd.DataFrame, val_df: pd.DataFrame, tokenizer):\n",
    "    \"\"\"Build HuggingFace datasets with tokenization.\"\"\"\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        # Tokenize source\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"src\"],\n",
    "            max_length=CFG.max_source_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"tgt\"],\n",
    "                max_length=CFG.max_target_length,\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "            )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "    val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "    \n",
    "    # Tokenize\n",
    "    train_ds = train_ds.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"src\", \"tgt\"],\n",
    "        desc=\"Tokenizing train\",\n",
    "    )\n",
    "    val_ds = val_ds.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"src\", \"tgt\"],\n",
    "        desc=\"Tokenizing val\",\n",
    "    )\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489bf1d",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072116f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    \"\"\"Build compute_metrics function for Trainer.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)  # chrF++\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Decode predictions\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        # Replace -100 with pad token\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]  # wrapped for sacrebleu\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        \n",
    "        # Geometric mean (competition metric)\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"chrf\": chrf_score,\n",
    "            \"geo_mean\": geo_mean,\n",
    "        }\n",
    "    \n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef93936",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Starting Akkadian V1 Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set seed\n",
    "    set_seed(CFG.seed)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_and_preprocess_data(COMP_DATA_DIR)\n",
    "    train_df, val_df = group_split(df, \"oare_id\", CFG.val_frac, CFG.seed)\n",
    "    print(f\"üìä Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"ü§ñ Loading model: {CFG.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "    \n",
    "    if CFG.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"   ‚úÖ Gradient checkpointing enabled\")\n",
    "    \n",
    "    # Build datasets\n",
    "    train_ds, val_ds = build_datasets(train_df, val_df, tokenizer)\n",
    "    print(f\"   Train tokens: {len(train_ds)}, Val tokens: {len(val_ds)}\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        \n",
    "        # Training\n",
    "        num_train_epochs=CFG.epochs,\n",
    "        per_device_train_batch_size=CFG.batch_size,\n",
    "        per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "        gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "        \n",
    "        # Optimizer\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        warmup_ratio=CFG.warmup_ratio,\n",
    "        \n",
    "        # Precision\n",
    "        fp16=CFG.fp16 and torch.cuda.is_available(),\n",
    "        bf16=CFG.bf16,\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=CFG.eval_strategy,\n",
    "        save_strategy=CFG.save_strategy,\n",
    "        save_total_limit=CFG.save_total_limit,\n",
    "        load_best_model_at_end=CFG.load_best_model_at_end,\n",
    "        metric_for_best_model=CFG.metric_for_best_model,\n",
    "        greater_is_better=CFG.greater_is_better,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CFG.max_target_length,\n",
    "        \n",
    "        # Misc\n",
    "        dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",  # disable wandb etc on Kaggle\n",
    "        seed=CFG.seed,\n",
    "        \n",
    "        # Multi-GPU (Kaggle T4 x2)\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=build_compute_metrics(tokenizer),\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nüèãÔ∏è Training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_dir = OUTPUT_DIR / \"final\"\n",
    "    print(f\"\\nüíæ Saving final model to {final_model_dir}\")\n",
    "    trainer.save_model(str(final_model_dir))\n",
    "    tokenizer.save_pretrained(str(final_model_dir))\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nüìà Final evaluation...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"   BLEU: {eval_results.get('eval_bleu', 'N/A'):.2f}\")\n",
    "    print(f\"   chrF++: {eval_results.get('eval_chrf', 'N/A'):.2f}\")\n",
    "    print(f\"   Geo Mean: {eval_results.get('eval_geo_mean', 'N/A'):.2f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"üìÅ Model saved to: {final_model_dir}\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24141e2",
   "metadata": {},
   "source": [
    "## 7. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ef087",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainer = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f809e2",
   "metadata": {},
   "source": [
    "## 8. Upload to Kaggle Models\n",
    "\n",
    "After training, upload the model to Kaggle Models:\n",
    "\n",
    "```python\n",
    "# In Kaggle notebook, after training:\n",
    "import kaggle\n",
    "\n",
    "# Create model metadata\n",
    "model_dir = \"/kaggle/working/akkadian_v1/final\"\n",
    "\n",
    "# Upload via Kaggle API\n",
    "# kaggle models create -p {model_dir} --title \"akkadian-byt5-v1\"\n",
    "```\n",
    "\n",
    "Or manually:\n",
    "1. Download the `/kaggle/working/akkadian_v1/final` folder\n",
    "2. Go to Kaggle Models ‚Üí New Model\n",
    "3. Upload the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db81e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create a zip for easy download\n",
    "def create_model_zip():\n",
    "    \"\"\"Create a zip file of the trained model for easy download.\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    model_dir = OUTPUT_DIR / \"final\"\n",
    "    if not model_dir.exists():\n",
    "        print(\"‚ùå No model found to zip\")\n",
    "        return\n",
    "    \n",
    "    zip_path = CFG.kaggle_working / \"akkadian_v1_model\"\n",
    "    shutil.make_archive(str(zip_path), 'zip', model_dir)\n",
    "    print(f\"üì¶ Model zipped to: {zip_path}.zip\")\n",
    "\n",
    "\n",
    "# Uncomment to create zip after training:\n",
    "# create_model_zip()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
