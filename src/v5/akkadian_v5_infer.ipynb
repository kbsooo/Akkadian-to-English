{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94050741",
   "metadata": {},
   "source": [
    "# Akkadian V5 Inference\n",
    "\n",
    "Uses V5 normalization and ByT5 tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9ba12",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b565506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "\n",
    "    model_size: str = \"base\"  # \"base\" or \"large\"\n",
    "    model_path: Path = field(init=False)\n",
    "\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v5-base/pytorch/default/1\"\n",
    "        else:\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v5-large/pytorch/default/1\"\n",
    "\n",
    "\n",
    "CFG = Config(model_size=\"base\")\n",
    "\n",
    "# -----------------------------\n",
    "# V5 normalization (inline for Kaggle)\n",
    "# -----------------------------\n",
    "\n",
    "_VOWEL_MAP = {\n",
    "    \"\\u00e0\": \"a\", \"\\u00e1\": \"a\", \"\\u00e2\": \"a\", \"\\u0101\": \"a\", \"\\u00e4\": \"a\",\n",
    "    \"\\u00c0\": \"A\", \"\\u00c1\": \"A\", \"\\u00c2\": \"A\", \"\\u0100\": \"A\", \"\\u00c4\": \"A\",\n",
    "    \"\\u00e8\": \"e\", \"\\u00e9\": \"e\", \"\\u00ea\": \"e\", \"\\u0113\": \"e\", \"\\u00eb\": \"e\",\n",
    "    \"\\u00c8\": \"E\", \"\\u00c9\": \"E\", \"\\u00ca\": \"E\", \"\\u0112\": \"E\", \"\\u00cb\": \"E\",\n",
    "    \"\\u00ec\": \"i\", \"\\u00ed\": \"i\", \"\\u00ee\": \"i\", \"\\u012b\": \"i\", \"\\u00ef\": \"i\",\n",
    "    \"\\u00cc\": \"I\", \"\\u00cd\": \"I\", \"\\u00ce\": \"I\", \"\\u012a\": \"I\", \"\\u00cf\": \"I\",\n",
    "    \"\\u00f2\": \"o\", \"\\u00f3\": \"o\", \"\\u00f4\": \"o\", \"\\u014d\": \"o\", \"\\u00f6\": \"o\",\n",
    "    \"\\u00d2\": \"O\", \"\\u00d3\": \"O\", \"\\u00d4\": \"O\", \"\\u014c\": \"O\", \"\\u00d6\": \"O\",\n",
    "    \"\\u00f9\": \"u\", \"\\u00fa\": \"u\", \"\\u00fb\": \"u\", \"\\u016b\": \"u\", \"\\u00fc\": \"u\",\n",
    "    \"\\u00d9\": \"U\", \"\\u00da\": \"U\", \"\\u00db\": \"U\", \"\\u016a\": \"U\", \"\\u00dc\": \"U\",\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    \"\\u0161\": \"s\", \"\\u0160\": \"S\",\n",
    "    \"\\u1e63\": \"s\", \"\\u1e62\": \"S\",\n",
    "    \"\\u1e6d\": \"t\", \"\\u1e6c\": \"T\",\n",
    "    \"\\u1e2b\": \"h\", \"\\u1e2a\": \"H\",\n",
    "}\n",
    "\n",
    "_QUOTE_MAP = {\n",
    "    \"\\u201e\": '\"', \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\",\n",
    "    \"\\u02be\": \"'\", \"\\u02bf\": \"'\",\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    \"\\u2080\": \"0\", \"\\u2081\": \"1\", \"\\u2082\": \"2\", \"\\u2083\": \"3\", \"\\u2084\": \"4\",\n",
    "    \"\\u2085\": \"5\", \"\\u2086\": \"6\", \"\\u2087\": \"7\", \"\\u2088\": \"8\", \"\\u2089\": \"9\",\n",
    "    \"\\u2093\": \"x\",\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_QUOTE_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Protect literal gap tokens before removing <content> blocks\n",
    "    text = text.replace(\"<gap>\", \"__LIT_GAP__\")\n",
    "    text = text.replace(\"<big_gap>\", \"__LIT_BIG_GAP__\")\n",
    "\n",
    "    # Remove apostrophe line numbers only (1', 1'')\n",
    "    text = re.sub(r\"\\b\\d+'{1,2}\\b\", \" \", text)\n",
    "\n",
    "    # Remove <content> blocks first\n",
    "    text = re.sub(r\"<<([^>]+)>>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]+)>\", r\"\\1\", text)\n",
    "\n",
    "    # Large gaps\n",
    "    text = re.sub(r\"\\[\\s*\\u2026+\\s*\\u2026*\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "    text = re.sub(r\"\\[\\s*\\.\\.\\.+\\s*\\.\\.\\.+\\s*\\]\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # Ellipsis\n",
    "    text = text.replace(\"\\u2026\", \" __BIG_GAP__ \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" __BIG_GAP__ \", text)\n",
    "\n",
    "    # [x]\n",
    "    text = re.sub(r\"\\[\\s*x\\s*\\]\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # [content] -> content\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n",
    "\n",
    "    # Half brackets and variants\n",
    "    text = text.replace(\"\\u2039\", \"\").replace(\"\\u203A\", \"\")\n",
    "    text = text.replace(\"\\u2308\", \"\").replace(\"\\u2309\", \"\")\n",
    "    text = text.replace(\"\\u230A\", \"\").replace(\"\\u230B\", \"\")\n",
    "    text = text.replace(\"\\u02F9\", \"\").replace(\"\\u02FA\", \"\")\n",
    "\n",
    "    # Character maps\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "\n",
    "    # Scribal notations / word divider\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "\n",
    "    # Standalone x\n",
    "    text = re.sub(r\"\\bx\\b\", \" __GAP__ \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Convert placeholders\n",
    "    text = text.replace(\"__GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Restore literal tokens\n",
    "    text = text.replace(\"__LIT_GAP__\", \"<gap>\")\n",
    "    text = text.replace(\"__LIT_BIG_GAP__\", \"<big_gap>\")\n",
    "\n",
    "    # Cleanup\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e307a42",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data not found\")\n",
    "\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_model() -> Path:\n",
    "    if not is_kaggle():\n",
    "        local = Path(f\"outputs/akkadian_v5_{CFG.model_size}/model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "\n",
    "    if CFG.model_path.exists():\n",
    "        return CFG.model_path\n",
    "\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v5\" in d.name.lower() and CFG.model_size in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "\n",
    "    raise FileNotFoundError(f\"V5-{CFG.model_size} model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7e892",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866a06e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V5 Inference: {CFG.model_size.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0bb3c",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a13988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model from: {MODEL_DIR}\")\n",
    "\n",
    "# ByT5 vocab: 256 bytes + specials + extra_ids\n",
    "# Use ByT5Tokenizer for consistency\n",
    "\n",
    "tokenizer = ByT5Tokenizer(extra_ids=125)\n",
    "print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "assert len(tokenizer) == model.config.vocab_size, \"Vocab mismatch!\"\n",
    "print(\"   ‚úÖ Vocab match\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a161a76",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug: bool = False):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "\n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\")\n",
    "    for i in pbar:\n",
    "        batch = texts[i : i + batch_size]\n",
    "        translations.extend(generate_batch(batch))\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171bdc9",
   "metadata": {},
   "source": [
    "## 6. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b6a6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "print(\"\\nüîß Normalizing (V5)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")\n",
    "\n",
    "print(\"\\nüöÄ Running inference...\")\n",
    "print(\"\\n[DEBUG] First sample test...\")\n",
    "_test = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] Translation: '{_test[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "empty_count = sum(1 for t in translations if not t or not t.strip())\n",
    "if empty_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {empty_count} empty translations!\")\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940e2dd",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "assert len(submission) == len(test_df), \"Length mismatch!\"\n",
    "assert submission[\"translation\"].notna().all(), \"NaN values!\"\n",
    "\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V5-{CFG.model_size.upper()} Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
