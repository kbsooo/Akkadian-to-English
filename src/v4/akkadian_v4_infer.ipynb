{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6711818c",
   "metadata": {},
   "source": [
    "# Akkadian V4 Inference\n",
    "\n",
    "**Key Features:**\n",
    "- V4 model (Full FT + OCR-augmented training)\n",
    "- V2-identical normalization (NO augmentation at inference)\n",
    "- ByT5Tokenizer for consistency\n",
    "- NO repetition_penalty (avoid BLEU degradation)\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v4/akkadian_v4_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad91ee",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858890b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fbefa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model selection: \"base\" or \"large\"\n",
    "    model_size: str = \"base\"  # Change to \"large\" for v4-large\n",
    "    \n",
    "    # Model path (set in __post_init__)\n",
    "    model_path: Path = field(init=False)\n",
    "    \n",
    "    # Inference params (same as V2)\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v4-base/pytorch/default/1\"\n",
    "        else:\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v4-large/pytorch/default/1\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è CHANGE THIS FOR v4-large\n",
    "# ============================================\n",
    "CFG = Config(model_size=\"base\")  # \"base\" or \"large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7789773",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8dfab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data directory not found\")\n",
    "    \n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_model():\n",
    "    \"\"\"Find model directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(f\"outputs/akkadian_v4_{CFG.model_size}/model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "    \n",
    "    # Check configured path\n",
    "    if CFG.model_path.exists():\n",
    "        return CFG.model_path\n",
    "    \n",
    "    # Search for v4 model\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v4\" in d.name.lower() and CFG.model_size in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "    \n",
    "    raise FileNotFoundError(f\"V4-{CFG.model_size} model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fc55f",
   "metadata": {},
   "source": [
    "## 3. Normalization (V2-identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a0ff7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',\n",
    "}\n",
    "\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "    '\\u2308': '[', '\\u2309': ']', '\\u230a': '[', '\\u230b': ']',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration to ASCII (V2-identical).\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265fd20",
   "metadata": {},
   "source": [
    "## 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4 Inference: {CFG.model_size.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d89cd9",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3605dc0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model from: {MODEL_DIR}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Use ByT5Tokenizer for consistency (same as training)\n",
    "# ByT5 vocab: 256 (bytes) + 3 (special) + 125 (extra_ids) = 384\n",
    "tokenizer = ByT5Tokenizer(extra_ids=125)\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model vocab size: {model.config.vocab_size}\")\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Verify vocab match\n",
    "assert len(tokenizer) == model.config.vocab_size, \\\n",
    "    f\"Vocab mismatch! Tokenizer: {len(tokenizer)}, Model: {model.config.vocab_size}\"\n",
    "print(\"   ‚úÖ Vocab sizes match\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79d39a",
   "metadata": {},
   "source": [
    "## 6. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ed07c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug=False):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # NOTE: NO repetition_penalty (can hurt BLEU on short sequences)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "        print(f\"   [DEBUG] Output tokens (first 20): {outputs[0][:20].tolist()}\")\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Decoded (first): '{decoded[0][:100]}'\")\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\", ncols=80)\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results = generate_batch(batch)\n",
    "        translations.extend(results)\n",
    "        pbar.set_postfix(done=f\"{min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60189bae",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a957c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "print(\"\\nüîß Normalizing (V2-identical)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "\n",
    "print(\"\\n[DEBUG] Testing first sample...\")\n",
    "test_result = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] First translation: '{test_result[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "# Validate outputs\n",
    "empty_count = sum(1 for t in translations if not t or not t.strip())\n",
    "if empty_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {empty_count} empty translations!\")\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c865c",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c83ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no empty translations (replace with placeholder if needed)\n",
    "translations = [t if t and t.strip() else \"[Translation unavailable]\" for t in translations]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Validate\n",
    "assert len(submission) == len(test_df), \"Submission length mismatch!\"\n",
    "assert submission[\"translation\"].notna().all(), \"Submission has NaN values!\"\n",
    "\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V4-{CFG.model_size.upper()} Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
