{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab4e410",
   "metadata": {},
   "source": [
    "# Akkadian V4 Inference\n",
    "\n",
    "**Key Features:**\n",
    "- V4 model (Full FT + OCR-augmented training)\n",
    "- V2-identical normalization (NO augmentation at inference)\n",
    "- Works on Kaggle Internet OFF\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v4/akkadian_v4_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6aa3d",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0ebaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae5ea6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model selection: \"base\" or \"large\"\n",
    "    model_size: str = \"base\"  # Change to \"large\" for v4-large\n",
    "    \n",
    "    # Model path (set in __post_init__)\n",
    "    model_path: Path = None\n",
    "    \n",
    "    # Inference params (same as V2)\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Find model path based on model_size\n",
    "        # You may need to adjust these paths based on your Kaggle dataset\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v4-base/pytorch/default/1\"\n",
    "        else:\n",
    "            self.model_path = self.kaggle_input / \"akkadian-v4-large/pytorch/default/1\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è CHANGE THIS FOR v4-large\n",
    "# ============================================\n",
    "CFG = Config(model_size=\"base\")  # \"base\" or \"large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef36297",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f02e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data directory not found\")\n",
    "    \n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_model():\n",
    "    \"\"\"Find model directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(f\"outputs/akkadian_v4_{CFG.model_size}/model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local model not found\")\n",
    "    \n",
    "    # Check configured path first\n",
    "    if CFG.model_path.exists():\n",
    "        return CFG.model_path\n",
    "    \n",
    "    # Search for v4 model\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v4\" in d.name.lower() and CFG.model_size in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "    \n",
    "    raise FileNotFoundError(f\"V4-{CFG.model_size} model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8464222",
   "metadata": {},
   "source": [
    "## 3. Normalization (V2-identical, NO augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb779d9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Character Mapping Tables (copied from V2 normalize.py)\n",
    "# ==============================================================================\n",
    "\n",
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',\n",
    "}\n",
    "\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"',\n",
    "    '\\u201c': '\"',\n",
    "    '\\u201d': '\"',\n",
    "    '\\u2018': \"'\",\n",
    "    '\\u2019': \"'\",\n",
    "    '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\",\n",
    "    '\\u02bf': \"'\",\n",
    "    '\\u2308': '[',\n",
    "    '\\u2309': ']',\n",
    "    '\\u230a': '[',\n",
    "    '\\u230b': ']',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Akkadian transliteration to ASCII-compatible format.\n",
    "    \n",
    "    IDENTICAL to V2 normalize.py - NO augmentation at inference.\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a0b55",
   "metadata": {},
   "source": [
    "## 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4 Inference: {CFG.model_size.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_model()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038147b",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d529aa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model from: {MODEL_DIR}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa951c",
   "metadata": {},
   "source": [
    "## 6. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521b35e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug=False):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "        print(f\"   [DEBUG] Output tokens (first 20): {outputs[0][:20].tolist()}\")\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Decoded (first): '{decoded[0][:100]}'\")\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\", ncols=80)\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results = generate_batch(batch)\n",
    "        translations.extend(results)\n",
    "        pbar.set_postfix(done=f\"{min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58221368",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8826bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "print(\"\\nüîß Normalizing (V2-identical, NO augmentation)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "\n",
    "print(\"\\n[DEBUG] Testing first sample...\")\n",
    "test_result = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] First translation: '{test_result[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3456c69",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V4-{CFG.model_size.upper()} Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
