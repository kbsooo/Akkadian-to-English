{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dfde93",
   "metadata": {},
   "source": [
    "# Akkadian V4 Training: Full FT + OCR Noise Augmentation\n",
    "\n",
    "**Key Features:**\n",
    "- ByT5 (base or large) with Full Fine-tuning\n",
    "- OCR noise augmentation for robustness\n",
    "- Same data as V2 (v2_sentence_train.csv)\n",
    "\n",
    "**Environment**: Google Colab with A100 GPU\n",
    "\n",
    "**Output**: Saved to Google Drive `/content/drive/MyDrive/akkadian/v4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b2994",
   "metadata": {},
   "source": [
    "## 0. Setup: Mount Drive & Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub login and data download\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Kaggle\n",
    "kbsooo_akkadian_v2_data_path = kagglehub.dataset_download('kbsooo/akkadian-v2-data')\n",
    "print(f'Data downloaded to: {kbsooo_akkadian_v2_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4f86e",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94423af3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fe12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for V4: Full FT + OCR Augmentation.\"\"\"\n",
    "    \n",
    "    # Model selection: \"base\" or \"large\"\n",
    "    model_size: str = \"base\"  # Change to \"large\" for v4-large\n",
    "    \n",
    "    # Paths (Colab + Google Drive)\n",
    "    data_dir: Path = None  # Set after kagglehub download\n",
    "    output_dir: Path = None  # Set after model_size is determined\n",
    "    \n",
    "    # Data files\n",
    "    train_file: str = \"v2_sentence_train.csv\"\n",
    "    val_file: str = \"v2_sentence_val.csv\"\n",
    "    \n",
    "    # OCR Augmentation\n",
    "    augment_prob: float = 0.3  # Probability of applying noise to each sample\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    epochs: int = 10\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Hardware\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # Model-specific settings (set in __post_init__)\n",
    "    model_name: str = field(init=False)\n",
    "    batch_size: int = field(init=False)\n",
    "    gradient_accumulation_steps: int = field(init=False)\n",
    "    learning_rate: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_name = \"google/byt5-base\"\n",
    "            self.batch_size = 4\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.learning_rate = 1e-4\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-base\")\n",
    "        else:\n",
    "            self.model_name = \"google/byt5-large\"\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.learning_rate = 5e-5\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-large\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è CHANGE THIS FOR v4-large\n",
    "# ============================================\n",
    "CFG = Config(model_size=\"base\")  # \"base\" or \"large\"\n",
    "\n",
    "# Set data directory\n",
    "CFG.data_dir = Path(kbsooo_akkadian_v2_data_path)\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4: {CFG.model_size.upper()} + Full FT + OCR Augmentation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Data directory: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output directory: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üé≤ Augment prob: {CFG.augment_prob}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cbbd3",
   "metadata": {},
   "source": [
    "## 2. OCR Noise Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dbea3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# OCR Noise Augmentation Functions\n",
    "# ==============================================================================\n",
    "\n",
    "# Diacritics that can be dropped\n",
    "_DIACRITICS_MAP = {\n",
    "    '≈°': 's', '≈†': 'S',\n",
    "    '·π£': 's', '·π¢': 'S',\n",
    "    '·π≠': 't', '·π¨': 'T',\n",
    "    '·∏´': 'h', '·∏™': 'H',\n",
    "    'ƒÅ': 'a', 'ƒÄ': 'A',\n",
    "    'ƒì': 'e', 'ƒí': 'E',\n",
    "    'ƒ´': 'i', 'ƒ™': 'I',\n",
    "    '≈´': 'u', '≈™': 'U',\n",
    "}\n",
    "\n",
    "# Quote variations (can go both ways)\n",
    "_QUOTE_PAIRS = [\n",
    "    ('\"', '‚Äû'),\n",
    "    ('\"', '\"'),\n",
    "    (\"'\", '''),\n",
    "    (\"'\", '''),\n",
    "]\n",
    "\n",
    "# Subscript variations\n",
    "_SUBSCRIPT_MAP = {\n",
    "    '‚ÇÄ': '0', '‚ÇÅ': '1', '‚ÇÇ': '2', '‚ÇÉ': '3', '‚ÇÑ': '4',\n",
    "    '‚ÇÖ': '5', '‚ÇÜ': '6', '‚Çá': '7', '‚Çà': '8', '‚Çâ': '9',\n",
    "}\n",
    "\n",
    "\n",
    "def drop_diacritics(text: str, prob: float = 0.5) -> str:\n",
    "    \"\"\"Randomly drop diacritics from characters.\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _DIACRITICS_MAP and random.random() < prob:\n",
    "            result.append(_DIACRITICS_MAP[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def vary_quotes(text: str) -> str:\n",
    "    \"\"\"Randomly swap quote styles.\"\"\"\n",
    "    for orig, alt in _QUOTE_PAIRS:\n",
    "        if random.random() < 0.5:\n",
    "            text = text.replace(orig, alt)\n",
    "    return text\n",
    "\n",
    "\n",
    "def vary_subscripts(text: str, prob: float = 0.5) -> str:\n",
    "    \"\"\"Randomly convert subscripts to regular numbers or vice versa.\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _SUBSCRIPT_MAP and random.random() < prob:\n",
    "            result.append(_SUBSCRIPT_MAP[char])\n",
    "        elif char.isdigit() and random.random() < prob * 0.3:\n",
    "            # Occasionally convert number to subscript\n",
    "            subscripts = '‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ'\n",
    "            result.append(subscripts[int(char)])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def drop_hyphens(text: str, prob: float = 0.2) -> str:\n",
    "    \"\"\"Randomly drop hyphens between syllables.\"\"\"\n",
    "    if random.random() < prob:\n",
    "        return text.replace('-', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_ocr_artifacts(text: str, prob: float = 0.1) -> str:\n",
    "    \"\"\"Randomly insert OCR-like artifacts.\"\"\"\n",
    "    if random.random() < prob:\n",
    "        artifacts = ['¬∑', '¬∞', '+', '√ó']\n",
    "        pos = random.randint(0, len(text))\n",
    "        artifact = random.choice(artifacts)\n",
    "        return text[:pos] + artifact + text[pos:]\n",
    "    return text\n",
    "\n",
    "\n",
    "def vary_brackets(text: str) -> str:\n",
    "    \"\"\"Vary bracket styles.\"\"\"\n",
    "    variations = [\n",
    "        ('[', '‚åà'), (']', '‚åâ'),\n",
    "        ('[', '‚åä'), (']', '‚åã'),\n",
    "    ]\n",
    "    for orig, alt in variations:\n",
    "        if random.random() < 0.3:\n",
    "            text = text.replace(orig, alt)\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_ocr_noise(text: str, prob: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Apply random OCR-like noise to transliteration.\n",
    "    \n",
    "    This teaches the model to handle various input styles.\n",
    "    Only applied during TRAINING, not inference.\n",
    "    \"\"\"\n",
    "    if random.random() > prob:\n",
    "        return text  # No augmentation for this sample\n",
    "    \n",
    "    # Apply one or more noise types\n",
    "    noise_funcs = [\n",
    "        lambda t: drop_diacritics(t, prob=0.3),\n",
    "        vary_quotes,\n",
    "        lambda t: vary_subscripts(t, prob=0.3),\n",
    "        lambda t: drop_hyphens(t, prob=0.15),\n",
    "        lambda t: add_ocr_artifacts(t, prob=0.1),\n",
    "        vary_brackets,\n",
    "    ]\n",
    "    \n",
    "    # Apply 1-3 random noise functions\n",
    "    n_funcs = random.randint(1, 3)\n",
    "    selected_funcs = random.sample(noise_funcs, n_funcs)\n",
    "    \n",
    "    for func in selected_funcs:\n",
    "        text = func(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test augmentation\n",
    "print(\"\\nüìù OCR Augmentation Examples:\")\n",
    "test_text = \"≈°um-ma a-wi-lum ·π£a-bi-tam i-na ·∏´u-bu-ul-li-≈°u\"\n",
    "for i in range(5):\n",
    "    augmented = apply_ocr_noise(test_text, prob=1.0)\n",
    "    print(f\"   [{i}] {augmented}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fb05a",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load pre-normalized sentence-level data.\"\"\"\n",
    "    train_path = CFG.data_dir / CFG.train_file\n",
    "    val_path = CFG.data_dir / CFG.val_file\n",
    "    \n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "    if not val_path.exists():\n",
    "        raise FileNotFoundError(f\"Val file not found: {val_path}\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    \n",
    "    # Validate columns\n",
    "    required_cols = {\"src\", \"tgt\"}\n",
    "    for name, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            missing = required_cols - set(df.columns)\n",
    "            raise ValueError(f\"Missing columns in {name}: {missing}\")\n",
    "    \n",
    "    # Drop NaN\n",
    "    train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "    val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "print(\"üìñ Loading data...\")\n",
    "train_df, val_df = load_data()\n",
    "\n",
    "print(f\"   Train: {len(train_df):,} samples\")\n",
    "print(f\"   Val: {len(val_df):,} samples\")\n",
    "\n",
    "print(f\"\\nüìù Sample (original):\")\n",
    "print(f\"   src: {train_df.iloc[0]['src'][:80]}...\")\n",
    "print(f\"   tgt: {train_df.iloc[0]['tgt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c57b8",
   "metadata": {},
   "source": [
    "## 4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8103f13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model: {CFG.model_name}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ‚úÖ Gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8be21",
   "metadata": {},
   "source": [
    "## 5. Tokenization with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882cb4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_with_augment(examples, augment=True):\n",
    "    \"\"\"Tokenize with optional OCR augmentation on source.\"\"\"\n",
    "    # Apply augmentation to source (training only)\n",
    "    if augment:\n",
    "        sources = [apply_ocr_noise(s, prob=CFG.augment_prob) for s in examples[\"src\"]]\n",
    "    else:\n",
    "        sources = examples[\"src\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        sources,\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"\\nüî§ Tokenizing datasets...\")\n",
    "\n",
    "# Train with augmentation\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(\n",
    "    lambda x: tokenize_with_augment(x, augment=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"src\", \"tgt\"],\n",
    "    desc=\"Tokenizing train (with augmentation)\"\n",
    ")\n",
    "\n",
    "# Validation without augmentation (fair evaluation)\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(\n",
    "    lambda x: tokenize_with_augment(x, augment=False),\n",
    "    batched=True,\n",
    "    remove_columns=[\"src\", \"tgt\"],\n",
    "    desc=\"Tokenizing val (no augmentation)\"\n",
    ")\n",
    "\n",
    "print(f\"   Train: {len(train_ds):,} samples\")\n",
    "print(f\"   Val: {len(val_ds):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a190ef2",
   "metadata": {},
   "source": [
    "## 6. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c765237",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    \"\"\"Build metrics computation function.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "        \n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130648d2",
   "metadata": {},
   "source": [
    "## 7. Logging Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b6a9f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TqdmLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Enhanced logging with clear metrics display.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_epoch = 0\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.current_epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.train_loss = []\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Epoch {self.current_epoch + 1}/{args.num_train_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.train_loss.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.train_loss:\n",
    "            avg_loss = sum(self.train_loss) / len(self.train_loss)\n",
    "            print(f\"\\nüìâ Epoch {self.current_epoch + 1} Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"üìà Validation Results (Epoch {self.current_epoch + 1})\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "            print(f\"   Loss:     {metrics.get('eval_loss', 0):.4f}\")\n",
    "            print(f\"   BLEU:     {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF++:   {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo Mean: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéâ Training Complete!\")\n",
    "        print(f\"   Total steps: {state.global_step:,}\")\n",
    "        if state.best_metric:\n",
    "            print(f\"   Best metric: {state.best_metric:.2f}\")\n",
    "        print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab98a3",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9841788",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    max_grad_norm=CFG.max_grad_norm,\n",
    "    fp16=CFG.fp16 and torch.cuda.is_available(),\n",
    "    bf16=CFG.bf16 and torch.cuda.is_available(),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[TqdmLoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27671b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "print(f\"   Model: {CFG.model_size.upper()}\")\n",
    "print(f\"   Epochs: {CFG.epochs}\")\n",
    "print(f\"   Batch size: {CFG.batch_size} x {CFG.gradient_accumulation_steps} = {CFG.batch_size * CFG.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {CFG.learning_rate}\")\n",
    "print(f\"   Augment prob: {CFG.augment_prob}\")\n",
    "print()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7d3c2",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40160d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = CFG.output_dir / \"model\"\n",
    "print(f\"\\nüíæ Saving model to: {model_dir}\")\n",
    "trainer.save_model(str(model_dir))\n",
    "tokenizer.save_pretrained(str(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nüìà Final Evaluation:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"   BLEU:     {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"   chrF++:   {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"   Geo Mean: {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663b3ee",
   "metadata": {},
   "source": [
    "## 10. Create Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c933f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = CFG.output_dir / f\"akkadian_v4_{CFG.model_size}\"\n",
    "shutil.make_archive(str(zip_path), 'zip', model_dir)\n",
    "print(f\"\\nüì¶ Model archived: {zip_path}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V4-{CFG.model_size.upper()} Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Model: {model_dir}\")\n",
    "print(f\"üì¶ Archive: {zip_path}.zip\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the archive from Google Drive\")\n",
    "print(\"2. Upload to Kaggle as a dataset for inference\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
