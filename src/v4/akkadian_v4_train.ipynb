{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3b8d2b",
   "metadata": {},
   "source": [
    "# Akkadian V4 Training: Full FT + OCR Noise Augmentation\n",
    "\n",
    "**Key Features:**\n",
    "- ByT5 (base or large) with Full Fine-tuning\n",
    "- OCR noise augmentation on RAW data (not pre-normalized)\n",
    "- Original train.csv + published_texts.csv for more diversity\n",
    "\n",
    "**Environment**: Google Colab with A100 GPU\n",
    "\n",
    "**Output**: Saved to Google Drive `/content/drive/MyDrive/akkadian/v4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc9cc9",
   "metadata": {},
   "source": [
    "## 0. Setup: Mount Drive & Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63dd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub login and data download\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dc091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download competition data (contains raw train.csv)\n",
    "competition_path = kagglehub.dataset_download('deep-past-initiative-machine-translation')\n",
    "print(f'Competition data: {competition_path}')\n",
    "\n",
    "# Download published_texts for augmentation\n",
    "published_path = kagglehub.dataset_download('kbsooo/akkadian-v2-data')\n",
    "print(f'V2 data (published_texts): {published_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486fe02f",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91914d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cefad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for V4: Full FT + OCR Augmentation.\"\"\"\n",
    "    \n",
    "    # Model selection: \"base\" or \"large\"\n",
    "    model_size: str = \"base\"  # Change to \"large\" for v4-large\n",
    "    \n",
    "    # Paths (set after data download)\n",
    "    competition_dir: Path = None\n",
    "    v2_data_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    \n",
    "    # OCR Augmentation\n",
    "    augment_prob: float = 0.4  # Probability of applying noise\n",
    "    use_published_texts: bool = True  # Include published_texts.csv\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    val_size: float = 0.1\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    epochs: int = 10\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Hardware\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # Model-specific settings (set in __post_init__)\n",
    "    model_name: str = field(init=False)\n",
    "    batch_size: int = field(init=False)\n",
    "    gradient_accumulation_steps: int = field(init=False)\n",
    "    learning_rate: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_name = \"google/byt5-base\"\n",
    "            self.batch_size = 4\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.learning_rate = 1e-4\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-base\")\n",
    "        else:\n",
    "            self.model_name = \"google/byt5-large\"\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.learning_rate = 5e-5\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-large\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è CHANGE THIS FOR v4-large\n",
    "# ============================================\n",
    "CFG = Config(model_size=\"base\")  # \"base\" or \"large\"\n",
    "\n",
    "# Set data directories\n",
    "CFG.competition_dir = Path(competition_path)\n",
    "CFG.v2_data_dir = Path(published_path)\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4: {CFG.model_size.upper()} + Full FT + OCR Augmentation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Competition data: {CFG.competition_dir}\")\n",
    "print(f\"üìÅ V2 data: {CFG.v2_data_dir}\")\n",
    "print(f\"üìÅ Output: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üé≤ Augment prob: {CFG.augment_prob}\")\n",
    "print(f\"üìö Use published_texts: {CFG.use_published_texts}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3d758",
   "metadata": {},
   "source": [
    "## 2. OCR Noise Augmentation (for RAW data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c12db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# OCR Noise Augmentation Functions\n",
    "# Applied to RAW transliteration (with diacritics) for maximum effect\n",
    "# ==============================================================================\n",
    "\n",
    "# Diacritics that can be dropped (simulating OCR errors)\n",
    "_DIACRITICS_DROP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈°, ≈†\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£, ·π¢\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠, ·π¨\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´, ·∏™\n",
    "    '\\u0101': 'a', '\\u0100': 'A',  # ƒÅ, ƒÄ\n",
    "    '\\u0113': 'e', '\\u0112': 'E',  # ƒì, ƒí\n",
    "    '\\u012b': 'i', '\\u012a': 'I',  # ƒ´, ƒ™\n",
    "    '\\u016b': 'u', '\\u016a': 'U',  # ≈´, ≈™\n",
    "}\n",
    "\n",
    "# Quote variations (ASCII and Unicode)\n",
    "_QUOTE_VARIATIONS = [\n",
    "    ('\"', '\\u201e'),  # \" ‚Üí ‚Äû\n",
    "    ('\"', '\\u201c'),  # \" ‚Üí \"\n",
    "    ('\"', '\\u201d'),  # \" ‚Üí \"\n",
    "    (\"'\", '\\u2018'),  # ' ‚Üí '\n",
    "    (\"'\", '\\u2019'),  # ' ‚Üí '\n",
    "]\n",
    "\n",
    "# Subscript ‚Üî number\n",
    "_SUBSCRIPT_TO_NUM = {\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "}\n",
    "_NUM_TO_SUBSCRIPT = {v: k for k, v in _SUBSCRIPT_TO_NUM.items()}\n",
    "\n",
    "\n",
    "def _drop_diacritics(text: str, prob: float = 0.4) -> str:\n",
    "    \"\"\"Randomly drop diacritics (simulating OCR/ASCII degradation).\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _DIACRITICS_DROP and random.random() < prob:\n",
    "            result.append(_DIACRITICS_DROP[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _vary_quotes(text: str) -> str:\n",
    "    \"\"\"Randomly swap quote styles.\"\"\"\n",
    "    for orig, alt in _QUOTE_VARIATIONS:\n",
    "        if random.random() < 0.3:\n",
    "            if random.random() < 0.5:\n",
    "                text = text.replace(orig, alt)\n",
    "            else:\n",
    "                text = text.replace(alt, orig)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _vary_subscripts(text: str, prob: float = 0.3) -> str:\n",
    "    \"\"\"Randomly convert subscripts ‚Üî numbers.\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _SUBSCRIPT_TO_NUM and random.random() < prob:\n",
    "            result.append(_SUBSCRIPT_TO_NUM[char])\n",
    "        elif char in _NUM_TO_SUBSCRIPT and random.random() < prob * 0.3:\n",
    "            result.append(_NUM_TO_SUBSCRIPT[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _drop_hyphens(text: str, prob: float = 0.1) -> str:\n",
    "    \"\"\"Randomly drop some hyphens (but not all).\"\"\"\n",
    "    if random.random() > prob:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if random.random() < 0.3:\n",
    "            word = word.replace('-', '')\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def _vary_brackets(text: str) -> str:\n",
    "    \"\"\"Vary bracket styles (philological notation).\"\"\"\n",
    "    if random.random() < 0.2:\n",
    "        text = text.replace('[', '\\u2308').replace(']', '\\u2309')  # ‚åà ‚åâ\n",
    "    if random.random() < 0.2:\n",
    "        text = text.replace('[', '\\u230a').replace(']', '\\u230b')  # ‚åä ‚åã\n",
    "    return text\n",
    "\n",
    "\n",
    "def _protect_special_tokens(text: str) -> tuple:\n",
    "    \"\"\"Extract and protect special tokens like <gap>, <unk>.\"\"\"\n",
    "    protected = {}\n",
    "    counter = 0\n",
    "    for token in ['<gap>', '<unk>', '<GAP>', '<UNK>']:\n",
    "        while token in text:\n",
    "            placeholder = f\"__PROTECTED_{counter}__\"\n",
    "            text = text.replace(token, placeholder, 1)\n",
    "            protected[placeholder] = token\n",
    "            counter += 1\n",
    "    return text, protected\n",
    "\n",
    "\n",
    "def _restore_special_tokens(text: str, protected: dict) -> str:\n",
    "    \"\"\"Restore protected special tokens.\"\"\"\n",
    "    for placeholder, token in protected.items():\n",
    "        text = text.replace(placeholder, token)\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_ocr_noise(text: str, prob: float = 0.4) -> str:\n",
    "    \"\"\"\n",
    "    Apply random OCR-like noise to RAW transliteration.\n",
    "    \n",
    "    Protects special tokens (<gap>, <unk>) from corruption.\n",
    "    Only applied during TRAINING, not inference.\n",
    "    \"\"\"\n",
    "    if not text or random.random() > prob:\n",
    "        return text\n",
    "    \n",
    "    # Protect special tokens\n",
    "    text, protected = _protect_special_tokens(text)\n",
    "    \n",
    "    # Apply 1-3 random noise functions\n",
    "    noise_funcs = [\n",
    "        lambda t: _drop_diacritics(t, prob=0.3),\n",
    "        _vary_quotes,\n",
    "        lambda t: _vary_subscripts(t, prob=0.2),\n",
    "        lambda t: _drop_hyphens(t, prob=0.1),\n",
    "        _vary_brackets,\n",
    "    ]\n",
    "    \n",
    "    n_funcs = random.randint(1, min(3, len(noise_funcs)))\n",
    "    selected_funcs = random.sample(noise_funcs, n_funcs)\n",
    "    \n",
    "    for func in selected_funcs:\n",
    "        text = func(text)\n",
    "    \n",
    "    # Restore special tokens\n",
    "    text = _restore_special_tokens(text, protected)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test augmentation\n",
    "print(\"\\nüìù OCR Augmentation Examples:\")\n",
    "test_text = \"≈°um-ma a-wi-lum ·π£a-bi-tam i-na ·∏´u-bu-ul-li-≈°u <gap>\"\n",
    "for i in range(5):\n",
    "    augmented = apply_ocr_noise(test_text, prob=1.0)\n",
    "    print(f\"   [{i}] {augmented}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f0668",
   "metadata": {},
   "source": [
    "## 3. Normalization (for consistent output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8b499",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Normalization (V2-identical) - applied to BOTH train source and target\n",
    "# ==============================================================================\n",
    "\n",
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',\n",
    "}\n",
    "\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "    '\\u2308': '[', '\\u2309': ']', '\\u230a': '[', '\\u230b': ']',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration to ASCII (V2-identical).\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_translation(text) -> str:\n",
    "    \"\"\"Normalize English translation.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3161e55",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    \"\"\"Load RAW data from competition + published_texts.\"\"\"\n",
    "    \n",
    "    # 1. Load original train.csv (RAW, with diacritics)\n",
    "    train_path = CFG.competition_dir / \"train.csv\"\n",
    "    if not train_path.exists():\n",
    "        # Try alternative paths\n",
    "        for p in CFG.competition_dir.glob(\"**/train.csv\"):\n",
    "            train_path = p\n",
    "            break\n",
    "    \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f\"   Raw train.csv: {len(train_df)} rows\")\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    train_df = train_df.rename(columns={\n",
    "        'transliteration': 'src_raw',\n",
    "        'translation': 'tgt_raw'\n",
    "    })\n",
    "    train_df['source'] = 'train'\n",
    "    \n",
    "    # 2. Load published_texts.csv if enabled\n",
    "    if CFG.use_published_texts:\n",
    "        pub_path = CFG.competition_dir / \"published_texts.csv\"\n",
    "        if not pub_path.exists():\n",
    "            for p in CFG.competition_dir.glob(\"**/published_texts.csv\"):\n",
    "                pub_path = p\n",
    "                break\n",
    "        \n",
    "        if pub_path.exists():\n",
    "            pub_df = pd.read_csv(pub_path)\n",
    "            print(f\"   published_texts.csv: {len(pub_df)} rows\")\n",
    "            \n",
    "            # Only use rows with both src and tgt\n",
    "            pub_df = pub_df.rename(columns={\n",
    "                'transliteration': 'src_raw',\n",
    "                'translation': 'tgt_raw'\n",
    "            })\n",
    "            pub_df = pub_df.dropna(subset=['src_raw', 'tgt_raw'])\n",
    "            pub_df['source'] = 'published'\n",
    "            \n",
    "            print(f\"   published_texts with translations: {len(pub_df)} rows\")\n",
    "            \n",
    "            # Combine\n",
    "            train_df = pd.concat([train_df, pub_df], ignore_index=True)\n",
    "    \n",
    "    # Drop NaN\n",
    "    train_df = train_df.dropna(subset=['src_raw', 'tgt_raw']).reset_index(drop=True)\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "print(\"üìñ Loading raw data...\")\n",
    "raw_df = load_raw_data()\n",
    "print(f\"   Total: {len(raw_df)} rows\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìù Sample (RAW):\")\n",
    "print(f\"   src: {raw_df.iloc[0]['src_raw'][:80]}...\")\n",
    "print(f\"   tgt: {raw_df.iloc[0]['tgt_raw'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9b014",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "print(f\"\\nüîÄ Splitting into train/val (val_size={CFG.val_size})...\")\n",
    "train_df, val_df = train_test_split(\n",
    "    raw_df, test_size=CFG.val_size, random_state=CFG.seed\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"   Train: {len(train_df)} rows\")\n",
    "print(f\"   Val: {len(val_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f2c74",
   "metadata": {},
   "source": [
    "## 5. Data Preparation with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc60f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, augment=False):\n",
    "    \"\"\"\n",
    "    Prepare data for training:\n",
    "    1. (Optional) Apply OCR noise to source\n",
    "    2. Normalize both source and target\n",
    "    \"\"\"\n",
    "    prepared = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing\"):\n",
    "        src_raw = row['src_raw']\n",
    "        tgt_raw = row['tgt_raw']\n",
    "        \n",
    "        # Apply OCR noise to RAW source (before normalization)\n",
    "        if augment:\n",
    "            src_noisy = apply_ocr_noise(src_raw, prob=CFG.augment_prob)\n",
    "        else:\n",
    "            src_noisy = src_raw\n",
    "        \n",
    "        # Normalize both\n",
    "        src = normalize_transliteration(src_noisy)\n",
    "        tgt = normalize_translation(tgt_raw)\n",
    "        \n",
    "        if src and tgt:  # Skip empty\n",
    "            prepared.append({'src': src, 'tgt': tgt})\n",
    "    \n",
    "    return pd.DataFrame(prepared)\n",
    "\n",
    "\n",
    "print(\"\\nüîß Preparing training data (with augmentation)...\")\n",
    "train_prepared = prepare_training_data(train_df, augment=True)\n",
    "print(f\"   Prepared train: {len(train_prepared)} rows\")\n",
    "\n",
    "print(\"\\nüîß Preparing validation data (no augmentation)...\")\n",
    "val_prepared = prepare_training_data(val_df, augment=False)\n",
    "print(f\"   Prepared val: {len(val_prepared)} rows\")\n",
    "\n",
    "print(f\"\\nüìù Sample (after augmentation + normalization):\")\n",
    "print(f\"   src: {train_prepared.iloc[0]['src'][:80]}...\")\n",
    "print(f\"   tgt: {train_prepared.iloc[0]['tgt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba758b",
   "metadata": {},
   "source": [
    "## 6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3626229",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model: {CFG.model_name}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "print(f\"   Model vocab: {model.config.vocab_size}\")\n",
    "print(f\"   Model params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Verify vocab match\n",
    "assert len(tokenizer) == model.config.vocab_size, \\\n",
    "    f\"Vocab mismatch! Tokenizer: {len(tokenizer)}, Model: {model.config.vocab_size}\"\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ‚úÖ Gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7623a",
   "metadata": {},
   "source": [
    "## 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cc8da",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    \"\"\"Tokenize source and target.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"\\nüî§ Tokenizing datasets...\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_prepared[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "val_ds = Dataset.from_pandas(val_prepared[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "print(f\"   Train: {len(train_ds)} samples\")\n",
    "print(f\"   Val: {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0521c",
   "metadata": {},
   "source": [
    "## 8. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b5234",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    \"\"\"Build metrics computation function.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "        \n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54adaa",
   "metadata": {},
   "source": [
    "## 9. Logging Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604826d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TqdmLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Enhanced logging with clear metrics display.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_epoch = 0\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.current_epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.train_loss = []\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Epoch {self.current_epoch + 1}/{args.num_train_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.train_loss.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.train_loss:\n",
    "            avg_loss = sum(self.train_loss) / len(self.train_loss)\n",
    "            print(f\"\\nüìâ Epoch {self.current_epoch + 1} Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"üìà Validation Results (Epoch {self.current_epoch + 1})\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "            print(f\"   Loss:     {metrics.get('eval_loss', 0):.4f}\")\n",
    "            print(f\"   BLEU:     {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF++:   {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo Mean: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéâ Training Complete!\")\n",
    "        print(f\"   Total steps: {state.global_step:,}\")\n",
    "        if state.best_metric:\n",
    "            print(f\"   Best metric: {state.best_metric:.2f}\")\n",
    "        print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0c730",
   "metadata": {},
   "source": [
    "## 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    max_grad_norm=CFG.max_grad_norm,\n",
    "    fp16=CFG.fp16 and torch.cuda.is_available(),\n",
    "    bf16=CFG.bf16 and torch.cuda.is_available(),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[TqdmLoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ac18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "print(f\"   Model: {CFG.model_size.upper()}\")\n",
    "print(f\"   Data: {len(train_ds)} train, {len(val_ds)} val\")\n",
    "print(f\"   Epochs: {CFG.epochs}\")\n",
    "print(f\"   Batch: {CFG.batch_size} x {CFG.gradient_accumulation_steps} = {CFG.batch_size * CFG.gradient_accumulation_steps}\")\n",
    "print(f\"   LR: {CFG.learning_rate}\")\n",
    "print(f\"   Augment: {CFG.augment_prob}\")\n",
    "print()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0ec24",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = CFG.output_dir / \"model\"\n",
    "print(f\"\\nüíæ Saving model to: {model_dir}\")\n",
    "trainer.save_model(str(model_dir))\n",
    "tokenizer.save_pretrained(str(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà Final Evaluation:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"   BLEU:     {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"   chrF++:   {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"   Geo Mean: {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f785d2",
   "metadata": {},
   "source": [
    "## 12. Create Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = CFG.output_dir / f\"akkadian_v4_{CFG.model_size}\"\n",
    "shutil.make_archive(str(zip_path), 'zip', model_dir)\n",
    "print(f\"\\nüì¶ Model archived: {zip_path}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ V4-{CFG.model_size.upper()} Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Model: {model_dir}\")\n",
    "print(f\"üì¶ Archive: {zip_path}.zip\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the archive from Google Drive\")\n",
    "print(\"2. Upload to Kaggle as a dataset for inference\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
