{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3886284a",
   "metadata": {},
   "source": [
    "# Akkadian V4 Training: Full FT + OCR Noise Augmentation\n",
    "\n",
    "**Key Features:**\n",
    "- ByT5 (base or large) with Full Fine-tuning\n",
    "- OCR noise augmentation on RAW data\n",
    "- Original train.csv + published_texts.csv (AICC_translation)\n",
    "- oare_id-based train/val split to prevent leakage\n",
    "\n",
    "**Environment**: Google Colab with A100 GPU\n",
    "\n",
    "**Output**: Saved to Google Drive `/content/drive/MyDrive/akkadian/v4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ae6c8",
   "metadata": {},
   "source": [
    "## 0. Setup: Mount Drive & Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f252dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub login and data download\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download competition data\n",
    "competition_path = kagglehub.competition_download('deep-past-initiative-machine-translation')\n",
    "print(f'Competition data: {competition_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9c6d",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8a4a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for V4: Full FT + OCR Augmentation.\"\"\"\n",
    "    \n",
    "    # Model selection: \"base\" or \"large\"\n",
    "    model_size: str = \"base\"  # Change to \"large\" for v4-large\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: Path = None  # Set after download\n",
    "    output_dir: Path = None\n",
    "    \n",
    "    # OCR Augmentation - applied BEFORE normalization\n",
    "    augment_prob: float = 0.4\n",
    "    use_published_texts: bool = False  # Reserved for DAPT phase\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    val_size: float = 0.1\n",
    "    max_source_length: int = 512\n",
    "    max_target_length: int = 512\n",
    "    epochs: int = 10\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Hardware\n",
    "    fp16: bool = False\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # Model-specific\n",
    "    model_name: str = field(init=False)\n",
    "    batch_size: int = field(init=False)\n",
    "    gradient_accumulation_steps: int = field(init=False)\n",
    "    learning_rate: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.model_size == \"base\":\n",
    "            self.model_name = \"google/byt5-base\"\n",
    "            self.batch_size = 4\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.learning_rate = 1e-4\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-base\")\n",
    "        else:\n",
    "            self.model_name = \"google/byt5-large\"\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.learning_rate = 5e-5\n",
    "            self.output_dir = Path(\"/content/drive/MyDrive/akkadian/v4-large\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è CHANGE THIS FOR v4-large\n",
    "# ============================================\n",
    "CFG = Config(model_size=\"base\")  # \"base\" or \"large\"\n",
    "\n",
    "CFG.data_dir = Path(competition_path)\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"üöÄ Akkadian V4: {CFG.model_size.upper()} + Full FT + OCR Augmentation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Data: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üé≤ Augment prob: {CFG.augment_prob}\")\n",
    "print(f\"üìö Use published_texts: {CFG.use_published_texts}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574e2c5",
   "metadata": {},
   "source": [
    "## 2. Normalization (V2-identical)\n",
    "\n",
    "Applied to source AFTER augmentation, and to target always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e04c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈°\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´\n",
    "}\n",
    "\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "    '\\u2308': '[', '\\u2309': ']', '\\u230a': '[', '\\u230b': ']',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"Normalize Akkadian transliteration to ASCII.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_translation(text) -> str:\n",
    "    \"\"\"Normalize English translation.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a6ed5",
   "metadata": {},
   "source": [
    "## 3. OCR Noise Augmentation\n",
    "\n",
    "Key insight: Apply noise to RAW data, then normalize.\n",
    "Some noise (diacritics drop, hyphen drop) survives normalization.\n",
    "Quote/bracket variations are normalized away, but that's OK -\n",
    "the model sees variety during training which helps generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca9f2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "_DIACRITICS_DROP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',\n",
    "    '\\u0101': 'a', '\\u0100': 'A',\n",
    "    '\\u0113': 'e', '\\u0112': 'E',\n",
    "    '\\u012b': 'i', '\\u012a': 'I',\n",
    "    '\\u016b': 'u', '\\u016a': 'U',\n",
    "}\n",
    "\n",
    "_SUBSCRIPT_TO_NUM = {\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "}\n",
    "_NUM_TO_SUBSCRIPT = {v: k for k, v in _SUBSCRIPT_TO_NUM.items()}\n",
    "\n",
    "\n",
    "def _drop_diacritics(text: str, prob: float = 0.4) -> str:\n",
    "    \"\"\"Randomly drop diacritics (≈°‚Üís, etc.).\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _DIACRITICS_DROP and random.random() < prob:\n",
    "            result.append(_DIACRITICS_DROP[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _vary_subscripts(text: str, prob: float = 0.3) -> str:\n",
    "    \"\"\"Randomly convert subscripts ‚Üî numbers.\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _SUBSCRIPT_TO_NUM and random.random() < prob:\n",
    "            result.append(_SUBSCRIPT_TO_NUM[char])\n",
    "        elif char in _NUM_TO_SUBSCRIPT and random.random() < prob * 0.3:\n",
    "            result.append(_NUM_TO_SUBSCRIPT[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def _drop_some_hyphens(text: str, prob: float = 0.15) -> str:\n",
    "    \"\"\"Randomly drop some hyphens (not all).\"\"\"\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if '-' in word and random.random() < prob:\n",
    "            word = word.replace('-', '')\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def _protect_and_restore_tokens(text: str, tokens: List[str]) -> tuple:\n",
    "    \"\"\"Protect special tokens from modification.\"\"\"\n",
    "    protected = {}\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        while token in text:\n",
    "            placeholder = f\"__P{counter}__\"\n",
    "            text = text.replace(token, placeholder, 1)\n",
    "            protected[placeholder] = token\n",
    "            counter += 1\n",
    "    return text, protected\n",
    "\n",
    "\n",
    "def _restore_tokens(text: str, protected: dict) -> str:\n",
    "    for placeholder, token in protected.items():\n",
    "        text = text.replace(placeholder, token)\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_ocr_noise(text: str, prob: float = 0.4) -> str:\n",
    "    \"\"\"\n",
    "    Apply OCR-like noise to RAW transliteration.\n",
    "    \n",
    "    Effective noise types (survive normalization):\n",
    "    - Diacritics drop: ≈°‚Üís (most impactful)\n",
    "    - Subscript variation: ‚ÇÑ‚Üí4\n",
    "    - Hyphen drop: qa-ti‚Üíqati\n",
    "    \"\"\"\n",
    "    if not text or random.random() > prob:\n",
    "        return text\n",
    "    \n",
    "    # Protect special tokens\n",
    "    text, protected = _protect_and_restore_tokens(\n",
    "        text, ['<gap>', '<unk>', '<GAP>', '<UNK>']\n",
    "    )\n",
    "    \n",
    "    # Apply noise functions (these survive normalization)\n",
    "    noise_funcs = [\n",
    "        lambda t: _drop_diacritics(t, prob=0.35),\n",
    "        lambda t: _vary_subscripts(t, prob=0.25),\n",
    "        lambda t: _drop_some_hyphens(t, prob=0.12),\n",
    "    ]\n",
    "    \n",
    "    n_funcs = random.randint(1, len(noise_funcs))\n",
    "    for func in random.sample(noise_funcs, n_funcs):\n",
    "        text = func(text)\n",
    "    \n",
    "    text = _restore_tokens(text, protected)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"\\nüìù OCR Augmentation Examples (RAW ‚Üí Noisy):\")\n",
    "test_text = \"≈°um-ma a-wi-lum ·π£a-bi-tam ·∏´u-bu-ul-li-≈°u‚ÇÑ\"\n",
    "for i in range(5):\n",
    "    noisy = apply_ocr_noise(test_text, prob=1.0)\n",
    "    normalized = normalize_transliteration(noisy)\n",
    "    print(f\"   [{i}] {normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98385d",
   "metadata": {},
   "source": [
    "## 4. Load Data with oare_id-based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264a5d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_file(data_dir: Path, filename: str) -> Path:\n",
    "    \"\"\"Find file in data directory or subdirectories.\"\"\"\n",
    "    if (data_dir / filename).exists():\n",
    "        return data_dir / filename\n",
    "    for p in data_dir.glob(f\"**/{filename}\"):\n",
    "        return p\n",
    "    raise FileNotFoundError(f\"{filename} not found in {data_dir}\")\n",
    "\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Load RAW data with oare_id-based split to prevent leakage.\"\"\"\n",
    "    \n",
    "    # 1. Load train.csv\n",
    "    train_path = find_file(CFG.data_dir, \"train.csv\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f\"   train.csv: {len(train_df)} rows\")\n",
    "    \n",
    "    # Standardize columns\n",
    "    train_df = train_df.rename(columns={\n",
    "        'transliteration': 'src_raw',\n",
    "        'translation': 'tgt_raw'\n",
    "    })\n",
    "    train_df['source'] = 'train'\n",
    "    \n",
    "    # 2. Load published_texts.csv if enabled\n",
    "    if CFG.use_published_texts:\n",
    "        try:\n",
    "            pub_path = find_file(CFG.data_dir, \"published_texts.csv\")\n",
    "            pub_df = pd.read_csv(pub_path)\n",
    "            print(f\"   published_texts.csv: {len(pub_df)} rows total\")\n",
    "            \n",
    "            # Use correct column names!\n",
    "            # transliteration (or transliteration_orig) ‚Üí src_raw\n",
    "            # AICC_translation ‚Üí tgt_raw\n",
    "            pub_df = pub_df.rename(columns={\n",
    "                'transliteration': 'src_raw',\n",
    "                'AICC_translation': 'tgt_raw'\n",
    "            })\n",
    "            \n",
    "            # Keep only rows with both src and tgt\n",
    "            pub_df = pub_df.dropna(subset=['src_raw', 'tgt_raw'])\n",
    "            pub_df = pub_df[pub_df['tgt_raw'].str.strip().str.len() > 0]\n",
    "            \n",
    "            print(f\"   published_texts with AICC_translation: {len(pub_df)} rows\")\n",
    "            \n",
    "            if len(pub_df) > 0:\n",
    "                pub_df['source'] = 'published'\n",
    "                train_df = pd.concat([train_df, pub_df[['oare_id', 'src_raw', 'tgt_raw', 'source']]], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            print(\"   ‚ö†Ô∏è published_texts.csv not found, skipping\")\n",
    "    \n",
    "    # Drop NaN\n",
    "    train_df = train_df.dropna(subset=['src_raw', 'tgt_raw']).reset_index(drop=True)\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_by_oare_id(df: pd.DataFrame, val_size: float, seed: int):\n",
    "    \"\"\"\n",
    "    Split by oare_id to prevent train/val leakage.\n",
    "    All rows with same oare_id go to same split.\n",
    "    \"\"\"\n",
    "    unique_ids = df['oare_id'].unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(unique_ids)\n",
    "    \n",
    "    n_val = int(len(unique_ids) * val_size)\n",
    "    val_ids = set(unique_ids[:n_val])\n",
    "    train_ids = set(unique_ids[n_val:])\n",
    "    \n",
    "    train_df = df[df['oare_id'].isin(train_ids)].reset_index(drop=True)\n",
    "    val_df = df[df['oare_id'].isin(val_ids)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "print(\"üìñ Loading raw data...\")\n",
    "raw_df = load_raw_data()\n",
    "print(f\"   Total: {len(raw_df)} rows\")\n",
    "\n",
    "# Split by oare_id\n",
    "print(f\"\\nüîÄ Splitting by oare_id (val_size={CFG.val_size})...\")\n",
    "train_df, val_df = split_by_oare_id(raw_df, CFG.val_size, CFG.seed)\n",
    "print(f\"   Train: {len(train_df)} rows ({train_df['oare_id'].nunique()} documents)\")\n",
    "print(f\"   Val: {len(val_df)} rows ({val_df['oare_id'].nunique()} documents)\")\n",
    "\n",
    "# Verify no leakage\n",
    "train_ids = set(train_df['oare_id'].unique())\n",
    "val_ids = set(val_df['oare_id'].unique())\n",
    "assert len(train_ids & val_ids) == 0, \"Train/val leakage detected!\"\n",
    "print(\"   ‚úÖ No oare_id overlap (no leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e87e2",
   "metadata": {},
   "source": [
    "## 5. Prepare Data with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, augment: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare data:\n",
    "    1. (Optional) Apply OCR noise to RAW source\n",
    "    2. Normalize both source and target\n",
    "    \"\"\"\n",
    "    prepared = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing\"):\n",
    "        src_raw = row['src_raw']\n",
    "        tgt_raw = row['tgt_raw']\n",
    "        \n",
    "        # Apply noise to RAW source\n",
    "        if augment:\n",
    "            src_noisy = apply_ocr_noise(src_raw, prob=CFG.augment_prob)\n",
    "        else:\n",
    "            src_noisy = src_raw\n",
    "        \n",
    "        # Normalize\n",
    "        src = normalize_transliteration(src_noisy)\n",
    "        tgt = normalize_translation(tgt_raw)\n",
    "        \n",
    "        if src and tgt:\n",
    "            prepared.append({'src': src, 'tgt': tgt})\n",
    "    \n",
    "    return pd.DataFrame(prepared)\n",
    "\n",
    "\n",
    "print(\"\\nüîß Preparing training data (with augmentation)...\")\n",
    "train_prepared = prepare_data(train_df, augment=True)\n",
    "print(f\"   Train: {len(train_prepared)} samples\")\n",
    "\n",
    "print(\"\\nüîß Preparing validation data (no augmentation)...\")\n",
    "val_prepared = prepare_data(val_df, augment=False)\n",
    "print(f\"   Val: {len(val_prepared)} samples\")\n",
    "\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"   src: {train_prepared.iloc[0]['src'][:80]}...\")\n",
    "print(f\"   tgt: {train_prepared.iloc[0]['tgt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cc7b4",
   "metadata": {},
   "source": [
    "## 6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55510b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading model: {CFG.model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "print(f\"   Tokenizer: {len(tokenizer)}, Model: {model.config.vocab_size}\")\n",
    "print(f\"   Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "assert len(tokenizer) == model.config.vocab_size, \"Vocab mismatch!\"\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ‚úÖ Gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa1380",
   "metadata": {},
   "source": [
    "## 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0821078",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"\\nüî§ Tokenizing...\")\n",
    "train_ds = Dataset.from_pandas(train_prepared[[\"src\", \"tgt\"]])\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "val_ds = Dataset.from_pandas(val_prepared[[\"src\", \"tgt\"]])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "print(f\"   Train: {len(train_ds)}, Val: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fcb1ef",
   "metadata": {},
   "source": [
    "## 8. Metrics & Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718191c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]\n",
    "        \n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "class LogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.losses = []\n",
    "        print(f\"\\n{'='*60}\\nüìä Epoch {self.epoch + 1}/{args.num_train_epochs}\\n{'='*60}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.losses:\n",
    "            print(f\"\\nüìâ Train Loss: {sum(self.losses)/len(self.losses):.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\\nüìà Validation\\n{'‚îÄ'*40}\")\n",
    "            print(f\"   BLEU: {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF: {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo:  {metrics.get('eval_geo_mean', 0):.2f}\\n{'‚îÄ'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d619245",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    max_grad_norm=CFG.max_grad_norm,\n",
    "    fp16=CFG.fp16,\n",
    "    bf16=CFG.bf16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    ")\n",
    "\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[LogCallback()],\n",
    ")\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Training: {CFG.epochs} epochs, batch {CFG.batch_size}x{CFG.gradient_accumulation_steps}\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e0ddc",
   "metadata": {},
   "source": [
    "## 10. Save & Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db278b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = CFG.output_dir / \"model\"\n",
    "trainer.save_model(str(model_dir))\n",
    "tokenizer.save_pretrained(str(model_dir))\n",
    "print(f\"\\nüíæ Saved: {model_dir}\")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"\\nüìà Final: BLEU={results.get('eval_bleu',0):.2f}, chrF={results.get('eval_chrf',0):.2f}, Geo={results.get('eval_geo_mean',0):.2f}\")\n",
    "\n",
    "import shutil\n",
    "zip_path = CFG.output_dir / f\"akkadian_v4_{CFG.model_size}\"\n",
    "shutil.make_archive(str(zip_path), 'zip', model_dir)\n",
    "print(f\"üì¶ Archive: {zip_path}.zip\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n‚úÖ V4-{CFG.model_size.upper()} Complete!\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
