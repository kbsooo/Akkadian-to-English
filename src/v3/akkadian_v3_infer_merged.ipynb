{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9ab37e",
   "metadata": {},
   "source": [
    "# Akkadian V3 Inference: Merged Model (PEFT Î∂àÌïÑÏöî!)\n",
    "\n",
    "**Key Features:**\n",
    "- Pre-merged model ÏÇ¨Ïö© (LoRAÍ∞Ä Ïù¥ÎØ∏ base modelÏóê ÌÜµÌï©Îê®)\n",
    "- PEFT ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÌïÑÏöî ‚Üí Kaggle Internet OFFÏóêÏÑú ÏûëÎèô\n",
    "- Same normalization as training for consistent results\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU (Internet OFF ÏßÄÏõê)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265950f",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3d8c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9a265",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Inference params\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f160a7",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7f2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data directory not found\")\n",
    "    \n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_merged_model():\n",
    "    \"\"\"Find merged model in Kaggle input.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"outputs/akkadian_v3/merged_model\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local merged model not found\")\n",
    "    \n",
    "    # Kaggle: search for merged model (look for config.json + pytorch_model.bin or model.safetensors)\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"v3\" in d.name.lower() or \"merged\" in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return d\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return sub.parent\n",
    "    raise FileNotFoundError(\"Merged model not found in /kaggle/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48009641",
   "metadata": {},
   "source": [
    "## 3. Normalization (must match training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76312fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ASCII transliteration mapping\n",
    "_NORMALIZE_MAP = {\n",
    "    # Shin/Sibilants\n",
    "    '≈°': 's', '≈†': 'S',\n",
    "    '·π£': 's', '·π¢': 'S',\n",
    "    '≈õ': 's', '≈ö': 'S',\n",
    "    \n",
    "    # Emphatics\n",
    "    '·π≠': 't', '·π¨': 'T',\n",
    "    '·∏´': 'h', '·∏™': 'H',\n",
    "    '·∏•': 'h', '·∏§': 'H',\n",
    "    \n",
    "    # Ayin, Aleph\n",
    "    ' æ': \"'\", ' ø': \"'\", ''': \"'\", ''': \"'\",\n",
    "    'ÀÄ': \"'\", 'ÀÅ': \"'\",\n",
    "    ' î': \"'\", ' ï': \"'\",\n",
    "    \n",
    "    # Nasals\n",
    "    '·πÉ': 'm', '·πÇ': 'M',\n",
    "    '·πÖ': 'n', '·πÑ': 'N',\n",
    "    '√±': 'n', '√ë': 'N',\n",
    "    \n",
    "    # Long vowels (macron)\n",
    "    'ƒÅ': 'a', 'ƒÄ': 'A',\n",
    "    'ƒì': 'e', 'ƒí': 'E',\n",
    "    'ƒ´': 'i', 'ƒ™': 'I',\n",
    "    '≈ç': 'o', '≈å': 'O',\n",
    "    '≈´': 'u', '≈™': 'U',\n",
    "    \n",
    "    # Breve\n",
    "    'ƒÉ': 'a', 'ƒï': 'e', 'ƒ≠': 'i', '≈è': 'o', '≈≠': 'u',\n",
    "    \n",
    "    # Subscript numbers ‚Üí normal\n",
    "    '‚ÇÄ': '0', '‚ÇÅ': '1', '‚ÇÇ': '2', '‚ÇÉ': '3', '‚ÇÑ': '4',\n",
    "    '‚ÇÖ': '5', '‚ÇÜ': '6', '‚Çá': '7', '‚Çà': '8', '‚Çâ': '9',\n",
    "    \n",
    "    # Superscript\n",
    "    '‚Å∞': '0', '¬π': '1', '¬≤': '2', '¬≥': '3', '‚Å¥': '4',\n",
    "    '‚Åµ': '5', '‚Å∂': '6', '‚Å∑': '7', '‚Å∏': '8', '‚Åπ': '9',\n",
    "    \n",
    "    # Special\n",
    "    '√ó': 'x', '¬∑': '.', '¬∞': '',\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_transliteration(text: str) -> str:\n",
    "    \"\"\"Convert Akkadian transliteration to ASCII (must match training).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # NFD decomposition\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    \n",
    "    # Apply mapping\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _NORMALIZE_MAP:\n",
    "            result.append(_NORMALIZE_MAP[char])\n",
    "        elif unicodedata.category(char) == 'Mn':  # Skip combining marks\n",
    "            continue\n",
    "        else:\n",
    "            result.append(char)\n",
    "    \n",
    "    text = ''.join(result)\n",
    "    \n",
    "    # Whitespace cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e52573",
   "metadata": {},
   "source": [
    "## 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V3 Inference: Merged Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = find_merged_model()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Merged model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69655d8b",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f763ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading merged model from: {MODEL_DIR}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer and model from merged directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133cdc4",
   "metadata": {},
   "source": [
    "## 6. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6aecc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug=False):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "        print(f\"   [DEBUG] Output tokens (first 20): {outputs[0][:20].tolist()}\")\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\")\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results = generate_batch(batch)\n",
    "        translations.extend(results)\n",
    "        pbar.set_postfix(done=f\"{min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24640737",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51627ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Normalize\n",
    "print(\"\\nüîß Normalizing (ASCII conversion)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "\n",
    "# Debug first sample\n",
    "print(\"\\n[DEBUG] Testing first sample...\")\n",
    "test_result = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] First translation: '{test_result[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d971fd",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69277c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Save\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
