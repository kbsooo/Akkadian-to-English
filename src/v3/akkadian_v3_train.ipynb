{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f77d0c",
   "metadata": {},
   "source": [
    "# Akkadian V3 Training: ByT5-Large + LoRA\n",
    "\n",
    "**Key Features:**\n",
    "- ByT5-Large (1.2B params) with LoRA for parameter-efficient fine-tuning\n",
    "- Sentence-level data for better train/test distribution match\n",
    "- tqdm-based progress tracking for better visibility\n",
    "\n",
    "**Environment**: Google Colab with A100 GPU\n",
    "\n",
    "**Output**: Saved to Google Drive `/content/drive/MyDrive/akkadian/v3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bf238",
   "metadata": {},
   "source": [
    "## 0. Setup: Install Dependencies & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT for LoRA\n",
    "!pip install -q peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Hub login and data download\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Kaggle\n",
    "kbsooo_akkadian_v2_data_path = kagglehub.dataset_download('kbsooo/akkadian-v2-data')\n",
    "print(f'Data downloaded to: {kbsooo_akkadian_v2_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d459b55",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef1fb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf82c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration for ByT5-Large + LoRA on A100.\"\"\"\n",
    "    \n",
    "    # Model\n",
    "    model_name: str = \"google/byt5-large\"\n",
    "    \n",
    "    # Paths (Colab + Google Drive)\n",
    "    data_dir: Path = None  # Set after kagglehub download\n",
    "    output_dir: Path = Path(\"/content/drive/MyDrive/akkadian/v3\")\n",
    "\n",
    "    # Data files (sentence-level, already normalized)\n",
    "    train_file: str = \"v2_sentence_train.csv\"\n",
    "    val_file: str = \"v2_sentence_val.csv\"\n",
    "    \n",
    "    # LoRA Configuration (from V3 strategy doc)\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q\", \"v\"])\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4  # A100 can handle more\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-4\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Hardware - FP16 OFF for ByT5 numerical stability\n",
    "    fp16: bool = False\n",
    "    bf16: bool = False  # Can try True on A100\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# Set data directory from kagglehub download\n",
    "CFG.data_dir = Path(kbsooo_akkadian_v2_data_path)\n",
    "\n",
    "# Ensure output directory exists\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V3: ByT5-Large + LoRA Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Data directory: {CFG.data_dir}\")\n",
    "print(f\"üìÅ Output directory: {CFG.output_dir}\")\n",
    "print(f\"ü§ñ Model: {CFG.model_name}\")\n",
    "print(f\"üîß LoRA: r={CFG.lora_r}, alpha={CFG.lora_alpha}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reproducibility\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0616b",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load pre-normalized sentence-level data.\"\"\"\n",
    "    train_path = CFG.data_dir / CFG.train_file\n",
    "    val_path = CFG.data_dir / CFG.val_file\n",
    "    \n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "    if not val_path.exists():\n",
    "        raise FileNotFoundError(f\"Val file not found: {val_path}\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    \n",
    "    # Validate columns\n",
    "    required_cols = {\"src\", \"tgt\"}\n",
    "    for name, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            missing = required_cols - set(df.columns)\n",
    "            raise ValueError(f\"Missing columns in {name}: {missing}\")\n",
    "    \n",
    "    # Drop NaN\n",
    "    train_df = train_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "    val_df = val_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "print(\"üìñ Loading preprocessed sentence-level data...\")\n",
    "train_df, val_df = load_data()\n",
    "\n",
    "print(f\"   Train: {len(train_df):,} samples\")\n",
    "print(f\"   Val: {len(val_df):,} samples\")\n",
    "\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"   src: {train_df.iloc[0]['src'][:80]}...\")\n",
    "print(f\"   tgt: {train_df.iloc[0]['tgt'][:80]}...\")\n",
    "\n",
    "# Truncation risk check\n",
    "src_over = (train_df[\"src\"].str.len() > CFG.max_source_length).mean()\n",
    "tgt_over = (train_df[\"tgt\"].str.len() > CFG.max_target_length).mean()\n",
    "print(f\"\\n‚ö†Ô∏è Truncation risk: src>{CFG.max_source_length}: {src_over:.1%}, tgt>{CFG.max_target_length}: {tgt_over:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa911f",
   "metadata": {},
   "source": [
    "## 3. Model & LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading base model: {CFG.model_name}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "print(f\"   Base model loaded: {sum(p.numel() for p in base_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd066b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "print(f\"\\nüîß Applying LoRA (r={CFG.lora_r}, alpha={CFG.lora_alpha})...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=CFG.lora_r,\n",
    "    lora_alpha=CFG.lora_alpha,\n",
    "    lora_dropout=CFG.lora_dropout,\n",
    "    target_modules=CFG.lora_target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if CFG.gradient_checkpointing:\n",
    "    model.enable_input_require_grads()  # Required for LoRA + gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ‚úÖ Gradient checkpointing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5352e4",
   "metadata": {},
   "source": [
    "## 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d09b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    \"\"\"Tokenize source and target texts.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=CFG.max_source_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=CFG.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"\\nüî§ Tokenizing datasets...\")\n",
    "train_ds = Dataset.from_pandas(train_df[[\"src\", \"tgt\"]])\n",
    "val_ds = Dataset.from_pandas(val_df[[\"src\", \"tgt\"]])\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"], desc=\"Tokenizing train\")\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"src\", \"tgt\"], desc=\"Tokenizing val\")\n",
    "\n",
    "print(f\"   Train: {len(train_ds):,} samples\")\n",
    "print(f\"   Val: {len(val_ds):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988015ea",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05c456",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_compute_metrics(tokenizer):\n",
    "    \"\"\"Build metrics computation function.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        # Replace -100 with pad token\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [[l.strip()] for l in decoded_labels]  # List of references\n",
    "        \n",
    "        # Compute scores\n",
    "        bleu_score = bleu.corpus_score(decoded_preds, decoded_labels).score\n",
    "        chrf_score = chrf.corpus_score(decoded_preds, decoded_labels).score\n",
    "        geo_mean = np.sqrt(bleu_score * chrf_score) if bleu_score > 0 and chrf_score > 0 else 0.0\n",
    "        \n",
    "        return {\"bleu\": bleu_score, \"chrf\": chrf_score, \"geo_mean\": geo_mean}\n",
    "    \n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fa3b4",
   "metadata": {},
   "source": [
    "## 6. Custom Callbacks for Better Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38130c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TqdmLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Enhanced logging with tqdm-style progress and clear metrics display.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_epoch = 0\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.current_epoch = int(state.epoch) if state.epoch else 0\n",
    "        self.train_loss = []\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Epoch {self.current_epoch + 1}/{args.num_train_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.train_loss.append(logs[\"loss\"])\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.train_loss:\n",
    "            avg_loss = sum(self.train_loss) / len(self.train_loss)\n",
    "            print(f\"\\nüìâ Epoch {self.current_epoch + 1} Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n{'‚îÄ'*40}\")\n",
    "            print(f\"üìà Validation Results (Epoch {self.current_epoch + 1})\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "            print(f\"   Loss:     {metrics.get('eval_loss', 0):.4f}\")\n",
    "            print(f\"   BLEU:     {metrics.get('eval_bleu', 0):.2f}\")\n",
    "            print(f\"   chrF++:   {metrics.get('eval_chrf', 0):.2f}\")\n",
    "            print(f\"   Geo Mean: {metrics.get('eval_geo_mean', 0):.2f}\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéâ Training Complete!\")\n",
    "        print(f\"   Total steps: {state.global_step:,}\")\n",
    "        print(f\"   Best metric: {state.best_metric:.2f}\" if state.best_metric else \"\")\n",
    "        print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f331205",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(CFG.output_dir / \"checkpoints\"),\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=CFG.warmup_ratio,\n",
    "    max_grad_norm=CFG.max_grad_norm,\n",
    "    fp16=CFG.fp16 and torch.cuda.is_available(),\n",
    "    bf16=CFG.bf16 and torch.cuda.is_available(),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_geo_mean\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CFG.max_target_length,\n",
    "    dataloader_num_workers=CFG.dataloader_num_workers,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "# Handle API version differences\n",
    "try:\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "    training_args = Seq2SeqTrainingArguments(**training_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=build_compute_metrics(tokenizer),\n",
    "    callbacks=[TqdmLoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "print(f\"   Epochs: {CFG.epochs}\")\n",
    "print(f\"   Batch size: {CFG.batch_size} x {CFG.gradient_accumulation_steps} = {CFG.batch_size * CFG.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {CFG.learning_rate}\")\n",
    "print()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb123dea",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4479aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter to Google Drive\n",
    "adapter_dir = CFG.output_dir / \"lora_adapter\"\n",
    "print(f\"\\nüíæ Saving LoRA adapter to: {adapter_dir}\")\n",
    "model.save_pretrained(str(adapter_dir))\n",
    "tokenizer.save_pretrained(str(adapter_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nüìà Final Evaluation:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"   BLEU:     {results.get('eval_bleu', 0):.2f}\")\n",
    "print(f\"   chrF++:   {results.get('eval_chrf', 0):.2f}\")\n",
    "print(f\"   Geo Mean: {results.get('eval_geo_mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cc6e4",
   "metadata": {},
   "source": [
    "## 9. Create Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1198509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create ZIP archive for easy download\n",
    "zip_path = CFG.output_dir / \"akkadian_v3_lora\"\n",
    "shutil.make_archive(str(zip_path), 'zip', adapter_dir)\n",
    "print(f\"\\nüì¶ Model archived: {zip_path}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ V3 Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ LoRA adapter: {adapter_dir}\")\n",
    "print(f\"üì¶ Archive: {zip_path}.zip\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the archive from Google Drive\")\n",
    "print(\"2. Upload to Kaggle as a dataset for inference\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
