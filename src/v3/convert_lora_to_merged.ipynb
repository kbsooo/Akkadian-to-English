{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e99ef6",
   "metadata": {},
   "source": [
    "# V3 LoRA â†’ Merged Model ë³€í™˜\n",
    "\n",
    "ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì €ì¥ëœ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ê³ ,\n",
    "base modelê³¼ mergeí•˜ì—¬ **ì „ì²´ ëª¨ë¸**ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ í•˜ë©´ ì¶”ë¡  ì‹œ PEFTê°€ í•„ìš” ì—†ì–´ì„œ Kaggle (internet off)ì—ì„œë„ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14112de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Paths\n",
    "ADAPTER_DIR = Path(\"/content/drive/MyDrive/akkadian/v3/lora_adapter\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/akkadian/v3/merged_model\")\n",
    "\n",
    "print(f\"ğŸ“ LoRA adapter: {ADAPTER_DIR}\")\n",
    "print(f\"ğŸ“ Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"\\nğŸ¤– Loading base model: google/byt5-large\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-large\")\n",
    "print(\"   Base model loaded\")\n",
    "\n",
    "# Load tokenizer from adapter\n",
    "print(\"\\nğŸ”¤ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(ADAPTER_DIR))\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ba830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA adapter\n",
    "print(f\"\\nğŸ”§ Loading LoRA adapter from: {ADAPTER_DIR}\")\n",
    "model = PeftModel.from_pretrained(base_model, str(ADAPTER_DIR))\n",
    "print(\"   LoRA adapter loaded\")\n",
    "\n",
    "# Merge weights\n",
    "print(\"\\nğŸ”€ Merging adapter weights into base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"   âœ… Merge complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nğŸ’¾ Saving merged model to: {OUTPUT_DIR}\")\n",
    "model.save_pretrained(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "print(\"   âœ… Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20473707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP archive\n",
    "import shutil\n",
    "zip_path = OUTPUT_DIR.parent / \"akkadian_v3_merged\"\n",
    "shutil.make_archive(str(zip_path), 'zip', OUTPUT_DIR)\n",
    "print(f\"\\nğŸ“¦ Archive created: {zip_path}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ë³€í™˜ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“ Merged model: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ“¦ Archive: {zip_path}.zip\")\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. akkadian_v3_merged.zip ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"2. Kaggle Datasetìœ¼ë¡œ ì—…ë¡œë“œ\")\n",
    "print(\"3. ì¶”ë¡  ì½”ë“œì—ì„œ ì´ ëª¨ë¸ ì‚¬ìš© (PEFT ë¶ˆí•„ìš”!)\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
