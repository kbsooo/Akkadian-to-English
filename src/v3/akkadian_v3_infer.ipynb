{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f7feb8",
   "metadata": {},
   "source": [
    "# Akkadian V3 Inference: Merged Model\n",
    "\n",
    "**Key Features:**\n",
    "- Pre-merged model (LoRA already integrated into base model)\n",
    "- NO PEFT required ‚Üí Works on Kaggle Internet OFF\n",
    "- Same normalization as training\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU (Internet OFF supported)\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v3/akkadian_v3_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9b167",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a54654",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a4c78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model path (merged model - no PEFT needed)\n",
    "    model_path: str = \"/kaggle/input/akkadian-v3/pytorch/default/4\"\n",
    "    \n",
    "    # Inference params\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc9393",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2a0a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data directory not found\")\n",
    "    \n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b714",
   "metadata": {},
   "source": [
    "## 3. Normalization (must match training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b84ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ASCII transliteration mapping\n",
    "_NORMALIZE_MAP = {\n",
    "    # Shin/Sibilants\n",
    "    '≈°': 's', '≈†': 'S',\n",
    "    '·π£': 's', '·π¢': 'S',\n",
    "    '≈õ': 's', '≈ö': 'S',\n",
    "    \n",
    "    # Emphatics\n",
    "    '·π≠': 't', '·π¨': 'T',\n",
    "    '·∏´': 'h', '·∏™': 'H',\n",
    "    '·∏•': 'h', '·∏§': 'H',\n",
    "    \n",
    "    # Ayin, Aleph\n",
    "    ' æ': \"'\", ' ø': \"'\", ''': \"'\", ''': \"'\",\n",
    "    'ÀÄ': \"'\", 'ÀÅ': \"'\",\n",
    "    ' î': \"'\", ' ï': \"'\",\n",
    "    \n",
    "    # Nasals\n",
    "    '·πÉ': 'm', '·πÇ': 'M',\n",
    "    '·πÖ': 'n', '·πÑ': 'N',\n",
    "    '√±': 'n', '√ë': 'N',\n",
    "    \n",
    "    # Long vowels (macron)\n",
    "    'ƒÅ': 'a', 'ƒÄ': 'A',\n",
    "    'ƒì': 'e', 'ƒí': 'E',\n",
    "    'ƒ´': 'i', 'ƒ™': 'I',\n",
    "    '≈ç': 'o', '≈å': 'O',\n",
    "    '≈´': 'u', '≈™': 'U',\n",
    "    \n",
    "    # Breve\n",
    "    'ƒÉ': 'a', 'ƒï': 'e', 'ƒ≠': 'i', '≈è': 'o', '≈≠': 'u',\n",
    "    \n",
    "    # Subscript numbers ‚Üí normal\n",
    "    '‚ÇÄ': '0', '‚ÇÅ': '1', '‚ÇÇ': '2', '‚ÇÉ': '3', '‚ÇÑ': '4',\n",
    "    '‚ÇÖ': '5', '‚ÇÜ': '6', '‚Çá': '7', '‚Çà': '8', '‚Çâ': '9',\n",
    "    \n",
    "    # Superscript\n",
    "    '‚Å∞': '0', '¬π': '1', '¬≤': '2', '¬≥': '3', '‚Å¥': '4',\n",
    "    '‚Åµ': '5', '‚Å∂': '6', '‚Å∑': '7', '‚Å∏': '8', '‚Åπ': '9',\n",
    "    \n",
    "    # Special\n",
    "    '√ó': 'x', '¬∑': '.', '¬∞': '',\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_transliteration(text: str) -> str:\n",
    "    \"\"\"Convert Akkadian transliteration to ASCII (must match training).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # NFD decomposition\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    \n",
    "    # Apply mapping\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in _NORMALIZE_MAP:\n",
    "            result.append(_NORMALIZE_MAP[char])\n",
    "        elif unicodedata.category(char) == 'Mn':  # Skip combining marks\n",
    "            continue\n",
    "        else:\n",
    "            result.append(char)\n",
    "    \n",
    "    text = ''.join(result)\n",
    "    \n",
    "    # Whitespace cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f32b6",
   "metadata": {},
   "source": [
    "## 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95feaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V3 Inference: Merged Model (PEFT-free)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = Path(CFG.model_path)\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Merged model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29108b07",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b692f4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading merged model from: {MODEL_DIR}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer and model directly (no PEFT needed!)\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9046b17",
   "metadata": {},
   "source": [
    "## 6. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f997fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug=False):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "        print(f\"   [DEBUG] Output tokens (first 20): {outputs[0][:20].tolist()}\")\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Decoded (first): '{decoded[0][:100]}'\")\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\", ncols=80)\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results = generate_batch(batch)\n",
    "        translations.extend(results)\n",
    "        pbar.set_postfix(done=f\"{min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c554c0",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Normalize\n",
    "print(\"\\nüîß Normalizing (ASCII conversion)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "\n",
    "# Debug first sample\n",
    "print(\"\\n[DEBUG] Testing first sample...\")\n",
    "test_result = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] First translation: '{test_result[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd677d5",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Save\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
