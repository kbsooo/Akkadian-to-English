{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dd8e0a",
   "metadata": {},
   "source": [
    "# Akkadian V3 Inference: ByT5-Large + LoRA\n",
    "\n",
    "**Key Features:**\n",
    "- Load LoRA adapter weights and merge with base model\n",
    "- Same normalization as training for consistent results\n",
    "- tqdm progress bar for translation progress\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v3/akkadian_v3_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21154b84",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff031e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ece72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model\n",
    "    base_model_name: str = \"google/byt5-large\"\n",
    "    \n",
    "    # Inference params\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 256\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = True\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e5949",
   "metadata": {},
   "source": [
    "## 2. Normalization (MUST match training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d9423",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Vowels with diacritics ‚Üí base vowels\n",
    "_VOWEL_MAP = {\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "# Akkadian consonants ‚Üí ASCII\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈°, ≈†\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£, ·π¢\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠, ·π¨\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´, ·∏™\n",
    "}\n",
    "\n",
    "# OCR artifacts\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"', '\\u201c': '\"', '\\u201d': '\"',\n",
    "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201a': \"'\",\n",
    "    '\\u02be': \"'\", '\\u02bf': \"'\",\n",
    "    '\\u2308': '[', '\\u2309': ']', '\\u230a': '[', '\\u230b': ']',\n",
    "}\n",
    "\n",
    "# Subscripts\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text):\n",
    "    \"\"\"Normalize to ASCII - MUST match training preprocessing.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    text = text.replace('\\u2026', ' <gap> ')\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13292c14",
   "metadata": {},
   "source": [
    "## 3. Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return Path(\"/kaggle/input\").exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition test data.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        return Path(\"data\")\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"test.csv\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")\n",
    "\n",
    "\n",
    "def find_lora_adapter():\n",
    "    \"\"\"Find LoRA adapter in Kaggle input.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"outputs/akkadian_v3/lora_adapter\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local LoRA adapter not found\")\n",
    "    \n",
    "    # Kaggle: find adapter (look for adapter_config.json)\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if (d / \"adapter_config.json\").exists():\n",
    "            return d\n",
    "        for sub in d.glob(\"**/adapter_config.json\"):\n",
    "            return sub.parent\n",
    "    raise FileNotFoundError(\"LoRA adapter not found in /kaggle/input\")\n",
    "\n",
    "\n",
    "def find_base_model():\n",
    "    \"\"\"Find base model (byt5-large) in Kaggle input for offline use.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        # Local: use HuggingFace model name\n",
    "        return \"google/byt5-large\"\n",
    "    \n",
    "    # Kaggle: search for local byt5-large model\n",
    "    # Common paths from Kaggle Models\n",
    "    possible_paths = [\n",
    "        CFG.kaggle_input / \"byt5-large\" / \"pytorch\" / \"default\" / \"1\",\n",
    "        CFG.kaggle_input / \"byt5-large\",\n",
    "        CFG.kaggle_input / \"google-byt5-large\",\n",
    "    ]\n",
    "    \n",
    "    for p in possible_paths:\n",
    "        if p.exists() and (p / \"config.json\").exists():\n",
    "            return str(p)\n",
    "    \n",
    "    # Search all input directories for config.json (byt5-large marker)\n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"byt5\" in d.name.lower():\n",
    "            if (d / \"config.json\").exists():\n",
    "                return str(d)\n",
    "            for sub in d.glob(\"**/config.json\"):\n",
    "                return str(sub.parent)\n",
    "    \n",
    "    # Fallback to online (will fail if internet is off)\n",
    "    print(\"‚ö†Ô∏è Local byt5-large not found, trying online...\")\n",
    "    return \"google/byt5-large\"\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V3 Inference: ByT5-Large + LoRA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "ADAPTER_DIR = find_lora_adapter()\n",
    "BASE_MODEL_PATH = find_base_model()\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"üîß LoRA adapter: {ADAPTER_DIR}\")\n",
    "print(f\"ü§ñ Base model: {BASE_MODEL_PATH}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfab216",
   "metadata": {},
   "source": [
    "## 4. Load Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading base model: {BASE_MODEL_PATH}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer (ByT5 uses byte-level, extra_ids must match training)\n",
    "# Training uses AutoTokenizer which defaults to extra_ids=125 for byt5\n",
    "tokenizer = ByT5Tokenizer(extra_ids=125)\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Load base model (offline-compatible)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "print(f\"   Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32aa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA adapter\n",
    "print(f\"\\nüîß Loading LoRA adapter from: {ADAPTER_DIR}\")\n",
    "model = PeftModel.from_pretrained(base_model, str(ADAPTER_DIR))\n",
    "print(\"   LoRA adapter loaded\")\n",
    "\n",
    "# Merge and unload for faster inference\n",
    "print(\"   Merging adapter weights...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"   ‚úÖ Merged successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81256f40",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Ensure vocab sizes match\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Resized embeddings to: {model.config.vocab_size}\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if CFG.fp16 and device.type == \"cuda\":\n",
    "    model = model.half()\n",
    "    print(\"   ‚úÖ Using FP16\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696d889",
   "metadata": {},
   "source": [
    "## 5. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c487d39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    \n",
    "    # tqdm progress bar\n",
    "    pbar = tqdm(\n",
    "        range(0, len(texts), batch_size),\n",
    "        desc=\"üîÆ Translating\",\n",
    "        unit=\"batch\",\n",
    "        ncols=80\n",
    "    )\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translations.extend(generate_batch(batch))\n",
    "        pbar.set_postfix({\"done\": f\"{len(translations)}/{len(texts)}\"})\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c5596",
   "metadata": {},
   "source": [
    "## 6. Load Test Data & Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Normalize (CRITICAL: same as training)\n",
    "print(\"\\nüîß Normalizing (ASCII conversion)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78552e",
   "metadata": {},
   "source": [
    "## 7. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Validation\n",
    "assert len(submission) == len(test_df), \"Row count mismatch!\"\n",
    "assert submission[\"translation\"].notna().all(), \"Found NaN translations!\"\n",
    "\n",
    "# Save\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission):,}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa882a7",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Submit `submission.csv` to the competition."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
