{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cc6efa",
   "metadata": {},
   "source": [
    "# Akkadian V3 Inference: Merged Model\n",
    "\n",
    "**Key Features:**\n",
    "- Pre-merged model (LoRA already integrated into base model)\n",
    "- NO PEFT required ‚Üí Works on Kaggle Internet OFF\n",
    "- **V2-identical normalization** for consistent results\n",
    "\n",
    "**Environment**: Kaggle T4/P100 GPU (Internet OFF supported)\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "uv run jupytext --to notebook src/v3/akkadian_v3_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cf72f",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40370c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa847bfe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    kaggle_input: Path = Path(\"/kaggle/input\")\n",
    "    kaggle_working: Path = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Model path (merged model - no PEFT needed)\n",
    "    model_path: str = \"/kaggle/input/akkadian-v3/pytorch/default/4\"\n",
    "    \n",
    "    # Inference params (MUST match V2 for fair comparison)\n",
    "    max_source_length: int = 512   # Same as V2\n",
    "    max_target_length: int = 512   # Same as V2\n",
    "    batch_size: int = 4\n",
    "    num_beams: int = 4\n",
    "    fp16: bool = False  # ByT5 is unstable with FP16\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61083fb2",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca70fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return CFG.kaggle_input.exists()\n",
    "\n",
    "\n",
    "def find_competition_data():\n",
    "    \"\"\"Find competition data directory.\"\"\"\n",
    "    if not is_kaggle():\n",
    "        local = Path(\"data\")\n",
    "        if local.exists():\n",
    "            return local\n",
    "        raise FileNotFoundError(\"Local data directory not found\")\n",
    "    \n",
    "    for d in CFG.kaggle_input.iterdir():\n",
    "        if \"deep-past\" in d.name.lower() or \"akkadian\" in d.name.lower():\n",
    "            if (d / \"test.csv\").exists():\n",
    "                return d\n",
    "    raise FileNotFoundError(\"Competition data not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88777f1",
   "metadata": {},
   "source": [
    "## 3. Normalization (IDENTICAL to V2 normalize.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bdc0b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Character Mapping Tables (copied from V2 normalize.py)\n",
    "# ==============================================================================\n",
    "\n",
    "# Vowels with diacritics ‚Üí base vowels\n",
    "_VOWEL_MAP = {\n",
    "    # a variants\n",
    "    '\\u00e0': 'a', '\\u00e1': 'a', '\\u00e2': 'a', '\\u0101': 'a', '\\u00e4': 'a',\n",
    "    '\\u00c0': 'A', '\\u00c1': 'A', '\\u00c2': 'A', '\\u0100': 'A', '\\u00c4': 'A',\n",
    "    # e variants\n",
    "    '\\u00e8': 'e', '\\u00e9': 'e', '\\u00ea': 'e', '\\u0113': 'e', '\\u00eb': 'e',\n",
    "    '\\u00c8': 'E', '\\u00c9': 'E', '\\u00ca': 'E', '\\u0112': 'E', '\\u00cb': 'E',\n",
    "    # i variants  \n",
    "    '\\u00ec': 'i', '\\u00ed': 'i', '\\u00ee': 'i', '\\u012b': 'i', '\\u00ef': 'i',\n",
    "    '\\u00cc': 'I', '\\u00cd': 'I', '\\u00ce': 'I', '\\u012a': 'I', '\\u00cf': 'I',\n",
    "    # o variants\n",
    "    '\\u00f2': 'o', '\\u00f3': 'o', '\\u00f4': 'o', '\\u014d': 'o', '\\u00f6': 'o',\n",
    "    '\\u00d2': 'O', '\\u00d3': 'O', '\\u00d4': 'O', '\\u014c': 'O', '\\u00d6': 'O',\n",
    "    # u variants\n",
    "    '\\u00f9': 'u', '\\u00fa': 'u', '\\u00fb': 'u', '\\u016b': 'u', '\\u00fc': 'u',\n",
    "    '\\u00d9': 'U', '\\u00da': 'U', '\\u00db': 'U', '\\u016a': 'U', '\\u00dc': 'U',\n",
    "}\n",
    "\n",
    "# Special Akkadian consonants ‚Üí ASCII\n",
    "_CONSONANT_MAP = {\n",
    "    '\\u0161': 's', '\\u0160': 'S',  # ≈°, ≈† (shin)\n",
    "    '\\u1e63': 's', '\\u1e62': 'S',  # ·π£, ·π¢ (tsade)\n",
    "    '\\u1e6d': 't', '\\u1e6c': 'T',  # ·π≠, ·π¨ (emphatic t)\n",
    "    '\\u1e2b': 'h', '\\u1e2a': 'H',  # ·∏´, ·∏™ (het)\n",
    "}\n",
    "\n",
    "# OCR artifacts and typography\n",
    "_OCR_MAP = {\n",
    "    '\\u201e': '\"',   # ‚Äû German low quote\n",
    "    '\\u201c': '\"',   # \" left double quote\n",
    "    '\\u201d': '\"',   # \" right double quote\n",
    "    '\\u2018': \"'\",   # ' left single quote\n",
    "    '\\u2019': \"'\",   # ' right single quote\n",
    "    '\\u201a': \"'\",   # ‚Äö single low quote\n",
    "    '\\u02be': \"'\",   #  æ aleph (modifier letter right half ring)\n",
    "    '\\u02bf': \"'\",   #  ø ayin (modifier letter left half ring)\n",
    "    '\\u2308': '[',   # ‚åà left ceiling (half bracket)\n",
    "    '\\u2309': ']',   # ‚åâ right ceiling\n",
    "    '\\u230a': '[',   # ‚åä left floor\n",
    "    '\\u230b': ']',   # ‚åã right floor\n",
    "}\n",
    "\n",
    "# Subscripts ‚Üí numbers\n",
    "_SUBSCRIPT_MAP = str.maketrans({\n",
    "    '\\u2080': '0', '\\u2081': '1', '\\u2082': '2', '\\u2083': '3', '\\u2084': '4',\n",
    "    '\\u2085': '5', '\\u2086': '6', '\\u2087': '7', '\\u2088': '8', '\\u2089': '9',\n",
    "    '\\u2093': 'x',\n",
    "})\n",
    "\n",
    "# Combined translation table\n",
    "_FULL_MAP = str.maketrans({**_VOWEL_MAP, **_CONSONANT_MAP, **_OCR_MAP})\n",
    "\n",
    "\n",
    "def normalize_transliteration(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Akkadian transliteration to ASCII-compatible format.\n",
    "    \n",
    "    IDENTICAL to V2 src/v2/normalize.py:normalize_transliteration\n",
    "    \n",
    "    Transformations:\n",
    "    1. Unicode NFC normalization\n",
    "    2. Diacritics removal (√† ‚Üí a, ≈° ‚Üí s, etc.)\n",
    "    3. OCR artifact cleanup (curly quotes ‚Üí straight quotes)\n",
    "    4. Subscript normalization (‚ÇÑ ‚Üí 4)\n",
    "    5. Gap/damage markers ([...] ‚Üí <gap>)\n",
    "    6. Unknown sign markers (x ‚Üí <unk>)\n",
    "    7. Editorial mark removal (!?/)\n",
    "    8. Whitespace normalization\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and text != text):  # NaN check\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. Unicode normalization\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # 2. Apply character mappings (diacritics, consonants, OCR)\n",
    "    text = text.translate(_FULL_MAP)\n",
    "    \n",
    "    # 3. Subscript normalization\n",
    "    text = text.translate(_SUBSCRIPT_MAP)\n",
    "    \n",
    "    # 4. Handle ellipsis and big gaps\n",
    "    text = text.replace('\\u2026', ' <gap> ')  # ‚Ä¶\n",
    "    text = re.sub(r'\\.\\.\\.+', ' <gap> ', text)\n",
    "    \n",
    "    # 5. Handle bracketed content (damaged/reconstructed)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', ' <gap> ', text)\n",
    "    \n",
    "    # 6. Handle unknown signs\n",
    "    text = re.sub(r'\\bx\\b', ' <unk> ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 7. Remove editorial marks\n",
    "    text = re.sub(r'[!?/]', ' ', text)\n",
    "    \n",
    "    # 8. Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3c7fc",
   "metadata": {},
   "source": [
    "## 4. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Akkadian V3 Inference: Merged Model (PEFT-free)\")\n",
    "print(\"   Normalization: V2-identical\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "COMP_DIR = find_competition_data()\n",
    "MODEL_DIR = Path(CFG.model_path)\n",
    "\n",
    "print(f\"üìÅ Competition data: {COMP_DIR}\")\n",
    "print(f\"ü§ñ Merged model: {MODEL_DIR}\")\n",
    "print(f\"üéÆ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a056382",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525db7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ Loading merged model from: {MODEL_DIR}\")\n",
    "print(\"   This may take a few minutes...\")\n",
    "\n",
    "# Use ByT5Tokenizer with extra_ids to match V2 exactly\n",
    "# ByT5 vocab: 256 (bytes) + 3 (special) + 125 (extra_ids) = 384\n",
    "tokenizer = ByT5Tokenizer(extra_ids=125)\n",
    "print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "print(f\"   Model vocab size: {model.config.vocab_size}\")\n",
    "print(f\"   Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Verify vocab sizes match\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    print(f\"   ‚ö†Ô∏è Vocab mismatch! Tokenizer: {len(tokenizer)}, Model: {model.config.vocab_size}\")\n",
    "    # Do NOT resize - use model's vocab size\n",
    "else:\n",
    "    print(f\"   ‚úÖ Vocab sizes match: {len(tokenizer)}\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e28fb",
   "metadata": {},
   "source": [
    "## 6. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacc502",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(texts, debug=False):\n",
    "    \"\"\"Generate translations for a batch of texts.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_source_length,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Input shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CFG.max_target_length,\n",
    "        num_beams=CFG.num_beams,\n",
    "        repetition_penalty=1.2,   # Prevent repetition\n",
    "        no_repeat_ngram_size=3,   # No 3-gram repeats\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Output shape: {outputs.shape}\")\n",
    "        print(f\"   [DEBUG] Output tokens (first 20): {outputs[0][:20].tolist()}\")\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   [DEBUG] Decoded (first): '{decoded[0][:100]}'\")\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def translate_all(texts, batch_size=None):\n",
    "    \"\"\"Translate all texts with progress bar.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CFG.batch_size\n",
    "    \n",
    "    translations = []\n",
    "    pbar = tqdm(range(0, len(texts), batch_size), desc=\"üîÆ Translating\", unit=\"batch\", ncols=80)\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results = generate_batch(batch)\n",
    "        translations.extend(results)\n",
    "        pbar.set_postfix(done=f\"{min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626cb1e",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìñ Loading test data...\")\n",
    "test_df = pd.read_csv(COMP_DIR / \"test.csv\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Normalize using V2-identical function\n",
    "print(\"\\nüîß Normalizing (V2-identical)...\")\n",
    "normalized = [normalize_transliteration(t) for t in tqdm(test_df[\"transliteration\"], desc=\"Normalizing\")]\n",
    "\n",
    "print(f\"\\nüìù Sample normalized:\")\n",
    "for i in range(min(2, len(normalized))):\n",
    "    print(f\"   [{i}] {normalized[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Running inference...\")\n",
    "\n",
    "# Debug first sample\n",
    "print(\"\\n[DEBUG] Testing first sample...\")\n",
    "test_result = generate_batch([normalized[0]], debug=True)\n",
    "print(f\"[DEBUG] First translation: '{test_result[0][:100]}...'\")\n",
    "\n",
    "translations = translate_all(normalized)\n",
    "\n",
    "print(f\"\\nüìù Sample outputs:\")\n",
    "for i in range(min(3, len(translations))):\n",
    "    print(f\"   [{i}] {translations[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714988a8",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1470988",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": translations,\n",
    "})\n",
    "\n",
    "# Save\n",
    "output_path = CFG.kaggle_working / \"submission.csv\" if is_kaggle() else Path(\"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Inference Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Saved: {output_path}\")\n",
    "print(f\"   Rows: {len(submission)}\")\n",
    "print()\n",
    "print(submission.head())\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
